{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c72-N0RGo1z",
        "colab_type": "code",
        "outputId": "6888eb6e-8793-46f5-8a8e-6643351532ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZvrBG6DXhkA",
        "colab_type": "code",
        "outputId": "47df979e-3312-4c63-ce1d-427e3ba1a10b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd gdrive/My\\ Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ds8aZ8OFzw8",
        "colab_type": "code",
        "outputId": "1553c0d1-e1b5-4be3-f421-79c138481c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        }
      },
      "source": [
        "# Install necessary packages -> uncomment what is currently needed\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install contractions\n",
        "!pip install wordsegment\n",
        "!pip install -U symspellpy\n",
        "!pip install emoji --upgrade\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install bert-for-tf2\n",
        "!pip install transformers\n",
        "# !pip install nltk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.24)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already up-to-date: symspellpy in /usr/local/lib/python3.6/dist-packages (6.5.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.5)\n",
            "Requirement already up-to-date: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.17.5)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.13.5)\n",
            "Requirement already satisfied: py-params>=0.7.3 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: params-flow>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.7.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7864AKjDrMsa",
        "colab_type": "code",
        "outputId": "61bc07d1-61cc-47e8-f5ff-6d36012104f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# All imports - DO NOT CHANGE THE ORDER OF INSTRUCTIONS\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "    sys.path.insert(0, 'bert_repo')\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode\n",
        "import contractions\n",
        "import gensim.downloader as api\n",
        "import re\n",
        "import wordsegment\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import emoji\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf2\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from modeling import BertModel, BertConfig\n",
        "from tokenization import FullTokenizer, convert_to_unicode\n",
        "from extract_features import InputExample, convert_examples_to_features\n",
        "from tqdm import tqdm\n",
        "#import tensorflow_addons as tfa\n",
        "# import nltk\n",
        "from google.colab import auth, drive\n",
        "# nltk.download('punkt')\n",
        "\n",
        "wordsegment.load()\n",
        "\n",
        "# Load SymSpell -> package for correcting misspellings\n",
        "sym_spell = SymSpell(2, 7)\n",
        "\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "# get TF logger \n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mER1mPUtiaPl",
        "colab_type": "code",
        "outputId": "16f688f6-3901-4063-8936-84cdd672115b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "#Import data\n",
        "training_examples_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/olid-training-v1.0.tsv'\n",
        "testing_examples_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/testset-levela.tsv'\n",
        "testing_labels_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/labels-levela.csv'\n",
        "\n",
        "training_dataset = pd.read_csv(training_examples_url, delimiter='\\t')\n",
        "testing_dataset_examples_A = pd.read_csv(testing_examples_A_url, delimiter='\\t')\n",
        "testing_dataset_labels_A = pd.read_csv(testing_labels_A_url, delimiter=',')\n",
        "\n",
        "print(training_dataset.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Afh-rMX8zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import 2020 data\n",
        "\n",
        "training_dataset2020 = pd.read_csv('Colab Notebooks/task_a_distant.tsv', delimiter='\\t', nrows=100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHJ43pnca3Cz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "9afaef3f-5767-4178-b65a-6b3cc56266d2"
      },
      "source": [
        "def convert_averages(row):\n",
        "    return 'OFF' if row['average'] >= 0.4 else 'NOT'\n",
        "\n",
        "\n",
        "training_dataset2020 = training_dataset2020.drop(labels=['id', 'std'], axis=1)\n",
        "training_dataset2020['average'] = training_dataset2020.apply(lambda row: convert_averages(row), axis=1)\n",
        "training_dataset2020 = training_dataset2020.rename(columns={ 'average': 'subtask_a', 'text': 'tweet' })\n",
        "training_dataset = training_dataset.append(training_dataset2020)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  sort=sort,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJNUAx_OiYP7",
        "colab_type": "text"
      },
      "source": [
        "## Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xDBjuNciVDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Additional datasets\n",
        "set_urls = [\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set1.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set2.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set3.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set4.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set5.csv',\n",
        "]\n",
        "\n",
        "sets = [pd.read_csv(url) for url in set_urls]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCDhsVxMLnLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labels_set1(row):\n",
        "  return 'OFF' if 1 in [row['toxic'], row['severe_toxic'], row['obscene'], row['threat'], row['insult'], row['identity_hate']] else 'NOT'\n",
        "\n",
        "def labels_set2(row):\n",
        "  return 'OFF' if row['class'] != 2 else 'NOT'\n",
        "\n",
        "def labels_set_ordinary(row):\n",
        "  return 'OFF' if row['subtask_a'] == 1 else 'NOT'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0uTpIMiVih",
        "colab_type": "code",
        "outputId": "99ba361e-8fc8-4839-ddde-04624dee924c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "# Preprocess additional datasets\n",
        "\n",
        "sets[0] = sets[0].rename(columns={ 'label': 'subtask_a' })\n",
        "sets[0]['subtask_a'] = sets[0].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[0] = sets[0].drop('id', axis=1)\n",
        "\n",
        "sets[1] = sets[1].rename(columns={ 'comment_text': 'tweet', 'label': 'subtask_a' })\n",
        "sets[1]['subtask_a'] = sets[1].apply(lambda row: labels_set1(row), axis=1)\n",
        "sets[1] = sets[1].drop(labels=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "\n",
        "sets[2] = sets[2].rename(columns={ 'label': 'subtask_a' })\n",
        "sets[2]['subtask_a'] = sets[2].apply(lambda row: labels_set2(row), axis=1)\n",
        "sets[2] = sets[2].drop(labels=['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'Unnamed: 0'], axis=1)\n",
        "\n",
        "sets[3] = sets[3].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[3]['subtask_a'] = sets[3].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[3] = sets[3].drop('Date', axis=1)\n",
        "\n",
        "sets[4] = sets[4].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[4]['subtask_a'] = sets[4].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[4] = sets[4].drop(labels=['Date', 'Usage'], axis=1)\n",
        "\n",
        "sets[5] = sets[5].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[5]['subtask_a'] = sets[5].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[5] = sets[5].drop(labels=['Date', 'id', 'Usage'], axis=1)\n",
        "\n",
        "sets_hate_only = [s[s['subtask_a'] == 'OFF'] for s in sets]\n",
        "\n",
        "for s in sets: # one can change to sets_hate_only\n",
        "  training_dataset = training_dataset.append(s)\n",
        "\n",
        "sets_hate_only\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Preprocess additional datasets\\n\\nsets[0] = sets[0].rename(columns={ 'label': 'subtask_a' })\\nsets[0]['subtask_a'] = sets[0].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[0] = sets[0].drop('id', axis=1)\\n\\nsets[1] = sets[1].rename(columns={ 'comment_text': 'tweet', 'label': 'subtask_a' })\\nsets[1]['subtask_a'] = sets[1].apply(lambda row: labels_set1(row), axis=1)\\nsets[1] = sets[1].drop(labels=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\\n\\nsets[2] = sets[2].rename(columns={ 'label': 'subtask_a' })\\nsets[2]['subtask_a'] = sets[2].apply(lambda row: labels_set2(row), axis=1)\\nsets[2] = sets[2].drop(labels=['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'Unnamed: 0'], axis=1)\\n\\nsets[3] = sets[3].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[3]['subtask_a'] = sets[3].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[3] = sets[3].drop('Date', axis=1)\\n\\nsets[4] = sets[4].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[4]['subtask_a'] = sets[4].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[4] = sets[4].drop(labels=['Date', 'Usage'], axis=1)\\n\\nsets[5] = sets[5].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[5]['subtask_a'] = sets[5].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[5] = sets[5].drop(labels=['Date', 'id', 'Usage'], axis=1)\\n\\nsets_hate_only = [s[s['subtask_a'] == 'OFF'] for s in sets]\\n\\nfor s in sets: # one can change to sets_hate_only\\n  training_dataset = training_dataset.append(s)\\n\\nsets_hate_only\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73i5BO8TqoTE",
        "colab_type": "text"
      },
      "source": [
        "# **Training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xHoiMYYlJQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 13\n",
        "\n",
        "# prepare training examples\n",
        "training_examples_A = training_dataset['tweet'][training_dataset['subtask_a'].notnull()]\n",
        "training_examples_B = training_dataset['tweet'][training_dataset['subtask_b'].notnull()]\n",
        "training_examples_C = training_dataset['tweet'][training_dataset['subtask_c'].notnull()]\n",
        "\n",
        "# prepare test examples and labels\n",
        "test_examples_A = testing_dataset_examples_A['tweet'][testing_dataset_examples_A['tweet'].notnull()]\n",
        "test_labels_A = (testing_dataset_labels_A['label'][testing_dataset_labels_A['label'].notnull()] == 'OFF').astype(int)\n",
        "\n",
        "# prepare training labels\n",
        "training_labels_A = (training_dataset['subtask_a'][training_dataset['subtask_a'].notnull()] == 'OFF').astype(int)\n",
        "training_labels_B = (training_dataset['subtask_b'][training_dataset['subtask_b'].notnull()] == 'TIN').astype(int)\n",
        "c_mapping = {'IND': 0, 'GRP': 1, 'OTH': 2}\n",
        "training_labels_C = training_dataset['subtask_c'][training_dataset['subtask_c'].notnull()].replace(c_mapping)\n",
        "\n",
        "# split training set into training and validation\n",
        "training_examples_A, validation_examples_A, training_labels_A, validation_labels_A = train_test_split(\n",
        "    training_examples_A, training_labels_A, test_size=0.1, stratify=training_labels_A, random_state=seed)\n",
        "training_examples_B, validation_examples_B, training_labels_B, validation_labels_B = train_test_split(\n",
        "    training_examples_B, training_labels_B, test_size=0.1, stratify=training_labels_B, random_state=seed)\n",
        "training_examples_C, validation_examples_C, training_labels_C, validation_labels_C = train_test_split(\n",
        "    training_examples_C, training_labels_C, test_size=0.1, stratify=training_labels_C, random_state=seed)\n",
        "\n",
        "training_x = np.array(training_examples_A)\n",
        "validation_x = np.array(validation_examples_A)\n",
        "training_y = np.array(training_labels_A)\n",
        "validation_y = np.array(validation_labels_A)\n",
        "test_x = np.array(test_examples_A)\n",
        "test_y = np.array(test_labels_A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bNpHc9qMpw",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fao-Vte1gtJ2",
        "colab_type": "text"
      },
      "source": [
        "### Common preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re0mCq1ogs6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags if exist\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    stripped_text = soup.get_text(separator=' ')\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "# remove unnecessary whitespaces\n",
        "def remove_whitespace(text):\n",
        "    text = text.strip()\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# remove accented chars (e.g. caffè -> caffe)\n",
        "def remove_accented_chars(text):\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# remove hashes and split words (e.g. '#fortTrump' -> 'fort trump')\n",
        "def split_hashtags(text):\n",
        "    splitted = text.split()\n",
        "    new_word_sequence = []\n",
        "\n",
        "    for chunk in splitted:\n",
        "        if chunk[0] == '#':\n",
        "            chunk = chunk[1:]\n",
        "            new_word_sequence.extend(wordsegment.segment(chunk))\n",
        "        else:\n",
        "            new_word_sequence.append(chunk)\n",
        "        \n",
        "    return ' '.join(tuple(new_word_sequence))\n",
        "\n",
        "\n",
        "def substitute_emojis(text):\n",
        "    demojized_text = emoji.demojize(text)\n",
        "    return re.compile('[_:]+').sub(' ', demojized_text)\n",
        "\n",
        "\n",
        "def preprocess_common(text):\n",
        "    text = strip_html_tags(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = split_hashtags(text)\n",
        "    text = substitute_emojis(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Gjm9OhOzug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove redundant @user tokens\n",
        "def remove_redundant_users(example):\n",
        "    user_count = 0\n",
        "    new_example = example[:]\n",
        "    for i, token in reversed(list(enumerate(example))):\n",
        "        if token == '@user':\n",
        "            user_count += 1\n",
        "        if user_count > 3:\n",
        "            new_example.pop(i)\n",
        "    else:\n",
        "        user_count = 0\n",
        "\n",
        "    return new_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJMq-8ygixr",
        "colab_type": "text"
      },
      "source": [
        "### Spacy preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPYd37I_giKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try leaving '?' and '!' as far as punctuation is concerned\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# exclude negation words from spacy stopwords list\n",
        "deselect_stop_words = ['no', 'not', 'noone', 'none', 'lacks', 'lack', 'nor', 'never', 'neighter', 'hardly', 'nobody', 'nothing', 'lacking', 'nowhere']\n",
        "for w in deselect_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    clean_text = []\n",
        "    \n",
        "    for token in doc:\n",
        "        flag = True\n",
        "        edit = token.text\n",
        "        '''\n",
        "        # remove punctuations\n",
        "        if token.pos_ == 'PUNCT' and flag == True and token.text != '@user': \n",
        "            flag = False\n",
        "       \n",
        "        # remove special characters\n",
        "        if token.pos_ == 'SYM' and flag == True: \n",
        "            flag = False\n",
        "        \n",
        "        # remove numbers\n",
        "        if (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n",
        "            flag = False\n",
        "\n",
        "        # correct misspelings\n",
        "        # if flag == True:\n",
        "            # suggestions = sym_spell.lookup(edit, Verbosity.TOP, 2)\n",
        "            # if len(suggestions) > 0:\n",
        "                # edit = suggestions[0].term\n",
        "\n",
        "        # remove stop words\n",
        "        if token.is_stop and token.pos_ != 'NUM': \n",
        "            flag = False\n",
        "\n",
        "        # convert tokens to base form\n",
        "        elif token.lemma_ != '-PRON-' and flag == True:\n",
        "            edit = token.lemma_\n",
        "        '''\n",
        "        # append tokens edited and not removed to list \n",
        "        if edit != '' and flag == True:\n",
        "            clean_text.append(edit)        \n",
        "    \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyyCkwWhFw1",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAHp6jlH12sD",
        "colab_type": "code",
        "outputId": "ac77e2dd-36f3-410c-aa4e-99fbd5af901e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "source": [
        "cleaned_x = [preprocess_spacy(example) for example in training_x[0:30]]\n",
        "reduced_users_x = [remove_redundant_users(example) for example in cleaned_x]\n",
        "print(reduced_users_x[0:30])\n",
        "print(training_x[0:30])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['@USER', 'This', 'one', 'is', 'just', 'cursed', 'like', 'this', '😟', 'why', 'not', 'report', 'her', 'to', 'the', 'police', '.', '🙄', '🤐', 'if', 'this', 'is', 'her', 'mum', 'she', 'is', 'very', 'very', 'very', 'stupid'], ['@USER', '@USER', '@USER', 'The', 'historic', 'moniker', 'for', 'the', 'Tories', 'is', 'The', 'Stupid', 'Party', '.', '  ', 'American', 'conservatives', 'test', 'out', 'with', 'lower', 'average', 'intelligence', 'than', 'liberals', '.'], ['Second', 'choice', 'always', 'wins', '🙂', '🙂', 'I', '’m', 'betting', 'I', '’ll', 'feel', 'headache', 'and', 'dizziness', 'as', 'soon', 'as', 'it', 'becomes', '5:30', 'am'], ['@USER', '@USER', 'Fuck', '@USER', 'and', 'everything', 'he', '’s', 'about', ',', 'including', '@USER', ',', 'Criminal', 'News', 'Network'], ['Life', '’s', 'shit', ',', 'the', 'earth', 'is', 'shit', 'and', 'everything', 'is', 'shit', 'unless', 'you', '’re', 'rich', '.', 'Wo', 'n’t', 'be', 'told', 'otherwise', 'because', 'it', '’s', 'strictly', 'facts'], ['@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', 'She', 'is', 'jus', 'here', 'to', 'observe', '.'], ['How', 'can', 'you', 'ever', 'be', 'rewarded', 'if', 'you', 'do', 'not', 'take', 'a', 'risk', '?', ' ', 'Stagnation', 'is', 'boring', ',', 'submission', 'is', 'bliss', '.', '  ', '#', 'femdom', '#', 'findom'], ['ahhhhhhhhhhhh', 'this', 'week', 'was', 'too', 'long'], ['@USER', 'Well', 'if', 'there', 'was', 'a', 'general', 'election', 'these', 'mps', 'would', 'be', 'voted', 'out', 'simple'], ['@USER', 'I', 'guess', 'even', 'you', 'have', 'limits', '.', '🤣', '😉'], ['@USER', 'It', \"'s\", 'a', 'very', 'tiny', 'problem', ',', 'but', 'can', 'you', 'add', 'the', 'buy', 'by', 'tens', 'option', '?'], ['I', 'hope', 'all', 'this', 'falls', 'through', 'I', 'need', 'to', 'start', 'making', 'bigger', 'moves'], ['@USER', '@USER', '@USER', '@USER', 'honestly', 'it', '’s', 'hard', 'being', 'right', 'all', 'the', 'time', '😇'], ['the', 'level', 'of', 'discourse', 'in', 'my', 'house', 'includes', 'arguments', 'about', 'exactly', 'how', 'Kurt', 'Vonnegut', 'depicts', 'genitals'], ['@USER', 'Right', ',', 'he', 'changed', 'terry', '’s', 'mind', '.', 'Terry', 'should', '’ve', 'stuck', 'with', 'his', 'gut', 'and', 'managed', '.', 'You', 'just', 'say', 'no'], ['Whenever', 'I', 'listen', 'to', 'The', 'Ramones', 'I', '’m', 'brought', 'back', 'to', '14', 'years', 'old', 'and', 'discovering', 'them', 'for', 'the', 'first', 'time', '.', 'It', '’s', 'a', 'very', 'cathartic', 'experience', '.'], ['SAINTS', ',', 'go', 'shit', 'as', 'manager', 'of', 'the', 'house', 'all', 'to', 'ourselves', 'sure', 'they', 'do', 'and', 'we', 'ai', \"n't\", 'never'], ['omg', 'the', 'cashier', 'at', 'uniqlo', 'was', 'so', 'cool', 'and', 'then', 'i', 'saw', 'he', 'had', 'a', 'koya', 'keychain', 'and', 'we', \"'re\", 'best', 'friends', 'now'], ['I', 'hear', 'there', \"'s\", 'a', 'chance', 'of', 'an', 'afterlife', 'I', 'may', 'not', 'be', 'able', 'to', 'get', 'in', 'but', 'at', 'least', 'I', 'wo', \"n't\", 'be', 'living'], ['@USER', 'have', 'you', 'seen', 'a', 'girl', 'with', 'a', 'sword', '?', 'drop', 'all', 'those', 'gadgets', 'of', 'yours', 'and', 'i', \"'ll\", 'remain', 'standing', '.'], ['If', 'I', 'got', 'ta', 'damn', 'near', 'threaten', 'you', 'for', 'you', 'to', 'listen', ',', 'then', 'we', 'got', 'ta', 'cut', 'ties', '.'], ['@USER', 'Boohoo', 'to', 'make', 'the', 'girls', 'pu', '-', 'uke'], ['@USER', 'billy', 'woods', 'is', 'great', '!', 'Love', 'his', 'work', 'in', 'Armand', 'Hammer', 'with', 'Elucid', ',', 'too', '(', 'and', 'Elucid', '’s', 'solo', 'stuff', ')'], ['These', 'people', 'def', 'have', 'a', 'mental', 'illness', '.', 'My', 'co', 'workers', 'and', 'the', 'customers', '.'], ['Riley', 'Dean', 'or', 'Patton', ',', 'which', 'do', 'you', 'prefer', '?', '@USER'], ['@USER', 'Papi', \"'s\", 'the', 'greatest', '.'], ['@USER', '@USER', 'Thanks', 'for', 'the', 'amazing', 'times', 'and', 'memories', '.', 'Good', 'luck', '!'], ['@USER', '@USER', 'Is', 'n’t', 'that', 'what', 'Bill', 'O’Reilly', 'did', '...', 'and', 'never', 'came', 'back', '?', 'Here', '’s', 'hoping', 'Tucker', 'goes', 'the', 'same', 'way', '.'], ['@USER', 'Nah', 'i', 'think', 'she', 'looks', 'fire', 'i', 'just', 'would', 'nt', 'bc', 'i', 'would', 'nt', 'feel', 'as', 'comfortable', 'performing', 'the', 'task', 'i', 'was', 'paid', 'for', 'lol'], ['I', 'know', 'bitches', 'be', 'talking', 'bout', 'me', '😂', 'I', 'also', 'know', 'bitches', 'ai', 'n’t', 'goin', 'say', 'shit', 'to', 'my', 'face', 'tho', '😂', 'FACTS']]\n",
            "['@USER This one is just cursed like this 😟 why not report her to the police. 🙄🤐 if this is her mum she is very very very stupid'\n",
            " '@USER @USER @USER The historic moniker for the Tories is The Stupid Party.   American conservatives test out with lower average intelligence than liberals.'\n",
            " 'Second choice always wins 🙂🙂 I’m betting I’ll feel headache and dizziness as soon as it becomes 5:30 am'\n",
            " '@USER @USER Fuck @USER and everything he’s about, including @USER, Criminal News Network'\n",
            " 'Life’s shit, the earth is shit and everything is shit unless you’re rich. Won’t be told otherwise because it’s strictly facts'\n",
            " '@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER She is jus here to observe.'\n",
            " 'How can you ever be rewarded if you do not take a risk?  Stagnation is boring, submission is bliss.   #femdom #findom'\n",
            " 'ahhhhhhhhhhhh this week was too long'\n",
            " '@USER Well if there was a general election these mps would be voted out simple'\n",
            " '@USER I guess even you have limits. 🤣😉'\n",
            " \"@USER It's a very tiny problem, but can you add the buy by tens option?\"\n",
            " 'I hope all this falls through I need to start making bigger moves'\n",
            " '@USER @USER @USER @USER honestly it’s hard being right all the time😇'\n",
            " 'the level of discourse in my house includes arguments about exactly how Kurt Vonnegut depicts genitals'\n",
            " '@USER Right, he changed terry’s mind. Terry should’ve stuck with his gut and managed. You just say no'\n",
            " 'Whenever I listen to The Ramones I’m brought back to 14 years old and discovering them for the first time. It’s a very cathartic experience.'\n",
            " \"SAINTS, go shit as manager of the house all to ourselves sure they do and we ain't never\"\n",
            " \"omg the cashier at uniqlo was so cool and then i saw he had a koya keychain and we're best friends now\"\n",
            " \"I hear there's a chance of an afterlife I may not be able to get in but at least I won't be living\"\n",
            " \"@USER have you seen a girl with a sword? drop all those gadgets of yours and i'll remain standing.\"\n",
            " 'If I gotta damn near threaten you for you to listen, then we gotta cut ties.'\n",
            " '@USER Boohoo to make the girls pu-uke'\n",
            " '@USER billy woods is great! Love his work in Armand Hammer with Elucid, too (and Elucid’s solo stuff)'\n",
            " 'These people def have a mental illness . My co workers and the customers.'\n",
            " 'Riley Dean or Patton , which do you prefer? @USER'\n",
            " \"@USER Papi's the greatest.\"\n",
            " '@USER @USER Thanks for the amazing times and memories. Good luck !'\n",
            " '@USER @USER Isn’t that what Bill O’Reilly did ... and never came back? Here’s hoping Tucker goes the same way.'\n",
            " '@USER Nah i think she looks fire i just wouldnt bc i wouldnt feel as comfortable performing the task i was paid for lol'\n",
            " 'I know bitches be talking bout me 😂 I also know bitches ain’t goin say shit to my face tho 😂 FACTS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btAs2RDT9uoS",
        "colab_type": "text"
      },
      "source": [
        "# **BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp5AzW_zilHm",
        "colab_type": "text"
      },
      "source": [
        "### BERT Preprocessing - single example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3woANjZiltJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_bert_before_tokenization(text):\n",
        "    preprocessed_text = preprocess_common(text)\n",
        "    spacy_x = preprocess_spacy(text)\n",
        "    cleaned_x = remove_redundant_users(spacy_x)\n",
        "    return ' '.join(cleaned_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkqKH8ssRuFI",
        "colab_type": "text"
      },
      "source": [
        "### Getting the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxg4a3Y97Mt",
        "colab_type": "code",
        "outputId": "606677eb-2810-4658-f00a-8eb467d7b8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
        "!unzip wwm_uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-04 11:18:15--  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1248381879 (1.2G) [application/zip]\n",
            "Saving to: ‘wwm_uncased_L-24_H-1024_A-16.zip.3’\n",
            "\n",
            "wwm_uncased_L-24_H- 100%[===================>]   1.16G  84.5MB/s    in 16s     \n",
            "\n",
            "2020-03-04 11:18:32 (75.3 MB/s) - ‘wwm_uncased_L-24_H-1024_A-16.zip.3’ saved [1248381879/1248381879]\n",
            "\n",
            "Archive:  wwm_uncased_L-24_H-1024_A-16.zip\n",
            "replace wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  A\n",
            "\n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlzqsoCERraJ",
        "colab_type": "text"
      },
      "source": [
        "### Building a tf.Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOyKrgZRRqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "    def bert_module_fn(is_training):\n",
        "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
        "\n",
        "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "        config = BertConfig.from_json_file(config_path)\n",
        "        model = BertModel(config=config, is_training=is_training,\n",
        "                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
        "          \n",
        "        seq_output = model.all_encoder_layers[-1]\n",
        "        pool_output = model.get_pooled_output()\n",
        "\n",
        "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "        lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "            \n",
        "        input_map = {\"input_ids\": input_ids,\n",
        "                     \"input_mask\": input_mask,\n",
        "                     \"segment_ids\": token_type}\n",
        "        \n",
        "        output_map = {\"pooled_output\": pool_output,\n",
        "                      \"sequence_output\": seq_output}\n",
        "\n",
        "        output_info_map = {\"vocab_file\": vocab_file,\n",
        "                           \"do_lower_case\": lower_case}\n",
        "                \n",
        "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "    return bert_module_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubGdlgtTsjW1",
        "colab_type": "text"
      },
      "source": [
        "### Exporting the module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fShtfnSQbO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = \"wwm_uncased_L-24_H-1024_A-16\"\n",
        "\n",
        "config_path = \"/content/gdrive/My Drive/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/gdrive/My Drive/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "\n",
        "module_fn = build_module_fn(config_path, vocab_path)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/gdrive/My Drive/{}/bert_model.ckpt\".format(MODEL_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgf8hzNYMst",
        "colab_type": "text"
      },
      "source": [
        "### Building the text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMkUX6C5V3r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(str_list):\n",
        "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
        "    unique_id = 0\n",
        "    for s in str_list:\n",
        "        line = convert_to_unicode(s)\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        text_a = line.strip()\n",
        "        splitted = re.split(r'[.?!]\\s*', text_a)\n",
        "        text_b = splitted[-1]\n",
        "        text_a.replace(text_b, '')\n",
        "\n",
        "\n",
        "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n",
        "        unique_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmsepoRxVsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.input_type_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_061naESlic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_preprocessor(voc_path, seq_len, lower=True):\n",
        "    tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
        "\n",
        "    def strings_to_arrays(sents):\n",
        "\n",
        "        sents = np.atleast_1d(sents).reshape((-1,))\n",
        "\n",
        "        examples = []\n",
        "        for example in read_examples(sents):\n",
        "            example.text_a = preprocess_bert_before_tokenization(example.text_a)\n",
        "            examples.append(example)\n",
        "\n",
        "        features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
        "        arrays = features_to_arrays(features)\n",
        "        \n",
        "        return arrays\n",
        "\n",
        "    return strings_to_arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycbxDVlalim",
        "colab_type": "text"
      },
      "source": [
        "### Implementing a BERT Keras layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIIv5wCKUkgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
        "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
        "                 tune_embeddings=False, trainable=True, **kwargs):\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.n_tune_layers = n_tune_layers\n",
        "        self.tune_embeddings = tune_embeddings\n",
        "        self.do_preprocessing = do_preprocessing\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.seq_len = seq_len\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "\n",
        "        self.var_per_encoder = 16\n",
        "        if self.pooling not in [\"cls\", \"mean\", \"lstm\", None]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
        "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
        "\n",
        "        trainable_layers = []\n",
        "        if self.tune_embeddings:\n",
        "            trainable_layers.append(\"embeddings\")\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            trainable_layers.append(\"pooler\")\n",
        "\n",
        "        if self.n_tune_layers > 0:\n",
        "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
        "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
        "            for i in range(self.n_tune_layers):\n",
        "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
        "        \n",
        "        # Add module variables to layer's trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if any([l in var.name for l in trainable_layers]):\n",
        "                self._trainable_weights.append(var)\n",
        "            else:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        self.build_preprocessor()\n",
        "        self.initialize_module()\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def build_abspath(self, path):\n",
        "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
        "          return path\n",
        "        else:\n",
        "          return os.path.abspath(path)\n",
        "\n",
        "    def build_preprocessor(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
        "\n",
        "    def initialize_module(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        \n",
        "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
        "                                     for var in self.bert.variables])\n",
        "\n",
        "        uninitialized = []\n",
        "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
        "            if not is_initialized:\n",
        "                uninitialized.append(var)\n",
        "\n",
        "        if len(uninitialized):\n",
        "            sess.run(tf.variables_initializer(uninitialized))\n",
        "\n",
        "    def call(self, input):\n",
        "        if self.do_preprocessing:\n",
        "          input = tf.numpy_function(self.preprocessor, \n",
        "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
        "                                    name='preprocessor')\n",
        "          for feature in input:\n",
        "            feature.set_shape((None, self.seq_len))\n",
        "        \n",
        "        input_ids, input_mask, segment_ids = input\n",
        "        \n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "\n",
        "            if self.pooling == \"lstm\":\n",
        "              lstm = tf.keras.layers.LSTM(1)\n",
        "              pooled = lstm(result)\n",
        "\n",
        "            else:\n",
        "              input_mask = tf.cast(input_mask, tf.float32)\n",
        "              mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "              masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                      tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "              \n",
        "              if self.pooling == \"mean\":\n",
        "                pooled = masked_reduce_mean(result, input_mask)\n",
        "              else:\n",
        "                pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dict = {\n",
        "            \"bert_path\": self.bert_path, \n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"n_tune_layers\": self.n_tune_layers,\n",
        "            \"tune_embeddings\": self.tune_embeddings,\n",
        "            \"do_preprocessing\": self.do_preprocessing,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "        super(BertLayer, self).get_config()\n",
        "        return config_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIjYf1EL_nQA",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQLsvYzaysD",
        "colab_type": "code",
        "outputId": "08375a16-cc51-43b7-cd12-2bbd3ea72638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=200, tune_embeddings=False,\n",
        "                    pooling=\"cls\", n_tune_layers=24, verbose=False)\n",
        "\n",
        "l1 = tf.keras.layers.Dense(768, activation='relu')\n",
        "l2 = tf.keras.layers.Dense(768, activation='relu')\n",
        "l3 = tf.keras.layers.Dense(768, activation='relu')\n",
        "d = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(encoder(inp))\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DmN-x8-FgAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall + tf.keras.backend.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH-6kHUmZ7Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                            np.unique(training_y),\n",
        "                                            training_y)\n",
        "class_weights /= max(class_weights)\n",
        "\n",
        "# print(class_weights) # [majority class weight, minority class weight]\n",
        "\n",
        "minority_count = len(training_labels_A[training_labels_A == 1])\n",
        "majority_count = len(training_labels_A[training_labels_A == 0])\n",
        "\n",
        "def weighted_loss(actual, predicted):\n",
        "    weights = class_weights\n",
        "    # weights = [0.0714, 1.0]\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    loss = bce(actual, predicted)\n",
        "   \n",
        "    return tf.keras.backend.mean(loss * weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuy1OMCbGTw",
        "colab_type": "code",
        "outputId": "e90139e2-1645-4db5-9951-973b241bad1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=7e-6, ),\n",
        "      loss=weighted_loss,\n",
        "      metrics=[f1_m])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer (BertLayer)       (None, 1024)              335141888 \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 335,142,913\n",
            "Trainable params: 303,360,001\n",
            "Non-trainable params: 31,782,912\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7AbjTBifTl",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbagCxaieoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a2cppA_UDg5W",
        "outputId": "95d044ea-faa5-4534-f486-f4d97e535cc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "saver = keras.callbacks.ModelCheckpoint(\"bert_large_H8_S256_B32.hdf5\")\n",
        "\n",
        "history = model.fit(training_x, training_y, validation_data=[validation_x, validation_y], batch_size=16, epochs=1, callbacks=[saver])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 101916 samples, validate on 11324 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/extract_features.py:283: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  9568/101916 [=>............................] - ETA: 2:04:39 - loss: 0.1838 - f1_m: 0.7020"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwOSC395hUmY",
        "colab_type": "text"
      },
      "source": [
        "### Loss function graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-ROTbohUXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mM9p7JRErm4",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJNgkArdBAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = model.predict(test_x, batch_size=16)\n",
        "\n",
        "model_emb = tf.keras.models.Model(inputs=[model.layers[0].input], outputs=[model.layers[-3].output])\n",
        "data = model_emb.predict(training_x)\n",
        "np.savez('bert-embeddings.mpz', data3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkP3H-PCEwxP",
        "colab_type": "text"
      },
      "source": [
        "### Count f1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDV8k9M4kBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def round_int(array, threshold):\n",
        "  return np.array([np.ceil(x) if x >= threshold else np.floor(x) for x in array]).astype(np.float64)\n",
        "\n",
        "flat_predicted = round_int(predicted[:, 0], 0.5)\n",
        "print(f1_score(test_y, flat_predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rivD55Qc85D",
        "colab_type": "text"
      },
      "source": [
        "## Saving and restoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix02jwj_u_64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json.dump(model.to_json(), open(\"bert_H4_S256_B64.json\", \"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWsbzSaeSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.model_from_json(json.load(open(\"bert_H4_S256_B64.json\")), \n",
        "                                        custom_objects={\"BertLayer\": BertLayer})\n",
        "\n",
        "model.load_weights(\"bert_H5_S256_B64.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7H2AY9MsvUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(training_x[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_RWAQ-Um9AT",
        "colab_type": "text"
      },
      "source": [
        "### Freeze model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg5crVOofJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
        "\n",
        "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes a Keras model into a pruned computation graph.\n",
        "\n",
        "    @param model The Keras model to be freezed.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    \n",
        "    sess = tf.keras.backend.get_session()\n",
        "    graph = sess.graph\n",
        "    \n",
        "    with graph.as_default():\n",
        "\n",
        "        input_tensors = model.inputs\n",
        "        output_tensors = model.outputs\n",
        "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
        "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
        "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
        "        \n",
        "        tmp_g = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in tmp_g.node:\n",
        "                node.device = \"\"\n",
        "        \n",
        "        tmp_g = optimize_for_inference(\n",
        "            tmp_g, input_ops, output_ops, dtypes, False)\n",
        "        \n",
        "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
        "        \n",
        "        if export_path is not None:\n",
        "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
        "                f.write(tmp_g.SerializeToString())\n",
        "        \n",
        "        return tmp_g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMUrFpgofMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph_4_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShmlWdXLcH80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/gaphex/bert_experimental/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/bert_experimental\")\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.graph_ops import load_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKRLyiXvrqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "restored_graph = load_graph(\"frozen_graph_4_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8bT-YXrgo6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_ops = restored_graph.get_operations()\n",
        "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
        "print(input_op, output_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHr2ZQGfg3y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
        "y = restored_graph.get_tensor_by_name(output_op + ':0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoapr4e86Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQFQOML7ivdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRf75n6ECUa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session(graph=restored_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw55NB6YseBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = sess.run(y, feed_dict={\n",
        "        x: training_x[:10].reshape((-1,1))\n",
        "    })\n",
        "\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vAiiJEnfYpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}