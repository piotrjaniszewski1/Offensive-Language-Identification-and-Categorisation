{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c72-N0RGo1z",
        "colab_type": "code",
        "outputId": "c8c4e056-aa06-4c87-9d23-bffe82fd79b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZvrBG6DXhkA",
        "colab_type": "code",
        "outputId": "11201eb9-8386-4f43-e862-1b62f6afea6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd gdrive/My\\ Drive"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ds8aZ8OFzw8",
        "colab_type": "code",
        "outputId": "ff0a3e0f-338b-430f-f10c-cb512af53905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "# Install necessary packages -> uncomment what is currently needed\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install contractions\n",
        "!pip install wordsegment\n",
        "!pip install -U symspellpy\n",
        "!pip install emoji --upgrade\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install bert-for-tf2\n",
        "!pip install transformers\n",
        "# !pip install nltk"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.24)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already up-to-date: symspellpy in /usr/local/lib/python3.6/dist-packages (6.5.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.5)\n",
            "Requirement already up-to-date: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.17.5)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.13.5)\n",
            "Requirement already satisfied: py-params>=0.7.3 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: params-flow>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.7.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7864AKjDrMsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All imports - DO NOT CHANGE THE ORDER OF INSTRUCTIONS\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "    sys.path.insert(0, 'bert_repo')\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode\n",
        "import contractions\n",
        "import gensim.downloader as api\n",
        "import re\n",
        "import wordsegment\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import emoji\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf2\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from modeling import BertModel, BertConfig\n",
        "from tokenization import FullTokenizer, convert_to_unicode\n",
        "from extract_features import InputExample, convert_examples_to_features\n",
        "from tqdm import tqdm\n",
        "#import tensorflow_addons as tfa\n",
        "# import nltk\n",
        "from google.colab import auth, drive\n",
        "# nltk.download('punkt')\n",
        "\n",
        "wordsegment.load()\n",
        "\n",
        "# Load SymSpell -> package for correcting misspellings\n",
        "sym_spell = SymSpell(2, 7)\n",
        "\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "# get TF logger \n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mER1mPUtiaPl",
        "colab_type": "code",
        "outputId": "bd2e1a90-7d84-4c9e-eadc-05c4e644af35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "#Import data\n",
        "training_examples_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/olid-training-v1.0.tsv'\n",
        "testing_examples_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/testset-levela.tsv'\n",
        "testing_labels_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/labels-levela.csv'\n",
        "\n",
        "training_dataset = pd.read_csv(training_examples_url, delimiter='\\t')\n",
        "testing_dataset_examples_A = pd.read_csv(testing_examples_A_url, delimiter='\\t')\n",
        "testing_dataset_labels_A = pd.read_csv(testing_labels_A_url, delimiter=',')\n",
        "\n",
        "print(training_dataset.head())\n",
        "training_dataset.size"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJNUAx_OiYP7",
        "colab_type": "text"
      },
      "source": [
        "## Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xDBjuNciVDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Additional datasets\n",
        "set_urls = [\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set1.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set2.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set3.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set4.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set5.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set6.csv',\n",
        "]\n",
        "\n",
        "sets = [pd.read_csv(url) for url in set_urls]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCDhsVxMLnLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labels_set1(row):\n",
        "  return 'OFF' if 1 in [row['toxic'], row['severe_toxic'], row['obscene'], row['threat'], row['insult'], row['identity_hate']] else 'NOT'\n",
        "\n",
        "def labels_set2(row):\n",
        "  return 'OFF' if row['class'] != 2 else 'NOT'\n",
        "\n",
        "def labels_set_ordinary(row):\n",
        "  return 'OFF' if row['subtask_a'] == 1 else 'NOT'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0uTpIMiVih",
        "colab_type": "code",
        "outputId": "180fb9a8-b5f9-4385-f272-c6cac43054a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Preprocess additional datasets\n",
        "\n",
        "sets[0] = sets[0].rename(columns={ 'label': 'subtask_a' })\n",
        "sets[0]['subtask_a'] = sets[0].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[0] = sets[0].drop('id', axis=1)\n",
        "\n",
        "sets[1] = sets[1].rename(columns={ 'comment_text': 'tweet', 'label': 'subtask_a' })\n",
        "sets[1]['subtask_a'] = sets[1].apply(lambda row: labels_set1(row), axis=1)\n",
        "sets[1] = sets[1].drop(labels=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "\n",
        "sets[2] = sets[2].rename(columns={ 'label': 'subtask_a' })\n",
        "sets[2]['subtask_a'] = sets[2].apply(lambda row: labels_set2(row), axis=1)\n",
        "sets[2] = sets[2].drop(labels=['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'Unnamed: 0'], axis=1)\n",
        "\n",
        "sets[3] = sets[3].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[3]['subtask_a'] = sets[3].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[3] = sets[3].drop('Date', axis=1)\n",
        "\n",
        "sets[4] = sets[4].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[4]['subtask_a'] = sets[4].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[4] = sets[4].drop(labels=['Date', 'Usage'], axis=1)\n",
        "\n",
        "sets[5] = sets[5].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[5]['subtask_a'] = sets[5].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[5] = sets[5].drop(labels=['Date', 'id', 'Usage'], axis=1)\n",
        "\n",
        "sets_hate_only = [s[s['subtask_a'] == 'OFF'] for s in sets]\n",
        "\n",
        "for s in sets: # one can change to sets_hate_only\n",
        "  training_dataset = training_dataset.append(s)\n",
        "\n",
        "sets_hate_only"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  sort=sort,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[      subtask_a                                              tweet\n",
              " 13          OFF  @user #cnn calls #michigan middle school 'buil...\n",
              " 14          OFF  no comment!  in #australia   #opkillingbay #se...\n",
              " 17          OFF                             retweet if you agree! \n",
              " 23          OFF    @user @user lumpy says i am a . prove it lumpy.\n",
              " 34          OFF  it's unbelievable that in the 21st century we'...\n",
              " ...         ...                                                ...\n",
              " 31934       OFF  lady banned from kentucky mall. @user  #jcpenn...\n",
              " 31946       OFF  @user omfg i'm offended! i'm a  mailbox and i'...\n",
              " 31947       OFF  @user @user you don't have the balls to hashta...\n",
              " 31948       OFF   makes you ask yourself, who am i? then am i a...\n",
              " 31960       OFF  @user #sikh #temple vandalised in in #calgary,...\n",
              " \n",
              " [2242 rows x 2 columns],\n",
              "                                                     tweet subtask_a\n",
              " 6            COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK       OFF\n",
              " 12      Hey... what is it..\\n@ | talk .\\nWhat is it......       OFF\n",
              " 16      Bye! \\n\\nDon't look, come or think of comming ...       OFF\n",
              " 42      You are gay or antisemmitian? \\n\\nArchangel WH...       OFF\n",
              " 43               FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!       OFF\n",
              " ...                                                   ...       ...\n",
              " 159494  \"\\n\\n our previous conversation \\n\\nyou fuckin...       OFF\n",
              " 159514                  YOU ARE A MISCHIEVIOUS PUBIC HAIR       OFF\n",
              " 159541  Your absurd edits \\n\\nYour absurd edits on gre...       OFF\n",
              " 159546  \"\\n\\nHey listen don't you ever!!!! Delete my e...       OFF\n",
              " 159554  and i'm going to keep posting the stuff u dele...       OFF\n",
              " \n",
              " [16225 rows x 2 columns],\n",
              "                                                    tweet subtask_a\n",
              " 1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...       OFF\n",
              " 2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...       OFF\n",
              " 3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...       OFF\n",
              " 4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...       OFF\n",
              " 5      !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...       OFF\n",
              " ...                                                  ...       ...\n",
              " 24776                                 you're all niggers       OFF\n",
              " 24777  you're such a retard i hope you get type 2 dia...       OFF\n",
              " 24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...       OFF\n",
              " 24780  young buck wanna eat!!.. dat nigguh like I ain...       OFF\n",
              " 24781              youu got wild bitches tellin you lies       OFF\n",
              " \n",
              " [20620 rows x 2 columns],\n",
              "      subtask_a                                              tweet\n",
              " 0          OFF                               \"You fuck your dad.\"\n",
              " 7          OFF  \"shut the fuck up. you and the rest of your fa...\n",
              " 8          OFF  \"Either you are fake or extremely stupid...may...\n",
              " 9          OFF  \"That you are an idiot who understands neither...\n",
              " 15         OFF  \"FOR SOME REASON U SOUND RETARDED. LOL. DAMN. ...\n",
              " ...        ...                                                ...\n",
              " 3929       OFF  \"Why don't you shut your stupid mouth, pennyla...\n",
              " 3931       OFF  \"You goofy Repub thugs don't have a clue, not ...\n",
              " 3934       OFF  \"True, maybe he's deliberately being an assh*l...\n",
              " 3935       OFF  \"JoelWeltman, you look like a pedophile from y...\n",
              " 3942       OFF  \"you are both morons and that is never happening\"\n",
              " \n",
              " [1049 rows x 2 columns],\n",
              "      subtask_a                                              tweet\n",
              " 2          OFF  \"Agree with Alan you are an extremest idiot.  ...\n",
              " 8          OFF  \"To engage in an intelligent debate with you i...\n",
              " 23         OFF  \"GO SUCK_YOU DADDYS_COCK_OBAMA LOVING LIBERAL_...\n",
              " 27         OFF  \"i live with my dad asshole!!! i anal fuked th...\n",
              " 29         OFF  \"You really think shes speaking spanish? You a...\n",
              " ...        ...                                                ...\n",
              " 2628       OFF  \"Another dumb comment. Put America first befor...\n",
              " 2629       OFF  \"Go use google simpleton, that's how the word ...\n",
              " 2637       OFF  \"Obviously the comic books you read have done ...\n",
              " 2640       OFF  \"Give up jamal jrk off, I would never possibly...\n",
              " 2645       OFF  \"Leave your email or phone number and maybe yo...\n",
              " \n",
              " [693 rows x 2 columns],\n",
              "      subtask_a                                              tweet\n",
              " 1          OFF              \"you're idiot.......................\"\n",
              " 2          OFF  \"I am a woman Babs, and the only \"war on women...\n",
              " 3          OFF  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...\n",
              " 4          OFF  \"haha green me red you now loser whos winning ...\n",
              " 6          OFF  \"Oh go kiss the ass of a goat....and you DUMMY...\n",
              " ...        ...                                                ...\n",
              " 2227       OFF                 \"What a smug basket you are.\\xa0 \"\n",
              " 2228       OFF  \"Hey, Nils, you are silent about Muslims killi...\n",
              " 2229       OFF  \"\\xa0You must have crack in your bong, as it a...\n",
              " 2231       OFF  \"YOU SHUT YOUR IGNORANT PIE HOLE YOU LITTLE IN...\n",
              " 2233       OFF  \"ball4real where are you with your miami g-ayn...\n",
              " \n",
              " [1077 rows x 2 columns]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73i5BO8TqoTE",
        "colab_type": "text"
      },
      "source": [
        "# **Training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xHoiMYYlJQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 13\n",
        "\n",
        "# prepare training examples\n",
        "training_examples_A = training_dataset['tweet'][training_dataset['subtask_a'].notnull()]\n",
        "training_examples_B = training_dataset['tweet'][training_dataset['subtask_b'].notnull()]\n",
        "training_examples_C = training_dataset['tweet'][training_dataset['subtask_c'].notnull()]\n",
        "\n",
        "# prepare test examples and labels\n",
        "test_examples_A = testing_dataset_examples_A['tweet'][testing_dataset_examples_A['tweet'].notnull()]\n",
        "test_labels_A = (testing_dataset_labels_A['label'][testing_dataset_labels_A['label'].notnull()] == 'OFF').astype(int)\n",
        "\n",
        "# prepare training labels\n",
        "training_labels_A = (training_dataset['subtask_a'][training_dataset['subtask_a'].notnull()] == 'OFF').astype(int)\n",
        "training_labels_B = (training_dataset['subtask_b'][training_dataset['subtask_b'].notnull()] == 'TIN').astype(int)\n",
        "c_mapping = {'IND': 0, 'GRP': 1, 'OTH': 2}\n",
        "training_labels_C = training_dataset['subtask_c'][training_dataset['subtask_c'].notnull()].replace(c_mapping)\n",
        "\n",
        "# split training set into training and validation\n",
        "training_examples_A, validation_examples_A, training_labels_A, validation_labels_A = train_test_split(\n",
        "    training_examples_A, training_labels_A, test_size=0.1, stratify=training_labels_A, random_state=seed)\n",
        "training_examples_B, validation_examples_B, training_labels_B, validation_labels_B = train_test_split(\n",
        "    training_examples_B, training_labels_B, test_size=0.1, stratify=training_labels_B, random_state=seed)\n",
        "training_examples_C, validation_examples_C, training_labels_C, validation_labels_C = train_test_split(\n",
        "    training_examples_C, training_labels_C, test_size=0.1, stratify=training_labels_C, random_state=seed)\n",
        "\n",
        "training_x = np.array(training_examples_A)\n",
        "validation_x = np.array(validation_examples_A)\n",
        "training_y = np.array(training_labels_A)\n",
        "validation_y = np.array(validation_labels_A)\n",
        "test_x = np.array(test_examples_A)\n",
        "test_y = np.array(test_labels_A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bNpHc9qMpw",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fao-Vte1gtJ2",
        "colab_type": "text"
      },
      "source": [
        "### Common preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re0mCq1ogs6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags if exist\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    stripped_text = soup.get_text(separator=' ')\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "# remove unnecessary whitespaces\n",
        "def remove_whitespace(text):\n",
        "    text = text.strip()\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# remove accented chars (e.g. caffè -> caffe)\n",
        "def remove_accented_chars(text):\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# remove hashes and split words (e.g. '#fortTrump' -> 'fort trump')\n",
        "def split_hashtags(text):\n",
        "    splitted = text.split()\n",
        "    new_word_sequence = []\n",
        "\n",
        "    for chunk in splitted:\n",
        "        if chunk[0] == '#':\n",
        "            chunk = chunk[1:]\n",
        "            new_word_sequence.extend(wordsegment.segment(chunk))\n",
        "        else:\n",
        "            new_word_sequence.append(chunk)\n",
        "        \n",
        "    return ' '.join(tuple(new_word_sequence))\n",
        "\n",
        "\n",
        "def substitute_emojis(text):\n",
        "    demojized_text = emoji.demojize(text)\n",
        "    return re.compile('[_:]+').sub(' ', demojized_text)\n",
        "\n",
        "\n",
        "def preprocess_common(text):\n",
        "    text = strip_html_tags(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = split_hashtags(text)\n",
        "    text = substitute_emojis(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Gjm9OhOzug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove redundant @user tokens\n",
        "def remove_redundant_users(example):\n",
        "    user_count = 0\n",
        "    new_example = example[:]\n",
        "    for i, token in reversed(list(enumerate(example))):\n",
        "        if token == '@user':\n",
        "            user_count += 1\n",
        "        if user_count > 3:\n",
        "            new_example.pop(i)\n",
        "    else:\n",
        "        user_count = 0\n",
        "\n",
        "    return new_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJMq-8ygixr",
        "colab_type": "text"
      },
      "source": [
        "### Spacy preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPYd37I_giKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try leaving '?' and '!' as far as punctuation is concerned\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# exclude negation words from spacy stopwords list\n",
        "deselect_stop_words = ['no', 'not', 'noone', 'none', 'lacks', 'lack', 'nor', 'never', 'neighter', 'hardly', 'nobody', 'nothing', 'lacking', 'nowhere']\n",
        "for w in deselect_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    clean_text = []\n",
        "    \n",
        "    for token in doc:\n",
        "        flag = True\n",
        "        edit = token.text\n",
        "\n",
        "        # remove punctuations\n",
        "        if token.pos_ == 'PUNCT' and flag == True and token.text != '@user': \n",
        "            flag = False\n",
        "       \n",
        "        # remove special characters\n",
        "        if token.pos_ == 'SYM' and flag == True: \n",
        "            flag = False\n",
        "        \n",
        "        # remove numbers\n",
        "        if (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n",
        "            flag = False\n",
        "\n",
        "        # correct misspelings\n",
        "        # if flag == True:\n",
        "            # suggestions = sym_spell.lookup(edit, Verbosity.TOP, 2)\n",
        "            # if len(suggestions) > 0:\n",
        "                # edit = suggestions[0].term\n",
        "\n",
        "        # remove stop words\n",
        "        if token.is_stop and token.pos_ != 'NUM': \n",
        "            flag = False\n",
        "\n",
        "        # convert tokens to base form\n",
        "        elif token.lemma_ != '-PRON-' and flag == True:\n",
        "            edit = token.lemma_\n",
        "\n",
        "        # append tokens edited and not removed to list \n",
        "        if edit != '' and flag == True:\n",
        "            clean_text.append(edit)        \n",
        "    \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyyCkwWhFw1",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAHp6jlH12sD",
        "colab_type": "code",
        "outputId": "e290e637-10ea-4aa0-bce2-4e4562a83a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "cleaned_x = [preprocess_spacy(example) for example in training_x[0:30]]\n",
        "reduced_users_x = [remove_redundant_users(example) for example in cleaned_x]\n",
        "print(reduced_users_x[0:30])\n",
        "print(training_x[0:30])"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['republicans', 'MASSACHUSETTS', 'GOVERNORSHIP', 'Coakley', 'Conceds', 'Gov.', 'Race', 'Charlie', 'baker&#8230;.', 'http://t.co/unfdqcqhyk'], ['stop', 'continue', 'delete', 'edit', 'legitimate', 'talk', 'page', 'comment', 'block', 'vandalism', ' ', '♠', 'dil'], ['Somebody', 'incompetent', 'station', 'yes', 'know', 'bit', 'military', 'absolutely', 'love', 'demonstrate', 'knowledge', 'hand', 'pussy', 'CID', 'unit', 'friend'], ['\\n', 'need', 'permission', 'undo', ' ', 'not', 'choose', 'route', 'try', 'clear', 'not', 'follow', 'example', 'revert', 'express', 'frustration', 'disdain', 'editor', 'view', ' ', 'talk', ' '], ['dance', 'pop', ' ', 'not', 'rock', 'fuck', 'stupid', 'article', ' ', 'base', 'Wiki', 'page', 'biased', 'source', ' ', 'care', 'source', 'mean', 'true', ' ', 'sky', 'pink', 'reliable', 'source', 'say', ' '], ['vandalize', '\\n\\n', 'vandalize', 'article', 'Lord', 'Rings', 'afraid', 'go', 'warning', ' ', 'violent', 'behavior', 'receive', 'warning', ' ', 'Punk'], ['gues', '\\n\\n', 'dirty', 'little', 'cock', 'sucking', 'slut'], ['February', 'UTC', '\\n\\n', 'Civil', 'way', 'verbally', 'fine', 'form', 'civility', 'suggest', 'continue', 'obdurate', 'ignorance', 'act', 'malice', ' '], ['hello', 'like', 'know', 'flag', 'article', 'deletion'], ['try', 'find', 'secondary', 'source', 'chance', ' ', 'day', 'message', ' ', 'thank', 'think', 'appreciate', ' ', 'talk'], ['Headline', 'text', '\\n', 'Cameron', 'Ward', 'dj', '\\n', 'Bold', 'textcameron', 'Ward', 'little', 'known', 'radio', 'dj', 'year', 'age', ' ', 'start', 'career', 'Millside', 'hospital', 'radio', 'member', 'Jan', 'Jules', \"'\", 'team', 'broadcast', 'thursday', 'night', ' ', 'strike', 'solo', 'career', 'move', 'ashfield', 'community', 'radio', 'different', 'show', 'year', ' ', 'Saturday', 'sport', 'broadcast', 'saturday', 'LIVE', 'MID', 'morning', 'SATURDAY', 'SUNHDAY', 'depene', 'day', 'midday', ' ', 'Finaly', 'team', 'nina', 'squire', 'Super', 'Sports', 'Saturday', 'p.m.', 'p.m.', ' ', 'leave', 'ashfield', 'comunity', 'radio', 'late', 'Regency', 'FM', 'broadcast', 'internet', ' ', 'run', 'saturday', 'morning', 'Saturday', 'Revolution', ' ', 'grow', 'cult', 'follow', 'north', 'east', 'derbyshire', 'bangor', 'Wales', ' ', 'sadly', 'Regency', 'Radio', 'close', 'lack', 'fund', ' ', 'speculate', 'Cameron', 'ashfield', 'community', 'radio', 'Trust', 'Fm', 'aquire', 'FM', 'licence', 'broadcast', 'chesterfield', 'north', 'east', 'derbyshire', '\\n\\n ', 'Headline', 'text', '\\n', 'Saturday', 'Revolution', '\\n', 'Bold', 'text', ' ', 'saturday', 'Revolution', 'main', 'feature', ' ', 'sport', 'date', 'Feel', 'good', 'factor', 'film', 'Phone', 'abroad', 'revolution', 'rewind', ' ', 'feature', 'come', 'band', 'Clay', 'cross', 'chesterfield', ' ', 'Comedy', 'Joel', 'regular', 'guest', 'cameron', 'usualy', 'good', 'listen', ' ', 'guest', 'Tyrone', 'elton', 'help', 'dart', 'song', 'selecta', 'Timy', 'mac', 'musician', 'Lewis', 'martin', 's', 'mum', 'help', 'cameron', 'decide', 'mum', 'mother', 'day', ' ', 'people', 'involve', 'Dave', 'Sadler', 'producer', 'Matt', 'King', 'film', 'reviewer', 'Joe', 'Lawson', 'Matts', 'replacement', 'sick', 'George', 'glossop', 'resident', 'gambler'], ['\\n\\n', 'sure', 'look', ' ', 'collectonian', '  ', 'talk', '  ', 'contribs'], ['\\n', 'no', 'establish', 'username', 'use', 'ip', 'point', 'small', 'business', 'IP', 'multiple', 'user', 'site', 'day', 'right', 'not', 'compromise', 'action', 'bad', 'apple', ' '], ['retain', 'evidence', 'face'], ['defend', 'woman', 'condescension', 'turn', 'defend', 'woman', 'verbally', 'assail', 'fâ\\x80', '¦'], ['city', '#', 'rome', '#', 'beautiful', '  ', 'sky', 'dawn', 'cloud', 'color', 'city', 'amazing', '#', 'landscape', '¦'], ['renew', 'edit', 'war', 'Circumcision', '\\n\\n', 'warn', 'February', 'follow', 'BRD', 'start', 'discussion', 'consensus', 'edit', 'add', 'maybe', 'reach', 'compromise', 'try'], ['&', 'look', 'nip', 'ring', 'wait', 'change', '&', '&', 'lmao'], ['happy', 'white', 'kwanzaa', '#', 'holiday', 'african', '#', 'whitesupremacist', 'celebrate', 'happykwanzaa', 'happyholiday', ' ', 'merrychristma'], ['\\n\\n ', 'barnstar', '\\n\\n  ', 'Socratic', 'Barnstar', 'discussion', 'underway', 'want', 'appreciation', 'research', 'diplomacy', 'contribute', 'discussion', 'Kurds', 'article', 'clarity', 'eloquence', 'articulate', 'position', 'proposal', 'never', 'editor', 'like', 'help', 'project', '  ', '—talk', 'hist'], ['\\n\\n', 'death', 'critic', '\\n\\n', 'instead', ' ', 'earlier', 'give', 'isolated', 'case', 'Theo', 'van', 'Gogh', 'Vincent', 'van', 'Gogh', 'Rashad', 'Khalifa', 'Irshad', 'Manji', 'receive', 'death', 'threat', 'kill', 'anonymous', 'unknown', 'radical', ' ', 'happen', 'claim', 'prove', 'case', 'Islam', 'sanction', 'death', 'sentence', 'critic', 'post', 'name', 'isolate', 'case', 'people', 'receive', 'death', 'threat', 'anonymous', 'radical', 'need', 'post', 'name', 'prominent', 'islamic', 'scholar', 'university', 'Al', 'Azhar', 'not', 'isolate', 'radical', 'issue', 'death', 'sentence', 'people', 'fatwa', ' ', 'Rushdie', 'reject', 'Al', 'Azhar', 'country', 'Iran', 'need', 'post', 'islamic', 'site', 'not', 'anti', '-', 'islamic', 'site', 'isolate', 'radical', 'site', 'issue', 'death', 'sentence', 'Menji', 'Rashad', 'Khalifa', 'Theo', 'van', 'Gogh', 'etc', 'bring', 'proof', 'throw', 'name', 'isolate', 'case', 'receive', 'death', 'threat', 'isolated', 'anonymous', 'radical', 'prove', 'Islam', 'sanction', 'death', 'sentence', 'critic', 'Muslims', 'mosque', 'receive', 'threat', 'Muslims', 'Sikhs', 'kill', 'mean', 'sanction', 'death', 'american', 'Muslims', ' ', 'pathetic', 'joke', 'dude', ' ', 'post', 'isolate', 'case', 'people', 'receive', 'death', 'threat', 'anonymous', 'radical', 'prove', 'thing', 'know', 'sure', 'kill', 'Rashad', 'add', 'need', 'post', 'name', 'muslim', 'scholar', 'issue', 'death', 'sentence', 'Rashad', 'Vincent', 'van', 'Gogg', 'Menji', 'need', 'post', 'evidence', 'consensus', 'muslim', 'scholar', 'case', ' ', 'case', 'refute', ' ', 'Nov', 'UTC', '\\n\\n ', 'prove', 'case', 'Islam', 'sanction', 'death', 'sentence', 'critic', 'post', 'name', 'isolate', 'case', 'people', 'receive', 'death', 'threat', 'anonymous', 'radical', ' ', 'need', 'post', 'name', 'prominent', 'islamic', 'scholar', 'university', 'Al', 'Azhar', 'not', 'isolate', 'radical', 'issue', 'death', 'sentence', 'people', 'fatwa', ' ', 'Rushdie', 'reject', 'Al', 'Azhar', 'country', '<', 'anonymous', 'radical', 'represent', 'sentiment', 'feel', 'faction', 'ISlam', '-they', 'represent', 'interpretation', 'Islam', 'provide', 'example', 'critic', 'murder', 'MUHAMMAD', 'Al', 'Azhar', 'not', 'high', 'authority', 'Muhammad', '\\n\\n', 'provide', 'story', 'Ibn', 'Ishaq', 'Muhammad', 'reject', 'Muslims', 'reject', ' ', 'Al', 'Azhar', 'Muslim', 'scholar', 'interpret', 'Muhammad', 'islamic', 'law', 'say', 'not', 'anti', '-', 'islamic', 'bigot', 'like', 'bold', 'claim', 'Islam', 'critic', 'kill', 'refute', 'give', 'example', 'contradict', 'claim', 'critic', 'receive', 'death', 'sentence', 'refute', 'post', 'list', 'isolated', 'critic', 'Rushdie', 'Menji', 'Rashad', 'Khalifa', 'Theo', 'van', 'Gogh', 'etc', 'receive', 'death', 'threat', 'anonymous', 'radical', 'like', 'post', 'long', 'list', 'Muslims', 'Sikhs', 'receive', 'death', 'threat', 'kill', 'claim', 'american', 'law', 'allow', 'Muslims', 'kill', 'refute', ' ', 'need', 'provide', 'name', 'prominent', 'know', 'muslim', 'scholar', 'not', 'anti', '-', 'Islam', 'site', 'isolated', 'radical', 'case', 'prove', 'issue', 'death', 'sentence', 'need', 'provide', 'proof', 'judgement', 'consensus', 'Muslims', 'scholar', 'refute', 'anti', '-', 'islamic', 'bigot', 'claim', 'Muhammad', 'islamic', 'law', 'say', 'expect', 'claim', 'insert', 'Encyclopedia', 'need', 'provide', 'proof', 'wrong', 'try', 'dude', 'fail', 'miserably', ' ', 'Dec', 'UTC', '\\n\\n', 'Furthermore', 'whe', 'opinon', 'secular', 'government', 'irrelevant', 'course', 'Musilm', 'country', 'reject', 'death', 'fatwa', 'Rushdie', 'international', 'politic', 'not', 'religion', 'state', 'Ayatollah', 'Khomenie', 'Shiah', 'CLERIC', 'way', 'Ayatollah', 'bear', 'aside', 'dictator', 'IRan', 'Khomenie', 'spiritial', 'religious', 'leader', 'huge', 'number', \"sha'ia\", 'world', 'Khomenie', 'write', 'fatwa', 'book', 'Islam', 'famous', 'fatwa', 'course', 'call', 'murder', 'Salman', 'Rushdie', 'Al', 'Azhar', 'sunni', 'university', ' ', 'SUPPOSEDLY', 'reject', 'KHomenei', 'teaching', 'not', 'mean', 'mean', 'prove', 'Al', 'Azhar', 'right', 'Khomenie', 'wrong', 'fact', 'KHomenie', 'shiah', 'cleric', 'Al', 'Azhar', 'sunni', 'university', ' ', 'gape', 'hole', 'argument', 'no', 'ev'], ['\\n\\n ', 'California', 'Gold', 'Rush', '\\n\\n', 'welcome', 'Wikipedia', ' ', 'revert', 'change', 'California', 'Gold', 'Rush', 'substantially', 'alter', 'article', ' ', 'article', 'Wikipedia', 'featured', 'article', 'drastic', 'change', 'propose', 'article', 'talk', 'page', ' ', 'thank', 'talk'], ['Template', 'Charles', 'F.', 'Adams', 'class', 'destroyer', '\\n\\n', 'notice', 'work', ' ', 'template', ' ', 'look', 'great', ' ', 'Kudos', 'hard', 'work', 'make', 'bland', 'template', 'look', 'nice'], ['RT', '@leechee420', 'go', 'GNC', 'today', 'cashier', 'bitch', 'sell', 'chocolate', 'cake', 'write', 'face', 'looke&#8230'], ['@valerie91097', 'uhh', 'messi', 'trash', 'be', 'flatter', 'game', 'scorre'], ['agree', 'term', '‘', 'blastocyst', '’', 'appear', 'base', 'false', 'distinction', 'draw', '‘', 'embryo', 'proper', 'embryo', 'early', 'stage', 'development', 'term', 'pre', '-', 'embryo', 'context', 'precise', 'stage', 'embryo', 'classify', 'blastocyst', 'pre', '-', 'embryo', 'involve', 'somewhat', 'subjective', 'judgement', 'article', 'currently', 'say', '\\n      \\n', 'blastocyst', 'stage', 'development', 'embryo', 'day', 'old', 'cell', '\\n\\n', 'highly', 'subjective', 'nature', 'term', 'bastocyst', 'pre', '-', 'embryo', 'problematic', 'criticise', 'try', 'dehumanize', 'human', 'embryo'], ['not', 'currently', 'employee', 'Loblaws', 'hear', 'local', 'Independent', 'Grocer', 'store', 'own', 'Loblaws', 'close', 'time', 'staff', 'work', 'year', 'offer', 'job', 'new', 'real', 'Canadian', 'Superstore', 'start', 'wage', 'actually', 'impossible', 'employee', 'find', 'job', 'Loblaws', 'store', 'phase', 'replace', 'RCS', 'store', 'store', 'Kanata', 'closed', 'replace', 'rcs', 'store'], ['resta', 'love', '  ', '#', 'good', 'regard', 'future'], ['Venues', '\\n\\n', 'list', 'stadium', ' ', 'remove'], [' ', '@user', '  ', '#', 'wednesday', 'question', 'biking', 'need', 'want', 'check', 'page']]\n",
            "['REPUBLICANS TAKE MASSACHUSETTS GOVERNORSHIP: Coakley Conceds Gov. Race To Charlie Baker&#8230;. http://t.co/unfdQCQHyK'\n",
            " 'Please stop. If you continue to delete or edit legitimate talk page comments, you will be blocked for vandalism.  ♠Dil'\n",
            " \"Somebody too incompetent to be stationed anywhere else. Yes I do know quite a bit about the military and I would absolutely love to demonstrate that knowledge with you first-hand. I wasn't in some pussy CID unit friend.\"\n",
            " '\"\\nI don\\'t need permission to undo it.  I did not choose that route because I am trying to make it clear I will not follow your example for you of using reverts to express frustration or disdain for another editor\\'s view.  talk  \"'\n",
            " \"It's dance-pop, it should be done.  It's not rock at all, so fuck that stupid article.  Why are we basing Wiki pages around biased sources?  Who cares if it's sourced, it doesn't mean it's true.  The sky isn't pink just because a reliable source says it is.  71.59.189.46\"\n",
            " \"Please Do Not Vandalize! \\n\\nIf you vandalize another article as you did to Lord of the Rings, I'm afraid I'm going to have to give you another warning.  If you keep up this violent behavior you just might receive...your last warning.  Punk!\"\n",
            " 'GUESS WHAT?\\n\\nYOUR STILL A DIRTY LITTLE COCK-SUCKING SLUT!'\n",
            " ', 28 February 2007 (UTC)\\n\\nCivil? In what way? Verbally? Fine. But there are other forms of civility, and I would suggest that continued, obdurate ignorance is itself an act of malice.  15:05'\n",
            " 'hello. i would like to know why you have flagged my article for deletion'\n",
            " \"I'll try to find some secondary sources when I get a chance.  If I don't get to it in a day or so, message me again.  Thanks for thinking of me for this, much appreciated.  (talk) .\"\n",
            " \"Headline text \\nCameron Ward DJ\\nBold textCameron Ward is a little known radio DJ who is only of 16 years of age.  He first started his career on Millside hospital radio as a member of the 'Jan and Jules' show team and broadcasted every thursday night from 6pm-9pm.  He then struck out a solo career and moved to ashfield community radio where he had three different shows over 2 years.  These were 'Saturday sport' which broadcast saturdays 5pm-7pm, 'LIVE MID MORNING SATURDAY/SUNHDAY' (depened which day it was) between 9am and 12 midday.  Finaly he teamed up with nina squires to do the 'Super Sports Saturday Show' between 2pm and 6pm.  He then left ashfield comunity radio in late 2005 to move to 'Regency FM' which broadcast on the internet.  There he ran a show on saturday mornings 10am-1pm 'The Saturday Revolution Show',  Which grew a cult following across north east derbyshire and bangor in Wales.  Then sadly Regency Radio closed down due to lack of funds.  It is speculated Cameron will move back to ashfield community radio or to Trust Fm which has now aquired a FM licence to broadcast across chesterfield and north east derbyshire. \\n\\n Headline text \\nThe Saturday Revolution!\\nBold text  The saturday Revolution had five main features.  These were the sports up date, the Feel good factor, the film top ten, Phone call abroad and the revolution rewind.  The show also featured up and coming bands from around Clay cross and chesterfield.  Comedy Joel was a regular guest on the show and he and cameron usualy made for a good listen.  The show had other guests Tyrone elton who helped out with the dart song selecta, Timy mac who was a musician, Lewis martin whos mum helped cameron to decide what to get his mum for mothers day.  Other people involved with the show were: Dave Sadler (the producer), Matt King (the film reviewer), Joe Lawson ( Matts replacement when he was sick) and George glossop the resident gambler.\"\n",
            " '\"\\n\\nSure, I\\'ll take a look.  Collectonian\\xa0(talk\\xa0· contribs) \"'\n",
            " '\"\\nThere are no other \"\"established usernames\"\" that use this IP, and that\\'s beside the point. This is a small business IP, and we have multiple users at this site every day. The rights of all should not be compromised by the actions of one bad apple.  \"'\n",
            " 'I have retained all the evidences to show it in your face.'\n",
            " 'you\\'re defending a woman against \"condescension\", who was in turn defending a woman that was verbally assailed in fâ\\x80¦ '\n",
            " 'my city #rome #beautiful   #sky #dawn #clouds #colors #city #amazing #landscape #shadowsâ\\x80¦ '\n",
            " \"Renewed edit warring at Circumcision \\n\\nYou were warned in February about this, yet you're doing it again. Follow BRD and start a discussion. If you can get a consensus for your edit, then, and only then, can you add it. Maybe you can reach a compromise? Give it a try.\"\n",
            " \"&#8220;@__Bino: Looking at nip rings I can't wait until I can change mine &#128527;&#8221; &#128221;&#128064;&#128209; lmao\"\n",
            " '#happy #white #kwanzaa, the #holiday only #african #whitesupremacists #celebrate. #happykwanzaa #happyholidays  #merrychristmas'\n",
            " '\"\\n\\n A barnstar for you! \\n\\n  The Socratic Barnstar Although the discussion is still underway, I wanted to show you my appreciation for the research and diplomacy that you have already contributed to the discussion on the Kurds article, as well as the clarity and eloquence with which you articulated your position and proposal. We could never have enough editors like yourself helping on the project. Keep it up!\\xa0—talk/hist \"'\n",
            " '\" \\n\\ndeath to critics\\n\\nSo instead of  \\'every\\' earlier, now you are giving isolated cases, such as Theo van Gogh, Vincent van Gogh, Rashad Khalifa, and Irshad Manji who received death threats or were killed by anonymous unknown radicals.  What happened to every claim? Anyway, to prove your case that Islam sanctions death sentence to its critics, you can\\'t just post the names of isolated cases where people received death threats from anonymous radicals! You need to post the names of prominent Islamic scholars and universities such as Al Azhar (not isolated radicals) who issued the death sentence against these people. The fatwa  against Rushdie was rejected by Al Azhar and all countries except Iran. So all you need to do is post Islamic sites (not anti-Islamic sites and isolated radical sites) who issued the death sentence against Menji, Rashad Khalifa, Theo van Gogh, etc. Bring your proof. Just throwing the names of isolated cases where someone received death threats from isolated anonymous radical doesn\\'t prove Islam sanctions death sentence to it\\'s critics! After 9/11, many Muslims and mosques received threats and some Muslims/Sikhs were even killed. That doesn\\'t mean the US sanctions death to all American Muslims.  You are such a pathetic joke, dude!  Posting isolated cases where people received death threats from anonymous radicals doesn\\'t prove a thing. We don\\'t even know for sure who killed Rashad. You added even his name. You need to post the names of Muslim scholars who issues death sentence against, say, Rashad, Vincent van Gogg, and Menji. Then you need to post evidence that this was consensus among Muslim scholars on each of these cases.  Without that, you don\\'t have any case, and you are refuted once again  09:33, 30 Nov 2004 (UTC)\\n\\n \"\"Anyway, to prove your case that Islam sanctions death sentence to its critics, you can\\'t just post the names of isolated cases where people received death threats from anonymous radicals!  You need to post the names of prominent Islamic scholars and universities such as Al Azhar (not isolated radicals) who issued the death sentence against these people. The fatwa  against Rushdie was rejected by Al Azhar and all countries\"\" <- these anonymous radicals represent the sentiments felt by their factions in ISlam -they represent their interpretation of Islam. I also provided examples of critics who were murdered by MUHAMMAD himself. Al-Azhar is not a higher authority than Muhammad.\\n\\nNo, you have only provided two stories in Ibn Ishaq about Muhammad that are rejected by Muslims, such as rejected here.  Al Azhar and Muslim scholars will interpret what Muhammad did and what Islamic law says, not an anti-Islamic bigot like you. You made a bold claim that in Islam critics are to be to be killed. I refuted that by giving examples that contradict that. You then claimed that every critic received a death sentence. You were refuted again. You then posted a list of isolated critics such as Rushdie, Menji, Rashad Khalifa, Theo van Gogh, etc, who received death threats from anonymous radicals. That\\'s like posting (a much longer list) of Muslims/Sikhs who received death threats or were killed after 9/11, and claiming that American law allows Muslims to be killed. You were refuted again.  You need to provide names of prominent well known Muslim scholars (not anti-Islam sites or isolated radicals) for each of these cases and prove that they issued the death sentence. After that, you need to provide proof that that judgement was a consensus among Muslims scholars. You didn\\'t do that. You were refuted once again. Since you are an anti-Islamic bigot, you can\\'t just claim that Muhammad did this or that or Islamic law says this or that and expect that claim to be inserted in Encyclopedia. You need to provide proof. You couldn\\'t because you were wrong. Try again, dude. You have failed quite miserably here  03:40, 1 Dec 2004 (UTC)\\n\\nFurthermore, whe opinons of secular governments are irrelevant - of course Musilm countries rejected the death fatwa against Rushdie, but this was due to international politics, not due to religion. Also, as stated before, Ayatollah Khomenie was a Shiah CLERIC. The only way to be an Ayatollah is to be born as one. Aside from being the dictator or IRan, Khomenie was also the spiritial and religious leader of huge numbers of Sha\\'ias around the world. Khomenie wrote many fatwas and books about Islam. His most famous fatwa, of course, calls for the murder of Salman Rushdie. Now Al-Azhar - a SUNNI university  SUPPOSEDLY rejected KHomenei\\'s teachings. This does not mean by any means prove that Al-Azhar was right, and Khomenie was wrong. The fact that KHomenie was a shiah cleric, while Al-Azhar is a sunni university is also a  gaping hole in this argument. You have no ev'\n",
            " '\"\\n\\n California Gold Rush \\n\\nWelcome to Wikipedia.  I reverted your changes to the California Gold Rush because they substantially altered the article.  This article is one of Wikipedia\\'s featured articles and therefore any drastic changes should be proposed on the article talk page.  Thank you. (talk) \"'\n",
            " 'Template:Charles F. Adams class destroyer \\n\\nJust noticed your work on the  template.  Looks great!  Kudos to you for your hard work in making this otherwise very bland template look so nice!'\n",
            " 'RT @leechee420: I went to GNC today and the cashier had \"bitch, we don\\'t sell chocolate cake here\" written all over her face when she looke&#8230;'\n",
            " '@valerie91097 uhh messi trash . and im flattered but what was the games scorre'\n",
            " \"Agreed. The term ‘blastocyst’ appears to be based on a false distinction that has been drawn between ‘the embryo proper’ and the embryo at an earlier stage of development, sometimes the term ‘pre-embryo’ is used in this context. However the precise stage at which an embryo can be classified as a ‘blastocyst’ or a 'pre-embryo' seems to involve a somewhat subjective judgement. The article currently says:\\n      \\nA blastocyst is a stage of development of an embryo when it is around five days old and made up of about 100 cells.\\n\\nBecause of their highly subjective nature, terms such as bastocyst and pre-embryo are problematic and may be criticised for trying to dehumanize the human embryo.\"\n",
            " 'I am not currently an employee of Loblaws but I did hear that when a local Your Independent Grocer store (which are owned by Loblaws) was closed that full-time staff who had worked there for many years were offered a job at the new Real Canadian Superstore at a starting wage of $7 or $8. It would actually be quite impossible for all those employees to find jobs at other Loblaws stores as they are now being phased out and replaced with RCS stores. Another store in Kanata was just closed and replaced with an RCS store.'\n",
            " '* #resta #love   #best regards the future '\n",
            " \"Venues \\n\\nWasn't there a list of the stadiums being used?  Why was that removed?\"\n",
            " ' @user   #wednesday! questions or #biking needs/wants? check out our page! ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btAs2RDT9uoS",
        "colab_type": "text"
      },
      "source": [
        "# **BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp5AzW_zilHm",
        "colab_type": "text"
      },
      "source": [
        "### BERT Preprocessing - single example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3woANjZiltJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_bert_before_tokenization(text):\n",
        "    preprocessed_text = preprocess_common(text)\n",
        "    spacy_x = preprocess_spacy(preprocessed_text)\n",
        "    cleaned_x = remove_redundant_users(spacy_x)\n",
        "    return ' '.join(cleaned_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkqKH8ssRuFI",
        "colab_type": "text"
      },
      "source": [
        "### Getting the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxg4a3Y97Mt",
        "colab_type": "code",
        "outputId": "d9bf7829-e5db-4a0e-90e6-bd0c2b40ef01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
        "!unzip wwm_uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-02 00:07:10--  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.216.128, 2607:f8b0:400c:c12::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.216.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1248381879 (1.2G) [application/zip]\n",
            "Saving to: ‘wwm_uncased_L-24_H-1024_A-16.zip.6’\n",
            "\n",
            "wwm_uncased_L-24_H- 100%[===================>]   1.16G  47.2MB/s    in 25s     \n",
            "\n",
            "2020-03-02 00:07:35 (48.3 MB/s) - ‘wwm_uncased_L-24_H-1024_A-16.zip.6’ saved [1248381879/1248381879]\n",
            "\n",
            "Archive:  wwm_uncased_L-24_H-1024_A-16.zip\n",
            "replace wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlzqsoCERraJ",
        "colab_type": "text"
      },
      "source": [
        "### Building a tf.Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOyKrgZRRqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "    def bert_module_fn(is_training):\n",
        "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
        "\n",
        "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "        config = BertConfig.from_json_file(config_path)\n",
        "        model = BertModel(config=config, is_training=is_training,\n",
        "                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
        "          \n",
        "        seq_output = model.all_encoder_layers[-1]\n",
        "        pool_output = model.get_pooled_output()\n",
        "\n",
        "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "        lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "            \n",
        "        input_map = {\"input_ids\": input_ids,\n",
        "                     \"input_mask\": input_mask,\n",
        "                     \"segment_ids\": token_type}\n",
        "        \n",
        "        output_map = {\"pooled_output\": pool_output,\n",
        "                      \"sequence_output\": seq_output}\n",
        "\n",
        "        output_info_map = {\"vocab_file\": vocab_file,\n",
        "                           \"do_lower_case\": lower_case}\n",
        "                \n",
        "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "    return bert_module_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubGdlgtTsjW1",
        "colab_type": "text"
      },
      "source": [
        "### Exporting the module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fShtfnSQbO",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "04291e08-4aa9-43dc-8ece-944310da0b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        }
      },
      "source": [
        "MODEL_DIR = \"wwm_uncased_L-24_H-1024_A-16\"\n",
        "\n",
        "config_path = \"/content/gdrive/My Drive/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/gdrive/My Drive/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "\n",
        "module_fn = build_module_fn(config_path, vocab_path)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/gdrive/My Drive/{}/bert_model.ckpt\".format(MODEL_DIR))\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "From bert_repo/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "From bert_repo/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "From bert_repo/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "From bert_repo/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "From bert_repo/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_hub/saved_model_lib.py:110: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AlreadyExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-37edd383f014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_module_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_and_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags_and_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m spec.export(\"bert-module\", \n\u001b[0;32m---> 16\u001b[0;31m             checkpoint_path=\"/content/gdrive/My Drive/{}/bert_model.ckpt\".format(MODEL_DIR))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_spec.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, path, _sentinel, checkpoint_path, name_transform_fn)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing mandatory `checkpoint_path` parameter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mname_transform_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_transform_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mexport_module_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_transform_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36mexport_module_spec\u001b[0;34m(spec, path, checkpoint_path, name_transform_fn)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, path, session)\u001b[0m\n\u001b[1;32m    323\u001b[0m       raise RuntimeError(\"session graph differs from the graph where the \"\n\u001b[1;32m    324\u001b[0m                          \"module was instantiated.\")\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, path, session)\u001b[0m\n\u001b[1;32m    633\u001b[0m             write_state=False)\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(self, path, variables_saver)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \"\"\"\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mmodule_def_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_def_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/saved_model_lib.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, path, variables_saver)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0massets_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_assets_key_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_all_assets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massets_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/saved_model_lib.py\u001b[0m in \u001b[0;36m_save_all_assets\u001b[0;34m(self, path, assets_map)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mtf_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massets_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m \u001b[0;32min\u001b[0m \u001b[0massets_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m       \u001b[0mtf_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_save_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(oldpath, newpath, overwrite)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m   \"\"\"\n\u001b[0;32m--> 469\u001b[0;31m   \u001b[0mcopy_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moldpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mcopy_v2\u001b[0;34m(src, dst, overwrite)\u001b[0m\n\u001b[1;32m    484\u001b[0m   \"\"\"\n\u001b[1;32m    485\u001b[0m   pywrap_tensorflow.CopyFile(\n\u001b[0;32m--> 486\u001b[0;31m       compat.as_bytes(src), compat.as_bytes(dst), overwrite)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAlreadyExistsError\u001b[0m: file already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgf8hzNYMst",
        "colab_type": "text"
      },
      "source": [
        "### Building the text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMkUX6C5V3r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(str_list):\n",
        "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
        "    unique_id = 0\n",
        "    for s in str_list:\n",
        "        line = convert_to_unicode(s)\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        text_a = line.strip()\n",
        "        # splitted = re.split(r'[.?!]\\s*', text_a)\n",
        "        # text_b = splitted[-1]\n",
        "        # text_a.replace(text_b, '')\n",
        "\n",
        "\n",
        "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=None)\n",
        "        unique_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmsepoRxVsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.input_type_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_061naESlic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_preprocessor(voc_path, seq_len, lower=True):\n",
        "    tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
        "\n",
        "    def strings_to_arrays(sents):\n",
        "\n",
        "        sents = np.atleast_1d(sents).reshape((-1,))\n",
        "\n",
        "        examples = []\n",
        "        for example in read_examples(sents):\n",
        "            example.text_a = preprocess_bert_before_tokenization(example.text_a)\n",
        "            examples.append(example)\n",
        "\n",
        "        features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
        "        arrays = features_to_arrays(features)\n",
        "        \n",
        "        return arrays\n",
        "\n",
        "    return strings_to_arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycbxDVlalim",
        "colab_type": "text"
      },
      "source": [
        "### Implementing a BERT Keras layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIIv5wCKUkgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
        "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
        "                 tune_embeddings=False, trainable=True, **kwargs):\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.n_tune_layers = n_tune_layers\n",
        "        self.tune_embeddings = tune_embeddings\n",
        "        self.do_preprocessing = do_preprocessing\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.seq_len = seq_len\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "\n",
        "        self.var_per_encoder = 16\n",
        "        if self.pooling not in [\"cls\", \"mean\", \"lstm\", None]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
        "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
        "\n",
        "        trainable_layers = []\n",
        "        if self.tune_embeddings:\n",
        "            trainable_layers.append(\"embeddings\")\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            trainable_layers.append(\"pooler\")\n",
        "\n",
        "        if self.n_tune_layers > 0:\n",
        "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
        "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
        "            for i in range(self.n_tune_layers):\n",
        "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
        "        \n",
        "        # Add module variables to layer's trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if any([l in var.name for l in trainable_layers]):\n",
        "                self._trainable_weights.append(var)\n",
        "            else:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        self.build_preprocessor()\n",
        "        self.initialize_module()\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def build_abspath(self, path):\n",
        "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
        "          return path\n",
        "        else:\n",
        "          return os.path.abspath(path)\n",
        "\n",
        "    def build_preprocessor(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
        "\n",
        "    def initialize_module(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        \n",
        "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
        "                                     for var in self.bert.variables])\n",
        "\n",
        "        uninitialized = []\n",
        "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
        "            if not is_initialized:\n",
        "                uninitialized.append(var)\n",
        "\n",
        "        if len(uninitialized):\n",
        "            sess.run(tf.variables_initializer(uninitialized))\n",
        "\n",
        "    def call(self, input):\n",
        "        print('call')\n",
        "\n",
        "        if self.do_preprocessing:\n",
        "          input = tf.numpy_function(self.preprocessor, \n",
        "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
        "                                    name='preprocessor')\n",
        "          for feature in input:\n",
        "            feature.set_shape((None, self.seq_len))\n",
        "        \n",
        "        input_ids, input_mask, segment_ids = input\n",
        "        \n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "\n",
        "            if self.pooling == \"lstm\":\n",
        "              lstm = tf.keras.layers.LSTM(1)\n",
        "              pooled = lstm(result)\n",
        "\n",
        "            else:\n",
        "              input_mask = tf.cast(input_mask, tf.float32)\n",
        "              mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "              masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                      tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "              \n",
        "              if self.pooling == \"mean\":\n",
        "                pooled = masked_reduce_mean(result, input_mask)\n",
        "              else:\n",
        "                pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dict = {\n",
        "            \"bert_path\": self.bert_path, \n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"n_tune_layers\": self.n_tune_layers,\n",
        "            \"tune_embeddings\": self.tune_embeddings,\n",
        "            \"do_preprocessing\": self.do_preprocessing,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "        super(BertLayer, self).get_config()\n",
        "        return config_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIjYf1EL_nQA",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQLsvYzaysD",
        "colab_type": "code",
        "outputId": "22ec83be-6946-4246-8c5f-d467442177b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=200, tune_embeddings=False,\n",
        "                    pooling='lstm', n_tune_layers=24, verbose=False)\n",
        "\n",
        "l1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "l2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "# l3 = tf.keras.layers.Dense(1024, activation='relu')\n",
        "d = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(d(l2(l1(encoder(inp)))))\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "call\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DmN-x8-FgAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall + tf.keras.backend.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH-6kHUmZ7Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                            np.unique(training_y),\n",
        "                                            training_y)\n",
        "class_weights /= max(class_weights)\n",
        "\n",
        "# print(class_weights) # [majority class weight, minority class weight]\n",
        "\n",
        "minority_count = len(training_labels_A[training_labels_A == 1])\n",
        "majority_count = len(training_labels_A[training_labels_A == 0])\n",
        "\n",
        "def weighted_loss(actual, predicted):\n",
        "    weights = class_weights\n",
        "    # weights = [0.0714, 1.0]\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    loss = bce(actual, predicted)\n",
        "   \n",
        "    return tf.keras.backend.mean(loss * weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuy1OMCbGTw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "fef683d8-9355-467f-b1cc-fa946e7355fc"
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, ),\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=[f1_m])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer_1 (BertLayer)     (None, 1)                 335141888 \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               256       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 335,158,785\n",
            "Trainable params: 302,326,273\n",
            "Non-trainable params: 32,832,512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7AbjTBifTl",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbagCxaieoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a2cppA_UDg5W",
        "colab": {}
      },
      "source": [
        "saver = keras.callbacks.ModelCheckpoint(\"bert_large_H8_S256_B32.hdf5\")\n",
        "\n",
        "history = model.fit(training_x, training_y, validation_data=[validation_x, validation_y], batch_size=16, epochs=3, callbacks=[saver])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwOSC395hUmY",
        "colab_type": "text"
      },
      "source": [
        "### Loss function graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-ROTbohUXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mM9p7JRErm4",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJNgkArdBAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = model.predict(test_x, batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkP3H-PCEwxP",
        "colab_type": "text"
      },
      "source": [
        "### Count f1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDV8k9M4kBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def round_int(array, threshold):\n",
        "  return np.array([np.ceil(x) if x >= threshold else np.floor(x) for x in array]).astype(np.float64)\n",
        "\n",
        "flat_predicted = round_int(predicted[:, 0], 0.3)\n",
        "print(f1_score(test_y, flat_predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rivD55Qc85D",
        "colab_type": "text"
      },
      "source": [
        "## Saving and restoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix02jwj_u_64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json.dump(model.to_json(), open(\"bert_H4_S256_B64.json\", \"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWsbzSaeSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.model_from_json(json.load(open(\"bert_H4_S256_B64.json\")), \n",
        "                                        custom_objects={\"BertLayer\": BertLayer})\n",
        "\n",
        "model.load_weights(\"bert_H5_S256_B64.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7H2AY9MsvUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(training_x[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_RWAQ-Um9AT",
        "colab_type": "text"
      },
      "source": [
        "### Freeze model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg5crVOofJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
        "\n",
        "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes a Keras model into a pruned computation graph.\n",
        "\n",
        "    @param model The Keras model to be freezed.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    \n",
        "    sess = tf.keras.backend.get_session()\n",
        "    graph = sess.graph\n",
        "    \n",
        "    with graph.as_default():\n",
        "\n",
        "        input_tensors = model.inputs\n",
        "        output_tensors = model.outputs\n",
        "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
        "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
        "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
        "        \n",
        "        tmp_g = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in tmp_g.node:\n",
        "                node.device = \"\"\n",
        "        \n",
        "        tmp_g = optimize_for_inference(\n",
        "            tmp_g, input_ops, output_ops, dtypes, False)\n",
        "        \n",
        "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
        "        \n",
        "        if export_path is not None:\n",
        "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
        "                f.write(tmp_g.SerializeToString())\n",
        "        \n",
        "        return tmp_g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMUrFpgofMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph_4_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShmlWdXLcH80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/gaphex/bert_experimental/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/bert_experimental\")\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.graph_ops import load_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKRLyiXvrqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "restored_graph = load_graph(\"frozen_graph_4_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8bT-YXrgo6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_ops = restored_graph.get_operations()\n",
        "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
        "print(input_op, output_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHr2ZQGfg3y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
        "y = restored_graph.get_tensor_by_name(output_op + ':0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoapr4e86Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQFQOML7ivdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRf75n6ECUa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session(graph=restored_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw55NB6YseBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = sess.run(y, feed_dict={\n",
        "        x: training_x[:10].reshape((-1,1))\n",
        "    })\n",
        "\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vAiiJEnfYpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}