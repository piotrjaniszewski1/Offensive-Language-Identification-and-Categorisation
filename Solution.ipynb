{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lE03y8dqFT1M"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ds8aZ8OFzw8",
        "colab_type": "code",
        "outputId": "5c5cd0c4-8a75-4229-ddce-1c8187e2ca22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install necessary packages -> uncomment what is currently needed\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install contractions\n",
        "!pip install wordsegment\n",
        "!pip install -U symspellpy\n",
        "!pip install emoji --upgrade\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install bert-for-tf2\n",
        "!pip install transformers\n",
        "# !pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 21.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 3.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 5.4MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Building wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81707 sha256=28f77c8000126829fad0247f2a3ebe68632cca76fc80627af221f131fa88d405\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, textsearch, contractions\n",
            "Successfully installed contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Collecting wordsegment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/6c/e6f4734d6f7d28305f52ec81377d7ce7d1856b97b814278e9960183235ad/wordsegment-1.3.1-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: wordsegment\n",
            "Successfully installed wordsegment-1.3.1\n",
            "Collecting symspellpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/0b/2daa14bf1ed649fff0d072b2e51ae98d8b45cae6cf8fdda41be01ce6c289/symspellpy-6.5.2-py3-none-any.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.4)\n",
            "Installing collected packages: symspellpy\n",
            "Successfully installed symspellpy-6.5.2\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=f25071122949b304ac0e7e5fc49a18f1a1dbb159d17e7deda51bcf6b3e526872\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "Collecting imbalanced-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/aa/eba717a14df36f0b6f000ebfaf24c3189cd7987130f66cc3513efead8c2a/imbalanced_learn-0.6.1-py3-none-any.whl (162kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.3.3)\n",
            "Collecting scikit-learn>=0.22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/d0/860c4f6a7027e00acff373d9f5327f4ae3ed5872234b3cbdd7bcb52e5eff/scikit_learn-0.22-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 22.6MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\n",
            "  Found existing installation: scikit-learn 0.21.3\n",
            "    Uninstalling scikit-learn-0.21.3:\n",
            "      Successfully uninstalled scikit-learn-0.21.3\n",
            "  Found existing installation: imbalanced-learn 0.4.3\n",
            "    Uninstalling imbalanced-learn-0.4.3:\n",
            "      Successfully uninstalled imbalanced-learn-0.4.3\n",
            "Successfully installed imbalanced-learn-0.6.1 scikit-learn-0.22\n",
            "Collecting bert-for-tf2\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/d8/14e0cfa03bbeb72c314f0648267c490bcceec5e8fb25081ec31307b5509c/bert-for-tf2-0.12.6.tar.gz\n",
            "Collecting py-params>=0.7.3\n",
            "  Downloading https://files.pythonhosted.org/packages/ec/17/71c5f3c0ab511de96059358bcc5e00891a804cd4049021e5fa80540f201a/py-params-0.8.2.tar.gz\n",
            "Collecting params-flow>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/12/2604f88932f285a473015a5adabf08496d88dad0f9c1228fab1547ccc9b5/params-flow-0.7.4.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.12.6-cp36-none-any.whl size=29115 sha256=cf6f938e9cd27a05fb38711a06e67f61458b0f1330655aff25208f5f431640d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/19/54/51eeca468b219a1bc910c54aff87f0648b28a1fb71c115ba0f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.8.2-cp36-none-any.whl size=4633 sha256=edf8715724683f94bf7b0bf65165e444aafa036e786cafc2886c4d590641ecc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/3a/9c/baf35d6f17f0c2c6b61bf8ac3ab9fc12df0e41432ccaeecacb\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.7.4-cp36-none-any.whl size=16196 sha256=62937fe4f201a70cb070924134c6ae194fe5341b9d9fc116fff9c299b9a941c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/30/40/507b60d68b67ac87f35e95c98f5b296a32f146d5ae1d1d5aa7\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.12.6 params-flow-0.7.4 py-params-0.8.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.40)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=41a2afb6f62a505e67a800dda9162c0834c43387f45c1e4d81165d36a8ca3717\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.35 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7864AKjDrMsa",
        "colab_type": "code",
        "outputId": "59934496-0ca2-434f-aac6-5c6f98257984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# All imports - DO NOT CHANGE THE ORDER OF INSTRUCTIONS\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "    sys.path.insert(0, 'bert_repo')\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode\n",
        "import contractions\n",
        "import gensim.downloader as api\n",
        "import re\n",
        "import wordsegment\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import emoji\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from modeling import BertModel, BertConfig\n",
        "from tokenization import FullTokenizer, convert_to_unicode\n",
        "from extract_features import InputExample, convert_examples_to_features\n",
        "from tqdm import tqdm\n",
        "#import tensorflow_addons as tfa\n",
        "# import nltk\n",
        "from google.colab import auth, drive\n",
        "# nltk.download('punkt')\n",
        "\n",
        "wordsegment.load()\n",
        "\n",
        "# Load SymSpell -> package for correcting misspellings\n",
        "sym_spell = SymSpell(2, 7)\n",
        "\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "# get TF logger \n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert_repo'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "remote: Total 336 (delta 0), reused 0 (delta 0), pack-reused 336\u001b[K\n",
            "Receiving objects: 100% (336/336), 291.40 KiB | 1.52 MiB/s, done.\n",
            "Resolving deltas: 100% (184/184), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mER1mPUtiaPl",
        "colab_type": "code",
        "outputId": "20c6b6d2-2bf1-4298-e742-62984cc9c5b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "#Import data\n",
        "training_examples_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/olid-training-v1.0.tsv'\n",
        "training_dataset = pd.read_csv(training_examples_url, delimiter='\\t')\n",
        "print(training_dataset.head())\n",
        "test_tweets_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/testset-levela.tsv'\n",
        "test_tweets = pd.read_csv(test_tweets_url, delimiter='\\t')\n",
        "print(test_tweets.head())\n",
        "test_labels_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/labels-levela.csv'\n",
        "test_labels = pd.read_csv(test_labels_url, delimiter=',', header=None, names=[\"id\", \"label\"])\n",
        "print(test_labels.head())\n",
        "test_dataset = test_tweets.set_index(\"id\").join(test_labels.set_index(\"id\"))\n",
        "test_dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "      id                                              tweet\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...\n",
            "3  13876  #Watching #Boomer getting the news that she is...\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...\n",
            "      id label\n",
            "0  15923   OFF\n",
            "1  27014   NOT\n",
            "2  30530   NOT\n",
            "3  13876   NOT\n",
            "4  60133   OFF\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15923</th>\n",
              "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27014</th>\n",
              "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30530</th>\n",
              "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13876</th>\n",
              "      <td>#Watching #Boomer getting the news that she is...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60133</th>\n",
              "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   tweet label\n",
              "id                                                            \n",
              "15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF\n",
              "27014  #ConstitutionDay is revered by Conservatives, ...   NOT\n",
              "30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT\n",
              "13876  #Watching #Boomer getting the news that she is...   NOT\n",
              "60133  #NoPasaran: Unity demo to oppose the far-right...   OFF"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73i5BO8TqoTE",
        "colab_type": "text"
      },
      "source": [
        "# **Training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xHoiMYYlJQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 13\n",
        "\n",
        "# prepare training examples\n",
        "training_examples_A = training_dataset['tweet'][training_dataset['subtask_a'].notnull()]\n",
        "training_examples_B = training_dataset['tweet'][training_dataset['subtask_b'].notnull()]\n",
        "training_examples_C = training_dataset['tweet'][training_dataset['subtask_c'].notnull()]\n",
        "\n",
        "# prepare test examples and labels\n",
        "test_examples_A = test_dataset['tweet'][test_dataset['label'].notnull()]\n",
        "test_labels_A = (test_dataset['label'][test_dataset['label'].notnull()] == 'OFF').astype(int)\n",
        "\n",
        "# prepare training labels\n",
        "training_labels_A = (training_dataset['subtask_a'][training_dataset['subtask_a'].notnull()] == 'OFF').astype(int)\n",
        "training_labels_B = (training_dataset['subtask_b'][training_dataset['subtask_b'].notnull()] == 'TIN').astype(int)\n",
        "c_mapping = {'IND': 0, 'GRP': 1, 'OTH': 2}\n",
        "training_labels_C = training_dataset['subtask_c'][training_dataset['subtask_c'].notnull()].replace(c_mapping)\n",
        "\n",
        "# split training set into training and validation\n",
        "training_examples_A, validation_examples_A, training_labels_A, validation_labels_A = train_test_split(\n",
        "    training_examples_A, training_labels_A, test_size=0.1, stratify=training_labels_A, random_state=seed)\n",
        "training_examples_B, validation_examples_B, training_labels_B, validation_labels_B = train_test_split(\n",
        "    training_examples_B, training_labels_B, test_size=0.1, stratify=training_labels_B, random_state=seed)\n",
        "training_examples_C, validation_examples_C, training_labels_C, validation_labels_C = train_test_split(\n",
        "    training_examples_C, training_labels_C, test_size=0.1, stratify=training_labels_C, random_state=seed)\n",
        "\n",
        "training_x = np.asarray(training_examples_A)\n",
        "validation_x = np.asarray(validation_examples_A)\n",
        "training_y = np.asarray(training_labels_A)\n",
        "validation_y = np.asarray(validation_labels_A)\n",
        "test_x = np.asarray(test_examples_A)\n",
        "test_y = np.asarray(test_labels_A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bNpHc9qMpw",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fao-Vte1gtJ2",
        "colab_type": "text"
      },
      "source": [
        "### Common preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re0mCq1ogs6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags if exist\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    stripped_text = soup.get_text(separator=' ')\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "# remove unnecessary whitespaces\n",
        "def remove_whitespace(text):\n",
        "    text = text.strip()\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# remove accented chars (e.g. caffè -> caffe)\n",
        "def remove_accented_chars(text):\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# remove hashes and split words (e.g. '#fortTrump' -> 'fort trump')\n",
        "def split_hashtags(text):\n",
        "    splitted = text.split()\n",
        "    new_word_sequence = []\n",
        "\n",
        "    for chunk in splitted:\n",
        "        if chunk[0] == '#':\n",
        "            chunk = chunk[1:]\n",
        "            new_word_sequence.extend(wordsegment.segment(chunk))\n",
        "        else:\n",
        "            new_word_sequence.append(chunk)\n",
        "        \n",
        "    return ' '.join(tuple(new_word_sequence))\n",
        "\n",
        "\n",
        "def substitute_emojis(text):\n",
        "    demojized_text = emoji.demojize(text)\n",
        "    return re.compile('[_:]+').sub(' ', demojized_text)\n",
        "\n",
        "\n",
        "def preprocess_common(text):\n",
        "    text = strip_html_tags(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = split_hashtags(text)\n",
        "    text = substitute_emojis(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Gjm9OhOzug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove redundant @user tokens\n",
        "def remove_redundant_users(example):\n",
        "    user_count = 0\n",
        "    new_example = example[:]\n",
        "    for i, token in reversed(list(enumerate(example))):\n",
        "        if token == '@user':\n",
        "            user_count += 1\n",
        "        if user_count > 3:\n",
        "            new_example.pop(i)\n",
        "    else:\n",
        "        user_count = 0\n",
        "\n",
        "    return new_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJMq-8ygixr",
        "colab_type": "text"
      },
      "source": [
        "### Spacy preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPYd37I_giKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try leaving '?' and '!' as far as punctuation is concerned\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# exclude negation words from spacy stopwords list\n",
        "deselect_stop_words = ['no', 'not', 'noone', 'none', 'lacks', 'lack', 'nor', 'never', 'neighter', 'hardly', 'nobody', 'nothing', 'lacking', 'nowhere']\n",
        "for w in deselect_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    clean_text = []\n",
        "    \n",
        "    for token in doc:\n",
        "        flag = True\n",
        "        edit = token.text\n",
        "\n",
        "        # remove punctuations\n",
        "        if token.pos_ == 'PUNCT' and flag == True and token.text != '@user': \n",
        "            flag = False\n",
        "       \n",
        "        # remove special characters\n",
        "        if token.pos_ == 'SYM' and flag == True: \n",
        "            flag = False\n",
        "        \n",
        "        # remove numbers\n",
        "        if (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n",
        "            flag = False\n",
        "\n",
        "        # correct misspelings\n",
        "        if flag == True:\n",
        "            suggestions = sym_spell.lookup(edit, Verbosity.TOP, 2)\n",
        "            if len(suggestions) > 0:\n",
        "                edit = suggestions[0].term\n",
        "\n",
        "        # remove stop words\n",
        "        if token.is_stop and token.pos_ != 'NUM': \n",
        "            flag = False\n",
        "\n",
        "        # convert tokens to base form\n",
        "        elif token.lemma_ != '-PRON-' and flag == True:\n",
        "            edit = token.lemma_\n",
        "\n",
        "        # append tokens edited and not removed to list \n",
        "        if edit != '' and flag == True:\n",
        "            clean_text.append(edit)        \n",
        "    \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyyCkwWhFw1",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAHp6jlH12sD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalization -> papers, complicated solutions, replace abbreviations with full names (e.g. MAGA)\n",
        "# check removing less stop words (some may have some significance)\n",
        "\n",
        "cleaned_x = [preprocess_spacy(example) for example in training_x[0:30]]\n",
        "reduced_users_x = [remove_redundant_users(example) for example in cleaned_x]\n",
        "print(reduced_users_x[0:30])\n",
        "print(training_x[0:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU50oihvDCCo",
        "colab_type": "text"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DmN-x8-FgAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4DvYS5oHBam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                            np.unique(training_y),\n",
        "                                            training_y)\n",
        "class_weights /= max(class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH-6kHUmZ7Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_loss(actual, predicted):\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    loss = bce(actual, predicted)\n",
        "   \n",
        "    return tf.keras.backend.mean(loss * class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btAs2RDT9uoS",
        "colab_type": "text"
      },
      "source": [
        "# **BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp5AzW_zilHm",
        "colab_type": "text"
      },
      "source": [
        "### BERT Preprocessing - single example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3woANjZiltJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctation(tokens):\n",
        "    punctuation = [char for char in string.punctuation]\n",
        "    return list(filter(lambda token: token not in punctuation, tokens))\n",
        "\n",
        "\n",
        "def preprocess_bert_before_tokenization(text):\n",
        "    preprocessed_text = preprocess_common(text)\n",
        "    spacy_x = preprocess_spacy(preprocessed_text)\n",
        "    cleaned_x = remove_redundant_users(spacy_x)\n",
        "    return ' '.join(cleaned_x)\n",
        "\n",
        "\n",
        "def preprocess_bert_after_tokenization(tokenized_x):\n",
        "    # update input_ids, input_mask, input_type_ids\n",
        "    return remove_punctation(cleaned_x) # remove i.a. hyphens remainings from words like 'de-platforming'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkqKH8ssRuFI",
        "colab_type": "text"
      },
      "source": [
        "### Getting the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxg4a3Y97Mt",
        "colab_type": "code",
        "outputId": "1b95adb6-6f86-4b11-c44d-bbb04e6c4764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-27 20:24:41--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.117.128, 2607:f8b0:400e:c02::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.117.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   119MB/s    in 3.3s    \n",
            "\n",
            "2019-12-27 20:24:45 (119 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlzqsoCERraJ",
        "colab_type": "text"
      },
      "source": [
        "### Building a tf.Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOyKrgZRRqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "    def bert_module_fn(is_training):\n",
        "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
        "\n",
        "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "        config = BertConfig.from_json_file(config_path)\n",
        "        model = BertModel(config=config, is_training=is_training,\n",
        "                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
        "          \n",
        "        seq_output = model.all_encoder_layers[-1]\n",
        "        pool_output = model.get_pooled_output()\n",
        "\n",
        "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "        lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "        \n",
        "        input_map = {\"input_ids\": input_ids,\n",
        "                     \"input_mask\": input_mask,\n",
        "                     \"segment_ids\": token_type}\n",
        "        \n",
        "        output_map = {\"pooled_output\": pool_output,\n",
        "                      \"sequence_output\": seq_output}\n",
        "\n",
        "        output_info_map = {\"vocab_file\": vocab_file,\n",
        "                           \"do_lower_case\": lower_case}\n",
        "                \n",
        "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "    return bert_module_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubGdlgtTsjW1",
        "colab_type": "text"
      },
      "source": [
        "### Exporting the module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fShtfnSQbO",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "30728d7a-3c47-4e27-b434-12e9841d683a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "MODEL_DIR = \"uncased_L-12_H-768_A-12\" #@param {type:\"string\"} ['uncased_L-12_H-768_A-12']\n",
        "\n",
        "config_path = \"/content/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "\n",
        "module_fn = build_module_fn(config_path, vocab_path)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/{}/bert_model.ckpt\".format(MODEL_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "From bert_repo/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "From bert_repo/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "From bert_repo/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "From bert_repo/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "From bert_repo/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_hub/saved_model_lib.py:110: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgf8hzNYMst",
        "colab_type": "text"
      },
      "source": [
        "### Building the text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMkUX6C5V3r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(str_list):\n",
        "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
        "    unique_id = 0\n",
        "    for s in str_list:\n",
        "        line = convert_to_unicode(s)\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        text_a = line.strip()\n",
        "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=None)\n",
        "        unique_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmsepoRxVsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.input_type_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_061naESlic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_preprocessor(voc_path, seq_len, lower=True):\n",
        "    tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
        "\n",
        "    def strings_to_arrays(sents):\n",
        "\n",
        "        sents = np.atleast_1d(sents).reshape((-1,))\n",
        "\n",
        "        examples = []\n",
        "        for example in read_examples(sents):\n",
        "            example.text_a = preprocess_bert_before_tokenization(example.text_a)\n",
        "            examples.append(example)\n",
        "\n",
        "        features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
        "        arrays = features_to_arrays(features)\n",
        "        \n",
        "        return arrays\n",
        "\n",
        "    return strings_to_arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycbxDVlalim",
        "colab_type": "text"
      },
      "source": [
        "### Implementing a BERT Keras layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIIv5wCKUkgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
        "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
        "                 tune_embeddings=False, trainable=True, **kwargs):\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.n_tune_layers = n_tune_layers\n",
        "        self.tune_embeddings = tune_embeddings\n",
        "        self.do_preprocessing = do_preprocessing\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.seq_len = seq_len\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "\n",
        "        self.var_per_encoder = 16\n",
        "        if self.pooling not in [\"cls\", \"mean\", None]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
        "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
        "\n",
        "        trainable_layers = []\n",
        "        if self.tune_embeddings:\n",
        "            trainable_layers.append(\"embeddings\")\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            trainable_layers.append(\"pooler\")\n",
        "\n",
        "        if self.n_tune_layers > 0:\n",
        "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
        "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
        "            for i in range(self.n_tune_layers):\n",
        "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
        "        \n",
        "        # Add module variables to layer's trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if any([l in var.name for l in trainable_layers]):\n",
        "                self._trainable_weights.append(var)\n",
        "            else:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        self.build_preprocessor()\n",
        "        self.initialize_module()\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def build_abspath(self, path):\n",
        "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
        "          return path\n",
        "        else:\n",
        "          return os.path.abspath(path)\n",
        "\n",
        "    def build_preprocessor(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
        "\n",
        "    def initialize_module(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        \n",
        "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
        "                                     for var in self.bert.variables])\n",
        "\n",
        "        uninitialized = []\n",
        "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
        "            if not is_initialized:\n",
        "                uninitialized.append(var)\n",
        "\n",
        "        if len(uninitialized):\n",
        "            sess.run(tf.variables_initializer(uninitialized))\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "        if self.do_preprocessing:\n",
        "          input = tf.numpy_function(self.preprocessor, \n",
        "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
        "                                    name='preprocessor')\n",
        "          for feature in input:\n",
        "            feature.set_shape((None, self.seq_len))\n",
        "        \n",
        "        input_ids, input_mask, segment_ids = input\n",
        "        \n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        \n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "            \n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            \n",
        "            if self.pooling == \"mean\":\n",
        "              pooled = masked_reduce_mean(result, input_mask)\n",
        "            else:\n",
        "              pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dict = {\n",
        "            \"bert_path\": self.bert_path, \n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"n_tune_layers\": self.n_tune_layers,\n",
        "            \"tune_embeddings\": self.tune_embeddings,\n",
        "            \"do_preprocessing\": self.do_preprocessing,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "        super(BertLayer, self).get_config()\n",
        "        return config_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIjYf1EL_nQA",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQLsvYzaysD",
        "colab_type": "code",
        "outputId": "42ccda0a-25fa-4ee7-f487-722252808faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=256, tune_embeddings=False,\n",
        "                    pooling='cls', n_tune_layers=2, verbose=False)\n",
        "\n",
        "layer = tf.keras.layers.Dense(64, activation='relu')(encoder(inp))\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(layer)\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuy1OMCbGTw",
        "colab_type": "code",
        "outputId": "b2e57827-d39a-404a-8a06-ad8aa80c003e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, ),\n",
        "      loss=weighted_loss,\n",
        "      metrics=[f1_m])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer (BertLayer)       (None, 768)               109482240 \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                49216     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 109,531,521\n",
            "Trainable params: 14,815,617\n",
            "Non-trainable params: 94,715,904\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7AbjTBifTl",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbagCxaieoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6svZ93niek0",
        "colab_type": "code",
        "outputId": "ef15a7ab-8a15-497a-b33d-1525712034a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "saver = keras.callbacks.ModelCheckpoint(\"bert_H2_S256_B128.hdf5\")\n",
        "\n",
        "history = model.fit(training_x, training_y, validation_data=[validation_x, validation_y], batch_size=128, epochs=3, callbacks=[saver])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/extract_features.py:283: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11916/11916 [==============================] - 304s 25ms/sample - loss: 0.4373 - f1_m: 0.1981 - val_loss: 0.3761 - val_f1_m: 0.5533\n",
            "Epoch 2/3\n",
            "11916/11916 [==============================] - 295s 25ms/sample - loss: 0.3700 - f1_m: 0.5817 - val_loss: 0.3600 - val_f1_m: 0.5802\n",
            "Epoch 3/3\n",
            "11916/11916 [==============================] - 292s 25ms/sample - loss: 0.3498 - f1_m: 0.6142 - val_loss: 0.3541 - val_f1_m: 0.6086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjenF1B39CNV",
        "colab_type": "code",
        "outputId": "2fe00c8d-d11d-4265-f904-0bfc2b453436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model.evaluate(test_x, test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 20s 23ms/sample - loss: 0.3079 - f1_m: 0.5873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3079082017721132, 0.587346]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwOSC395hUmY",
        "colab_type": "text"
      },
      "source": [
        "### Loss function graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-ROTbohUXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rivD55Qc85D",
        "colab_type": "text"
      },
      "source": [
        "## Saving and restoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJNgkArdBAs",
        "colab_type": "code",
        "outputId": "6de418e4-9ffe-427d-f547-dfd25cfa89fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "print(training_y[:10])\n",
        "model.predict(training_x[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 0 0 1 0 0 0 1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.67689705],\n",
              "       [0.824897  ],\n",
              "       [0.08231309],\n",
              "       [0.07174948],\n",
              "       [0.8153479 ],\n",
              "       [0.1083402 ],\n",
              "       [0.37947318],\n",
              "       [0.52877355],\n",
              "       [0.7440483 ],\n",
              "       [0.0785928 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix02jwj_u_64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json.dump(model.to_json(), open(\"bert_H2_S256_B64.json\", \"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWsbzSaeSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.model_from_json(json.load(open(\"bert_H2_S256_B64.json\")), \n",
        "                                        custom_objects={\"BertLayer\": BertLayer})\n",
        "\n",
        "model.load_weights(\"bert_H2_S256_B64.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7H2AY9MsvUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(training_x[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_RWAQ-Um9AT",
        "colab_type": "text"
      },
      "source": [
        "### Freeze model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg5crVOofJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
        "\n",
        "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes a Keras model into a pruned computation graph.\n",
        "\n",
        "    @param model The Keras model to be freezed.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    \n",
        "    sess = tf.keras.backend.get_session()\n",
        "    graph = sess.graph\n",
        "    \n",
        "    with graph.as_default():\n",
        "\n",
        "        input_tensors = model.inputs\n",
        "        output_tensors = model.outputs\n",
        "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
        "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
        "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
        "        \n",
        "        tmp_g = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in tmp_g.node:\n",
        "                node.device = \"\"\n",
        "        \n",
        "        tmp_g = optimize_for_inference(\n",
        "            tmp_g, input_ops, output_ops, dtypes, False)\n",
        "        \n",
        "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
        "        \n",
        "        if export_path is not None:\n",
        "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
        "                f.write(tmp_g.SerializeToString())\n",
        "        \n",
        "        return tmp_g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMUrFpgofMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShmlWdXLcH80",
        "colab_type": "code",
        "outputId": "46e7315c-4624-437c-e627-5347950e8456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/gaphex/bert_experimental/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/bert_experimental\")\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.graph_ops import load_graph"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert_experimental' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKRLyiXvrqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "restored_graph = load_graph(\"frozen_graph_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8bT-YXrgo6j",
        "colab_type": "code",
        "outputId": "5437696b-2534-4ffa-e11b-99ab284134ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "graph_ops = restored_graph.get_operations()\n",
        "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
        "print(input_op, output_op)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import/input_2_1 import/dense_1_2/Sigmoid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHr2ZQGfg3y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
        "y = restored_graph.get_tensor_by_name(output_op + ':0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoapr4e86Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQFQOML7ivdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRf75n6ECUa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session(graph=restored_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw55NB6YseBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = sess.run(y, feed_dict={\n",
        "        x: training_x[:10].reshape((-1,1))\n",
        "    })\n",
        "\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiPP_1ocU7aN",
        "colab_type": "text"
      },
      "source": [
        "# LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPA7iKSbDZyt",
        "colab_type": "text"
      },
      "source": [
        "## Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsuJW2kIVB6_",
        "colab_type": "code",
        "outputId": "2489002d-1910-4d3b-ca02-3f108ede5cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH_TO_DIR = '/content/drive/My Drive/STUDIA/SEM 9/pracownia/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ka_o7QWVW6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_preprocessing(example):\n",
        "    basic_preprocessed = preprocess_common(example)\n",
        "    cleaned_example = preprocess_spacy(basic_preprocessed)\n",
        "    return remove_redundant_users(cleaned_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYH2_VlHVbqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessed_train = [full_preprocessing(example) for example in training_x]\n",
        "preprocessed_valid = [full_preprocessing(example) for example in validation_x]\n",
        "preprocessed_test = [full_preprocessing(example) for example in test_x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtKqZ_nHVp8_",
        "colab_type": "code",
        "outputId": "e2c578c7-edaa-459f-e978-7f42ff44628e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "word2vec = Word2Vec.load(PATH_TO_DIR + \"corrected_word2vec.model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_q48BYtVvni",
        "colab_type": "code",
        "outputId": "ea3e845d-7bc7-4aa6-fc7c-2588122f7813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "sim_words = word2vec.wv.most_similar('maga')\n",
        "\n",
        "for i in sim_words[:10]:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('kag', 0.9932580590248108)\n",
            "('wwg1wga', 0.9898135662078857)\n",
            "('patriot', 0.9889000654220581)\n",
            "('drain', 0.9829832315444946)\n",
            "('6th', 0.9824297428131104)\n",
            "('deep', 0.9804140329360962)\n",
            "('state', 0.9798689484596252)\n",
            "('kag2018', 0.9782142043113708)\n",
            "('q', 0.9760551452636719)\n",
            "('united', 0.9707470536231995)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRBSiDgRV_OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2idx(word):\n",
        "    try:\n",
        "        return word2vec.wv.vocab[word].index\n",
        "    except KeyError:\n",
        "        return 0\n",
        "        \n",
        "def idx2word(token):\n",
        "    return word2vec.wv.index2word[token]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlm3JEKFWBqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2idx_set(X):\n",
        "    new_X = []\n",
        "\n",
        "    for line in X:\n",
        "        new_line = []\n",
        "        for w in line:\n",
        "            new_line.append(word2idx(w))\n",
        "        new_X.append(new_line)\n",
        "    \n",
        "    return new_X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa27ov0kZlr_",
        "colab_type": "code",
        "outputId": "8161c5d8-5e93-43fd-a7f4-70659e51a45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_lenght = 79\n",
        "w2v_weights = word2vec.wv.vectors\n",
        "vocab_size, embedding_size = w2v_weights.shape\n",
        "\n",
        "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 7457 - Embedding Dim: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps8_2U3RWFMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train = np.asarray(word2idx_set(preprocessed_train))\n",
        "X_train = pad_sequences(X_train, maxlen=max_lenght, padding='pre', value=0)\n",
        "\n",
        "X_valid = np.asarray(word2idx_set(preprocessed_valid))\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_lenght, padding='pre', value=0)\n",
        "\n",
        "X_test = np.asarray(word2idx_set(preprocessed_test))\n",
        "X_test = pad_sequences(X_test, maxlen=max_lenght, padding='pre', value=0)\n",
        "\n",
        "X_train.shape, X_valid.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cbSGjsUWHio",
        "colab_type": "code",
        "outputId": "979432fd-5523-49b7-9625-c4cdb899d89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "X_test[0]    # exemplary sequence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,  178,  406, 4419, 1461,  678,    0, 3758,\n",
              "       1166,   33,   51,    7,  481, 3280, 4582, 1625, 4616,  141, 5864,\n",
              "       1036,  297,  536,  192,  594, 5552, 1052,  592,  260,  178,  145,\n",
              "          0,    2], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lE03y8dqFT1M"
      },
      "source": [
        "## Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haOM-sk3Zxz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.initializers import Constant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0-sWzyjbERN",
        "colab_type": "code",
        "outputId": "c296f6af-2eb9-4290-f75a-e6219b3320ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import os\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(PATH_TO_DIR, 'glove.twitter.27B.100d.txt')) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# prepare embedding matrix\n",
        "EMBEDDING_DIM = 100\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "for word, vec in word2vec.wv.vocab.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[vec.index] = embedding_vector\n",
        "\n",
        "# load pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer_pretrained = Embedding(vocab_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=max_lenght,\n",
        "                            trainable=False,\n",
        "                            mask_zero=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USvfcNQzWg4j",
        "colab_type": "code",
        "outputId": "9ffa88a0-bd5f-40e4-da69-8c9ca3cdda3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "lstm_model = Sequential()\n",
        "\n",
        "# Keras Embedding layer with Word2Vec weights initialization\n",
        "lstm_model.add(embedding_layer_pretrained)\n",
        "\n",
        "lstm_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "lstm_model.add(Dense(64, activation='relu'))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "lstm_model.summary()\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss=weighted_loss, metrics=[precision_m, recall_m, f1_m])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 79, 100)           745700    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                6464      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 832,629\n",
            "Trainable params: 86,929\n",
            "Non-trainable params: 745,700\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg78ssCvcEz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_checkpoint(name):\n",
        "    return ModelCheckpoint(PATH_TO_DIR + 'models/' + name,\n",
        "                     monitor='val_loss',\n",
        "                     mode='min',\n",
        "                     verbose=1,\n",
        "                     save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFbLRxCSb7K4",
        "colab_type": "code",
        "outputId": "c94d1e34-bc0c-441b-9c6a-8a90873f4d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "## using pretrained embeddings trainable!\n",
        "model_name = 'weighted_drop_glove_more_dense'\n",
        "\n",
        "history = lstm_model.fit(X_train, training_y, epochs=10, batch_size=64,\n",
        "                    validation_data=(X_valid, validation_y), \n",
        "                    callbacks=[get_model_checkpoint('best_' + model_name + '.h5')])\n",
        "\n",
        "lstm_model.save_weights(PATH_TO_DIR + 'models/' + model_name + '.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/10\n",
            "11916/11916 [==============================] - 30s 3ms/step - loss: 0.4180 - precision_m: 0.5955 - recall_m: 0.3826 - f1_m: 0.4446 - val_loss: 0.3719 - val_precision_m: 0.7196 - val_recall_m: 0.5074 - val_f1_m: 0.5842\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.37186, saving model to /content/drive/My Drive/STUDIA/SEM 9/pracownia/models/best_weighted_drop_glove_more_dense.h5\n",
            "Epoch 2/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3705 - precision_m: 0.7020 - recall_m: 0.4955 - f1_m: 0.5696 - val_loss: 0.3620 - val_precision_m: 0.7481 - val_recall_m: 0.5092 - val_f1_m: 0.5939\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.37186 to 0.36202, saving model to /content/drive/My Drive/STUDIA/SEM 9/pracownia/models/best_weighted_drop_glove_more_dense.h5\n",
            "Epoch 3/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3570 - precision_m: 0.7298 - recall_m: 0.5100 - f1_m: 0.5898 - val_loss: 0.3537 - val_precision_m: 0.7331 - val_recall_m: 0.5660 - val_f1_m: 0.6285\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.36202 to 0.35366, saving model to /content/drive/My Drive/STUDIA/SEM 9/pracownia/models/best_weighted_drop_glove_more_dense.h5\n",
            "Epoch 4/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3460 - precision_m: 0.7453 - recall_m: 0.5418 - f1_m: 0.6186 - val_loss: 0.3520 - val_precision_m: 0.7208 - val_recall_m: 0.5394 - val_f1_m: 0.6083\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.35366 to 0.35197, saving model to /content/drive/My Drive/STUDIA/SEM 9/pracownia/models/best_weighted_drop_glove_more_dense.h5\n",
            "Epoch 5/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3410 - precision_m: 0.7459 - recall_m: 0.5583 - f1_m: 0.6272 - val_loss: 0.3507 - val_precision_m: 0.7459 - val_recall_m: 0.4822 - val_f1_m: 0.5760\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.35197 to 0.35067, saving model to /content/drive/My Drive/STUDIA/SEM 9/pracownia/models/best_weighted_drop_glove_more_dense.h5\n",
            "Epoch 6/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3330 - precision_m: 0.7534 - recall_m: 0.5691 - f1_m: 0.6397 - val_loss: 0.3525 - val_precision_m: 0.7311 - val_recall_m: 0.5493 - val_f1_m: 0.6168\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.35067\n",
            "Epoch 7/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3283 - precision_m: 0.7614 - recall_m: 0.5760 - f1_m: 0.6455 - val_loss: 0.3527 - val_precision_m: 0.7376 - val_recall_m: 0.5869 - val_f1_m: 0.6443\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.35067\n",
            "Epoch 8/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3184 - precision_m: 0.7701 - recall_m: 0.6082 - f1_m: 0.6715 - val_loss: 0.3610 - val_precision_m: 0.7222 - val_recall_m: 0.6169 - val_f1_m: 0.6574\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.35067\n",
            "Epoch 9/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3115 - precision_m: 0.7721 - recall_m: 0.6129 - f1_m: 0.6733 - val_loss: 0.3625 - val_precision_m: 0.7614 - val_recall_m: 0.5584 - val_f1_m: 0.6331\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.35067\n",
            "Epoch 10/10\n",
            "11916/11916 [==============================] - 28s 2ms/step - loss: 0.3021 - precision_m: 0.7796 - recall_m: 0.6272 - f1_m: 0.6880 - val_loss: 0.3805 - val_precision_m: 0.7740 - val_recall_m: 0.4636 - val_f1_m: 0.5671\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.35067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGRqe-bY-mLE",
        "colab_type": "code",
        "outputId": "dcc9939c-9f03-4b7d-a224-197962b43a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "lstm_model.evaluate(X_test, test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 2s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2912720369738202,\n",
              " 0.7705164645993432,\n",
              " 0.49533670281254966,\n",
              " 0.583955024563989]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUKDuR3YHubj",
        "colab_type": "text"
      },
      "source": [
        "## Loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h9eroZyH678",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.initializers import Constant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0BA9OiDH56B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model = Sequential()\n",
        "\n",
        "# Keras Embedding layer with Word2Vec weights initialization\n",
        "lstm_model.add(Embedding(vocab_size,\n",
        "                          embedding_size,\n",
        "                          input_length=max_lenght,\n",
        "                          trainable=False,\n",
        "                          mask_zero=True))\n",
        "\n",
        "lstm_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "lstm_model.add(Dense(64, activation='relu'))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "lstm_model.summary()\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss=weighted_loss, metrics=[precision_m, recall_m, f1_m])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFv34U6m-czD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.load_weights(PATH_TO_DIR + 'models/best_weighted_drop_glove_more_dense.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J95v6sgi4Ace",
        "colab_type": "text"
      },
      "source": [
        "# BERT and LSTM ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obl3BoRo4FN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6RvGRB155AZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for layer in lstm_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kesUyvb__TjW",
        "colab_type": "code",
        "outputId": "c00b1c63-3962-407b-9486-a8e9417a9cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer (BertLayer)       (None, 768)               109482240 \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                49216     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 109,531,521\n",
            "Trainable params: 14,815,617\n",
            "Non-trainable params: 94,715,904\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHjcbRg6_WXp",
        "colab_type": "code",
        "outputId": "161345d3-c8f9-4850-bc30-7315bb72ed1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 79, 100)           745700    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                6464      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 832,629\n",
            "Trainable params: 86,929\n",
            "Non-trainable params: 745,700\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhVWv7W_3_fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "concatenated = Concatenate()([model.layers[-2].output, lstm_model.layers[-2].output])\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "new_model = Model(inputs=[model.layers[0].input, lstm_model.layers[0].input], outputs=output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLOKo7lCCbAI",
        "colab_type": "code",
        "outputId": "4a386151-9f99-449a-a3b1-816dcf62993f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "embedding_input (InputLayer)    [(None, 79)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 79, 100)      745700      embedding_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          109482240   input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 100)          80400       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 64)           49216       bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           6464        lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 128)          0           dense[0][0]                      \n",
            "                                                                 dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            129         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 110,364,149\n",
            "Trainable params: 14,902,545\n",
            "Non-trainable params: 95,461,604\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Iq2w0WCi1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_model.compile(loss=weighted_loss, optimizer='adam', metrics=[f1_m])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFHmWli_Cwa1",
        "colab_type": "code",
        "outputId": "b08ed452-918d-4035-d30f-44b83d5dbc8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "history = new_model.fit([training_x, X_train], training_y, epochs=2, batch_size=64,\n",
        "                    validation_data=([validation_x, X_valid], validation_y))\n",
        "\n",
        "new_model.save_weights(PATH_TO_DIR + 'models/full_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/2\n",
            "11916/11916 [==============================] - 274s 23ms/sample - loss: 0.3642 - f1_m: 0.5915 - val_loss: 0.3408 - val_f1_m: 0.6253\n",
            "Epoch 2/2\n",
            "11916/11916 [==============================] - 266s 22ms/sample - loss: 0.3193 - f1_m: 0.6508 - val_loss: 0.3390 - val_f1_m: 0.6362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_uB3jUaEFRg",
        "colab_type": "code",
        "outputId": "bef14ac8-1170-4b75-f108-544993137799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "new_model.evaluate([test_x, X_test], test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 24s 27ms/sample - loss: 0.2831 - f1_m: 0.6459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28307747882465983, 0.6458599]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}