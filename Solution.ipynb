{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c72-N0RGo1z",
        "colab_type": "code",
        "outputId": "a6c6c987-d2c6-4831-a77e-d9094732ba2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZvrBG6DXhkA",
        "colab_type": "code",
        "outputId": "3ec52411-e1b7-4ee6-c0cc-bd26b967c7d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%cd gdrive/My\\ Drive"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive/My Drive'\n",
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ds8aZ8OFzw8",
        "colab_type": "code",
        "outputId": "75c316b2-c408-4951-d620-710a6206f53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        }
      },
      "source": [
        "# Install necessary packages -> uncomment what is currently needed\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install contractions\n",
        "!pip install wordsegment\n",
        "!pip install -U symspellpy\n",
        "!pip install emoji --upgrade\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install bert-for-tf2\n",
        "!pip install transformers\n",
        "# !pip install nltk"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.24)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already up-to-date: symspellpy in /usr/local/lib/python3.6/dist-packages (6.5.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.5)\n",
            "Requirement already up-to-date: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.14.1)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.13.5)\n",
            "Requirement already satisfied: params-flow>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.7.4)\n",
            "Requirement already satisfied: py-params>=0.7.3 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7864AKjDrMsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All imports - DO NOT CHANGE THE ORDER OF INSTRUCTIONS\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "    sys.path.insert(0, 'bert_repo')\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode\n",
        "import contractions\n",
        "import gensim.downloader as api\n",
        "import re\n",
        "import wordsegment\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import emoji\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf2\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from modeling import BertModel, BertConfig\n",
        "from tokenization import FullTokenizer, convert_to_unicode\n",
        "from extract_features import InputExample, convert_examples_to_features\n",
        "from tqdm import tqdm\n",
        "#import tensorflow_addons as tfa\n",
        "# import nltk\n",
        "from google.colab import auth, drive\n",
        "# nltk.download('punkt')\n",
        "\n",
        "wordsegment.load()\n",
        "\n",
        "# Load SymSpell -> package for correcting misspellings\n",
        "sym_spell = SymSpell(2, 7)\n",
        "\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "# get TF logger \n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mER1mPUtiaPl",
        "colab_type": "code",
        "outputId": "7209d83b-e0e7-40d8-ffd5-a0124067d386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "#Import data\n",
        "training_examples_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/olid-training-v1.0.tsv'\n",
        "testing_examples_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/testset-levela.tsv'\n",
        "testing_labels_A_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/data2019/labels-levela.csv'\n",
        "\n",
        "training_dataset = pd.read_csv(training_examples_url, delimiter='\\t')\n",
        "testing_dataset_examples_A = pd.read_csv(testing_examples_A_url, delimiter='\\t')\n",
        "testing_dataset_labels_A = pd.read_csv(testing_labels_A_url, delimiter=',')\n",
        "\n",
        "print(training_dataset.head())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJNUAx_OiYP7",
        "colab_type": "text"
      },
      "source": [
        "## Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k4FoVUy0f86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import 2020 data\n",
        "\n",
        "training_dataset2020 = pd.read_csv('Colab Notebooks/task_a_distant.tsv', delimiter='\\t', nrows=500000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LJzCAAo0fvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "137a78b7-e097-437e-f30e-20db5ff84dde"
      },
      "source": [
        "def convert_averages(row):\n",
        "    return 'OFF' if row['average'] >= 0.4 else 'NOT'\n",
        "\n",
        "\n",
        "training_dataset2020 = training_dataset2020.drop(labels=['id', 'std'], axis=1)\n",
        "training_dataset2020['average'] = training_dataset2020.apply(lambda row: convert_averages(row), axis=1)\n",
        "training_dataset2020 = training_dataset2020.rename(columns={ 'average': 'subtask_a', 'text': 'tweet' })\n",
        "\n",
        "training_offensive = training_dataset2020[training_dataset2020['subtask_a'] == 'OFF'].head(50000)\n",
        "training_neutral = training_dataset2020[training_dataset2020['subtask_a'] == 'NOT'].head(50000)\n",
        "\n",
        "training_dataset = training_dataset.append(training_offensive)\n",
        "training_dataset = training_dataset.append(training_neutral)\n",
        "training_dataset = training_dataset.sample(frac=1, random_state=13)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text   average\n",
            "0  First time I heard his name in camp, he seems ...  0.195773\n",
            "1  When I go to drink with Tsubaki he would alway...  0.262401\n",
            "2                   @USER His ass need to stay up 😂😂  0.833391\n",
            "3  most important tweet of the day : Fuck Donald ...  0.565238\n",
            "4  You wanna leave? then feel free cus I promise ...  0.664921\n",
            "                                               tweet subtask_a\n",
            "0  First time I heard his name in camp, he seems ...       NOT\n",
            "1  When I go to drink with Tsubaki he would alway...       NOT\n",
            "2                   @USER His ass need to stay up 😂😂       OFF\n",
            "3  most important tweet of the day : Fuck Donald ...       OFF\n",
            "4  You wanna leave? then feel free cus I promise ...       OFF\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  sort=sort,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xDBjuNciVDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Additional datasets\n",
        "set_urls = [\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set1.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set2.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set3.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set4.csv',\n",
        "            'https://raw.githubusercontent.com/piotrjaniszewski1/Offensive-Language-Identification-and-Categorization/master/Additional%20datasets/set5.csv',\n",
        "]\n",
        "\n",
        "sets = [pd.read_csv(url) for url in set_urls]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCDhsVxMLnLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def labels_set1(row):\n",
        "  return 'OFF' if 1 in [row['toxic'], row['severe_toxic'], row['obscene'], row['threat'], row['insult'], row['identity_hate']] else 'NOT'\n",
        "\n",
        "def labels_set2(row):\n",
        "  return 'OFF' if row['class'] != 2 else 'NOT'\n",
        "\n",
        "def labels_set_ordinary(row):\n",
        "  return 'OFF' if row['subtask_a'] == 1 else 'NOT'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP0uTpIMiVih",
        "colab_type": "code",
        "outputId": "36f196b0-f6d6-4efd-d5fd-36a708af9887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "# Preprocess additional datasets\n",
        "\n",
        "sets[0] = sets[0].rename(columns={ 'label': 'subtask_a' })\n",
        "sets[0]['subtask_a'] = sets[0].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[0] = sets[0].drop('id', axis=1)\n",
        "\n",
        "sets[1] = sets[1].rename(columns={ 'comment_text': 'tweet', 'label': 'subtask_a' })\n",
        "sets[1]['subtask_a'] = sets[1].apply(lambda row: labels_set1(row), axis=1)\n",
        "sets[1] = sets[1].drop(labels=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\n",
        "\n",
        "sets[2] = sets[2].rename(columns={ 'label': 'subtask_a' })\n",
        "sets[2]['subtask_a'] = sets[2].apply(lambda row: labels_set2(row), axis=1)\n",
        "sets[2] = sets[2].drop(labels=['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'Unnamed: 0'], axis=1)\n",
        "\n",
        "sets[3] = sets[3].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[3]['subtask_a'] = sets[3].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[3] = sets[3].drop('Date', axis=1)\n",
        "\n",
        "sets[4] = sets[4].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[4]['subtask_a'] = sets[4].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[4] = sets[4].drop(labels=['Date', 'Usage'], axis=1)\n",
        "\n",
        "sets[5] = sets[5].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\n",
        "sets[5]['subtask_a'] = sets[5].apply(lambda row: labels_set_ordinary(row), axis=1)\n",
        "sets[5] = sets[5].drop(labels=['Date', 'id', 'Usage'], axis=1)\n",
        "\n",
        "sets_hate_only = [s[s['subtask_a'] == 'OFF'] for s in sets]\n",
        "\n",
        "for s in sets: # one can change to sets_hate_only\n",
        "  training_dataset = training_dataset.append(s)\n",
        "\n",
        "sets_hate_only\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Preprocess additional datasets\\n\\nsets[0] = sets[0].rename(columns={ 'label': 'subtask_a' })\\nsets[0]['subtask_a'] = sets[0].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[0] = sets[0].drop('id', axis=1)\\n\\nsets[1] = sets[1].rename(columns={ 'comment_text': 'tweet', 'label': 'subtask_a' })\\nsets[1]['subtask_a'] = sets[1].apply(lambda row: labels_set1(row), axis=1)\\nsets[1] = sets[1].drop(labels=['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], axis=1)\\n\\nsets[2] = sets[2].rename(columns={ 'label': 'subtask_a' })\\nsets[2]['subtask_a'] = sets[2].apply(lambda row: labels_set2(row), axis=1)\\nsets[2] = sets[2].drop(labels=['count', 'hate_speech', 'offensive_language', 'neither', 'class', 'Unnamed: 0'], axis=1)\\n\\nsets[3] = sets[3].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[3]['subtask_a'] = sets[3].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[3] = sets[3].drop('Date', axis=1)\\n\\nsets[4] = sets[4].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[4]['subtask_a'] = sets[4].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[4] = sets[4].drop(labels=['Date', 'Usage'], axis=1)\\n\\nsets[5] = sets[5].rename(columns={ 'Insult': 'subtask_a', 'Comment': 'tweet' })\\nsets[5]['subtask_a'] = sets[5].apply(lambda row: labels_set_ordinary(row), axis=1)\\nsets[5] = sets[5].drop(labels=['Date', 'id', 'Usage'], axis=1)\\n\\nsets_hate_only = [s[s['subtask_a'] == 'OFF'] for s in sets]\\n\\nfor s in sets: # one can change to sets_hate_only\\n  training_dataset = training_dataset.append(s)\\n\\nsets_hate_only\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73i5BO8TqoTE",
        "colab_type": "text"
      },
      "source": [
        "# **Training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xHoiMYYlJQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 13\n",
        "\n",
        "# prepare training examples\n",
        "training_examples_A = training_dataset['tweet'][training_dataset['subtask_a'].notnull()]\n",
        "training_examples_B = training_dataset['tweet'][training_dataset['subtask_b'].notnull()]\n",
        "training_examples_C = training_dataset['tweet'][training_dataset['subtask_c'].notnull()]\n",
        "\n",
        "# prepare test examples and labels\n",
        "test_examples_A = testing_dataset_examples_A['tweet'][testing_dataset_examples_A['tweet'].notnull()]\n",
        "test_labels_A = (testing_dataset_labels_A['label'][testing_dataset_labels_A['label'].notnull()] == 'OFF').astype(int)\n",
        "\n",
        "# prepare training labels\n",
        "training_labels_A = (training_dataset['subtask_a'][training_dataset['subtask_a'].notnull()] == 'OFF').astype(int)\n",
        "training_labels_B = (training_dataset['subtask_b'][training_dataset['subtask_b'].notnull()] == 'TIN').astype(int)\n",
        "c_mapping = {'IND': 0, 'GRP': 1, 'OTH': 2}\n",
        "training_labels_C = training_dataset['subtask_c'][training_dataset['subtask_c'].notnull()].replace(c_mapping)\n",
        "\n",
        "# split training set into training and validation\n",
        "training_examples_A, validation_examples_A, training_labels_A, validation_labels_A = train_test_split(\n",
        "    training_examples_A, training_labels_A, test_size=0.1, stratify=training_labels_A, random_state=seed)\n",
        "training_examples_B, validation_examples_B, training_labels_B, validation_labels_B = train_test_split(\n",
        "    training_examples_B, training_labels_B, test_size=0.1, stratify=training_labels_B, random_state=seed)\n",
        "training_examples_C, validation_examples_C, training_labels_C, validation_labels_C = train_test_split(\n",
        "    training_examples_C, training_labels_C, test_size=0.1, stratify=training_labels_C, random_state=seed)\n",
        "\n",
        "training_x = np.array(training_examples_A)\n",
        "validation_x = np.array(validation_examples_A)\n",
        "training_y = np.array(training_labels_A)\n",
        "validation_y = np.array(validation_labels_A)\n",
        "test_x = np.array(test_examples_A)\n",
        "test_y = np.array(test_labels_A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bNpHc9qMpw",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fao-Vte1gtJ2",
        "colab_type": "text"
      },
      "source": [
        "### Common preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re0mCq1ogs6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags if exist\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    stripped_text = soup.get_text(separator=' ')\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "# remove unnecessary whitespaces\n",
        "def remove_whitespace(text):\n",
        "    text = text.strip()\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# remove accented chars (e.g. caffè -> caffe)\n",
        "def remove_accented_chars(text):\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# remove hashes and split words (e.g. '#fortTrump' -> 'fort trump')\n",
        "def split_hashtags(text):\n",
        "    splitted = text.split()\n",
        "    new_word_sequence = []\n",
        "\n",
        "    for chunk in splitted:\n",
        "        if chunk[0] == '#':\n",
        "            chunk = chunk[1:]\n",
        "            new_word_sequence.extend(wordsegment.segment(chunk))\n",
        "        else:\n",
        "            new_word_sequence.append(chunk)\n",
        "        \n",
        "    return ' '.join(tuple(new_word_sequence))\n",
        "\n",
        "\n",
        "def substitute_emojis(text):\n",
        "    demojized_text = emoji.demojize(text)\n",
        "    return re.compile('[_:]+').sub(' ', demojized_text)\n",
        "\n",
        "\n",
        "def preprocess_common(text):\n",
        "    text = strip_html_tags(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = split_hashtags(text)\n",
        "    text = substitute_emojis(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Gjm9OhOzug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove redundant @user tokens\n",
        "def remove_redundant_users(example):\n",
        "    user_count = 0\n",
        "    new_example = example[:]\n",
        "    for i, token in reversed(list(enumerate(example))):\n",
        "        if token == '@user':\n",
        "            user_count += 1\n",
        "        if user_count > 3:\n",
        "            new_example.pop(i)\n",
        "    else:\n",
        "        user_count = 0\n",
        "\n",
        "    return new_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJMq-8ygixr",
        "colab_type": "text"
      },
      "source": [
        "### Spacy preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPYd37I_giKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try leaving '?' and '!' as far as punctuation is concerned\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# exclude negation words from spacy stopwords list\n",
        "deselect_stop_words = ['no', 'not', 'noone', 'none', 'lacks', 'lack', 'nor', 'never', 'neighter', 'hardly', 'nobody', 'nothing', 'lacking', 'nowhere']\n",
        "for w in deselect_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    clean_text = []\n",
        "    \n",
        "    for token in doc:\n",
        "        flag = True\n",
        "        edit = token.text\n",
        "        '''\n",
        "        # remove punctuations\n",
        "        if token.pos_ == 'PUNCT' and flag == True and token.text != '@user': \n",
        "            flag = False\n",
        "       \n",
        "        # remove special characters\n",
        "        if token.pos_ == 'SYM' and flag == True: \n",
        "            flag = False\n",
        "        \n",
        "        # remove numbers\n",
        "        if (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n",
        "            flag = False\n",
        "\n",
        "        # correct misspelings\n",
        "        # if flag == True:\n",
        "            # suggestions = sym_spell.lookup(edit, Verbosity.TOP, 2)\n",
        "            # if len(suggestions) > 0:\n",
        "                # edit = suggestions[0].term\n",
        "\n",
        "        # remove stop words\n",
        "        if token.is_stop and token.pos_ != 'NUM': \n",
        "            flag = False\n",
        "\n",
        "        # convert tokens to base form\n",
        "        elif token.lemma_ != '-PRON-' and flag == True:\n",
        "            edit = token.lemma_\n",
        "        '''\n",
        "        # append tokens edited and not removed to list \n",
        "        if edit != '' and flag == True:\n",
        "            clean_text.append(edit)        \n",
        "    \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyyCkwWhFw1",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAHp6jlH12sD",
        "colab_type": "code",
        "outputId": "f03c3e30-f40f-430d-8897-dd4134726958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "cleaned_x = [preprocess_spacy(example) for example in training_x[0:30]]\n",
        "reduced_users_x = [remove_redundant_users(example) for example in cleaned_x]\n",
        "print(reduced_users_x[0:30])\n",
        "print(training_x[0:30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['@USER', '@USER', '@USER', 'We', 'do', 'know', 'she', 'belongs', 'to', 'the', 'violent', 'liberal', 'Antifa', 'party', 'who', 'w/', 'their', 'KKK', 'HOODS', 'beat', 'up', 'total', 'strangers', '&', 'amp', ';', 'are', 'funded', 'to', 'do', 'so', 'by', 'her', 'party', '.', 'Most', 'violent', '&', 'amp', ';', 'corrupt', 'party', 'trying', 'to', 'now', 'pour', 'on', 'the', 'tears', '&', 'amp', ';', 'get', 'us', 'to', 'believe', 'they', 'have', 'a', 'heart', '.', 'Hard', 'sell', 'to', 'anyone', 'including', 'voters', '!'], ['@USER', 'you', 'are', 'a', 'lying', 'corrupt', 'traitor', '!', '!', '!', 'Nobody', 'wants', 'to', 'hear', 'anymore', 'of', 'your', 'lies', '!', '!', '!', '#', 'DeepStateCorruption', 'URL'], ['@USER', '@USER', 'Typical', 'liberals', '.', 'Principled', 'until', 'it', 'comes', 'to', 'their', 'money', '.'], ['@USER', 'Because', 'she', 'knew', 'how', 'to', 'add', 'up', 'the', 'millions', 'in', '@USER', 'donations', 'received', 'for', 'access', '?'], ['@USER', 'Those', 'idiots', 'are', 'fringe', 'right', '.', 'They', 'are', 'hated', 'by', 'most', 'conservatives', '.'], ['@USER', '@USER', '@USER', 'I', 'thought', 'conservatives', 'were', 'the', 'ones', 'trying', 'to', 'rewrite', 'history', '?', '🤔', 'URL'], ['@USER', '@USER', '@USER', 'I', 'linked', 'to', 'an', 'article', 'showing', 'you', 'why', 'you', \"'re\", 'wrong', '.', 'You', \"'re\", 'telling', 'libertarians', 'and', 'conservatives', '\"', 'aka', 'the', 'right', 'what', 'they', 'supposedly', 'believe', '.', 'I', \"'m\", 'not', 'bigoted', ',', 'it', \"'s\", 'just', 'a', 'pattern', 'I', \"'ve\", 'noticed', 'that', 'never', 'seems', 'to', 'be', 'wrong', '.', '\"'], ['@USER', 'Gritting', 'her', 'teeth', ',', 'the', 'blonde', 'then', 'broke', 'her', 'shield', 'and', 'feeling', 'pumped', 'the', 'blonde', 'charged', 'after', 'him', '.', 'Give', 'me', 'all', 'you', 'got', '!', '\"', 'She', 'then', 'tossed', 'some', 'of', 'her', 'darts', ',', 'aiming', 'for', 'his', 'legs', 'and', 'would', 'cause', 'only', 'minor', 'pain', '.', '\"', 'Show', 'me', 'how', 'tough', 'you', 'are', '!', '\"', '\"'], ['@USER', 'The', 'term', 'parasite', '\"', 'has', 'now', 'been', 'added', 'to', 'my', 'list', 'of', '\"', 'fuck', 'you', '\"', 'terms', ',', 'which', 'also', 'includes', 'the', 'term', '\"', 'widget', '\"', '.', '\"'], ['@USER', '@USER', '@USER', '@USER', '@USER', 'I', 'just', 'spoke', 'with', 'ANTIFA', 'and', 'BLM', '.', 'They', 'said', 'you', 'lied', '.'], ['@USER', 'Antifa', 'would', 'burn', 'a', 'Conservatives', 'house', 'down', 'and', 'CNN', 'would', 'be', 'there', 'lighting', 'the', 'torches', '&', 'amp', ';', 'throwing', 'gas', 'on', 'the', 'flames', '.'], ['@USER', '@USER', '@USER', '@USER', '@USER', '...', 'how', 'you', 'want', 'to', 'be', 'defined', '\"', '?', 'PUL', '-', 'EASE', '!', 'When', 'you', 'live', 'under', 'a', '24/7/365', 'MSMedia', 'SHIT', '-', 'SMEAR', 'to', 'the', 'point', 'you', 'have', 'Marxine', 'Waters', 'sending', 'her', 'ANTIFA', 'minions', 'out', 'to', 'threaten', 'you', 'in', 'public', 'places', '-', 'you', 'learn', 'not', 'to', 'give', 'a', 'F*CK', 'what', 'the', '\"', 'Pavlovian', 'Left', '\"', 'thinks', '.', 'Night', ',', 'night', '🌙', '!', '\"'], ['@USER', 'Thank', 'goodness', 'he', 'is', 'not', 'MN', 'passive', 'aggressive', '.', 'Love', 'it', '!', '!'], ['@USER', '@USER', 'Ford', 'and', 'the', 'conservatives', 'hates', 'the', 'displays', 'of', 'democratic', 'freedoms', 'hence', 'their', 'attacks', 'on', 'the', 'Canadian', 'charter', 'of', 'rights', 'and', 'freedoms', 'and', 'their', 'hate', 'for', 'minorities', 'constitutional', 'rights', '#', 'conservativesattackondemocracy', 'in', '#', 'canada', '#', 'theCPCisapoliticalhategroup'], ['@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', 'False', '🤣'], ['@USER', '@USER', '@USER', '@USER', '@USER', '3', ')', 'I', 'will', 'always', 'defend', 'the', '2nd', '.', 'But', 'not', 'the', 'NRA', 'who', 'when', 'I', '1st', 'joined', 'was', 'about', 'promoting', '&', 'amp', ';', 'educating', 'gun', 'owners', '&', 'amp', ';', 'the', 'public', 'about', 'guns', '.', 'No', 'hypocrisy', 'here', 'Amy', 'well', 'except', 'maybe', 'on', 'your', 'part', '.', 'FYI', 'I', 'know', 'a', 'lot', 'of', 'Liberals', 'that', 'a', 'members', 'of', 'the', 'NRA', '.', 'I', 'do', \"n't\", 'always', 'agree', 'with', 'them'], ['@USER', '@USER', '@USER', 'Misdrawed', '.', 'Cause', 'she', 'is', 'not', 'white', 'and', 'blonde', '.', 'Why', 'he', 'did', 'that', '?', 'Can', 'you', 'answer', 'that', '?'], ['@USER', '@USER', '@USER', 'Not', 'to', 'mention', 'that', 'Jim', 'seriously', 'has', 'a', 'medical', 'condition', '.', 'What', 'an', 'SJW', 'tactic', 'to', 'take', '.', 'Sargon', 'seriously', 'has', 'become', 'what', 'ANTIFA', 'is', '.'], ['@USER', '@USER', 'Do', 'any', 'Dems', '/', 'libs', 'have', 'a', 'real', 'job', '?', '?', 'They', 'sure', 'have', 'a', 'lot', 'of', 'free', 'time', '!', '!'], ['@USER', '@USER', '@USER', 'That', 'is', 'exactly', 'what', 'liberals', 'and', 'Dems', 'think', 'about', 'people', 'on', 'the', 'right', '.', 'It', \"'s\", 'funny', 'how', 'we', \"'ve\", 'been', 'pitted', 'against', 'each', 'other', 'when', 'really', 'our', 'interests', 'are', 'all', 'very', 'similar', 'with', 'just', 'some', 'nuances', '.', 'I', \"'m\", 'a', 'criminal', 'defense', 'attorney', 'and', 'I', 'fight', 'every', 'day', 'to', 'protect', 'our', 'bill', 'of', 'rights', '.'], ['@USER', 'Be', 'patient', 'with', 'us', 'we', 'have', 'autism', '\"', 'Cap', '!', '\"'], ['@USER', '@USER', 'does', 'that', 'mean', 'He', 'still', 'our', '#', 'superman', '?', 'Pleaseeeeee', 'say', 'he', 'is', '🙏', '🏼', '🙏', '🏼', '🙏', '🏼', '🙏', '🏼'], ['...', 'a', 'bunch', 'of', 'self', '-', 'aggrandizing', 'liberals', 'in', 'fancy', 'dresses', 'mocking', 'Trump', '.', '”', 'Except', 'for', 'the', 'fancy', 'dresses', ',', 'sounds', 'a', 'lot', 'like', 'the', 'Dems', 'who', 'are', 'trying', 'to', 'lynch', 'Kavanaugh', '...', '#', 'politics', '#', 'MeTooWitchhunt', 'URL'], ['@USER', 'He', 'is', 'so', 'generous', 'with', 'his', 'offers', '.'], ['@USER', 'Hello', ',', 'twin', '.', 'Let', 'ke', 'correcting', 'your', 'typoness', '.', 'Our', 'group', 'name', 'is', '(', 'G)I', '-', 'DLE', 'in', 'case', 'you', 'are', 'too', 'lazy', 'to', 'use', '(', ')', '\"', 'or', '\"', '-', '\"', ',', 'you', 'can', 'use', 'IDLE', 'instead', '😉', '\"'], ['@USER', '@USER', '@USER', '@USER', 'Bwahahaha', '-', 'work', '?', 'They', 'are', 'Liberal', 'tools', ',', 'honest', 'work', 'is', 'for', 'the', 'Conservatives', 'who', 'they', 'depend', 'on', 'to', 'support', 'their', 'chubby', 'little', 'selves', '.', 'Also', ',', 'I', 'hear', 'they', 'are', 'getting', '$', '50', '(', 'and', 'free', 'food', ')', 'to', 'be', 'protesters', '.', '\"', '\"'], ['#', 'Republicans', '#', 'Conservatives', '@USER', 'wet', 'dreams', ':', 'Step', '1', ':', 'Confirm', 'predator', 'as', 'SCOTUS', 'judge', 'Step', '2', ':', 'Eliminate', 'women', \"'s\", 'choice', 'Step', '3', ':', 'Eliminate', 'non', 'white', \"'s\", 'voting', 'rights', 'Step', '4', ':', 'Eliminate', 'women', \"'s\", 'voting', 'rights', 'Step', '5', ':', 'End', 'immigration'], ['#', 'wcw', 'because', 'look', 'at', 'how', 'beautiful', 'she', 'is', '!', '!', '!', '!', '!', '!', 'I', 'wish', 'I', 'had', 'this', 'beauty', 'tho', 'but', 'I', 'am', 'just', 'in', 'love', 'with', 'my', 'baby', 'sister', '😍', '😩', '❤', '️', '@USER', 'URL'], ['@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', 'So', 'you', \"'re\", 'saying', 'Antifa', 'should', 'have', 'never', 'oppressed', 'the', 'Nazis', '...', 'gotcha', '.'], ['@USER', '@USER', 'You', 'may', 'be', 'able', 'to', 'reconnect', 'now', '.', 'Mind', 'trying', 'for', 'us', '?', '🤞']]\n",
            "['@USER @USER @USER We do know she belongs to the violent liberal Antifa party who w/ their KKK HOODS beat up total strangers &amp; are funded to do so by her party. Most violent &amp; corrupt party trying to now pour on the tears &amp; get us to believe they have a heart. Hard sell to anyone including voters!'\n",
            " '@USER you are a lying corrupt traitor!!! Nobody wants to hear anymore of your lies!!! #DeepStateCorruption URL'\n",
            " '@USER @USER Typical liberals. Principled until it comes to their money.'\n",
            " '@USER Because she knew how to add up the millions in @USER donations received for access?'\n",
            " '@USER Those idiots are fringe right. They are hated by most conservatives.'\n",
            " '@USER @USER @USER I thought conservatives were the ones trying to rewrite history? 🤔 URL'\n",
            " '@USER @USER @USER I linked to an article showing you why you\\'re wrong. You\\'re telling libertarians and conservatives\" aka the right what they supposedly believe. I\\'m not bigoted, it\\'s just a pattern I\\'ve noticed that never seems to be wrong.\"'\n",
            " '@USER Gritting her teeth, the blonde then broke her shield and feeling pumped the blonde charged after him. Give me all you got!\" She then tossed some of her darts, aiming for his legs and would cause only minor pain. \"Show me how tough you are!\"\"'\n",
            " '@USER The term parasite\" has now been added to my list of \"fuck you\" terms, which also includes the term \"widget\".\"'\n",
            " '@USER @USER @USER @USER @USER I just spoke with ANTIFA and BLM. They said you lied.'\n",
            " '@USER Antifa would burn a Conservatives house down and CNN would be there lighting the torches &amp; throwing gas on the flames.'\n",
            " '@USER @USER @USER @USER @USER ...how you want to be defined\"? PUL-EASE! When you live under a 24/7/365 MSMedia SHIT-SMEAR to the point you have Marxine Waters sending her ANTIFA minions out to threaten you in public places - you learn not to give a F*CK what the \"Pavlovian Left\" thinks. Night, night 🌙!\"'\n",
            " '@USER Thank goodness he is not MN passive aggressive. Love it!!'\n",
            " '@USER @USER Ford and the conservatives hates the displays of democratic freedoms hence their attacks on the Canadian charter of rights and freedoms and their hate for minorities constitutional rights #conservativesattackondemocracy in #canada #theCPCisapoliticalhategroup'\n",
            " '@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER False 🤣'\n",
            " \"@USER @USER @USER @USER @USER 3) I will always defend the 2nd. But not the NRA who when I 1st joined was about promoting &amp; educating gun owners &amp; the public about guns. No hypocrisy here Amy well except maybe on your part. FYI I know a lot of Liberals that a members of the NRA. I don't always agree with them\"\n",
            " '@USER @USER @USER Misdrawed. Cause she is not white and blonde. Why he did that? Can you answer that?'\n",
            " '@USER @USER @USER Not to mention that Jim seriously has a medical condition. What an SJW tactic to take. Sargon seriously has become what ANTIFA is .'\n",
            " '@USER @USER Do any Dems/libs have a real job?? They sure have a lot of free time!!'\n",
            " \"@USER @USER @USER That is exactly what liberals and Dems think about people on the right. It's funny how we've been pitted against each other when really our interests are all very similar with just some nuances. I'm a criminal defense attorney and I fight every day to protect our bill of rights.\"\n",
            " '@USER Be patient with us we have autism\" Cap!\"'\n",
            " '@USER @USER does that mean He still our #superman ? Pleaseeeeee say he is 🙏🏼🙏🏼🙏🏼🙏🏼'\n",
            " '... a bunch of self-aggrandizing liberals in fancy dresses mocking Trump.” Except for the fancy dresses, sounds a lot like the Dems who are trying to lynch Kavanaugh... #politics #MeTooWitchhunt URL'\n",
            " '@USER He is so generous with his offers.'\n",
            " '@USER Hello, twin. Let ke correcting your typoness. Our group name is (G)I-DLE in case you are too lazy to use ()\" or \"-\" , you can use IDLE instead 😉\"'\n",
            " '@USER @USER @USER @USER Bwahahaha-work? They are Liberal tools,honest work is for the Conservatives who they depend on to support their chubby little selves. Also,I hear they are getting $50 (and free food) to be protesters.\"\"'\n",
            " \"#Republicans #Conservatives @USER wet dreams: Step 1: Confirm predator as SCOTUS judge Step 2: Eliminate women's choice Step 3: Eliminate non white's voting rights Step 4: Eliminate women's voting rights Step 5: End immigration\"\n",
            " '#wcw because look at how beautiful she is!!!!!! I wish I had this beauty tho but I am just in love with my baby sister 😍😩❤️ @USER URL'\n",
            " \"@USER @USER @USER @USER @USER @USER @USER @USER So you're saying Antifa should have never oppressed the Nazis... gotcha.\"\n",
            " '@USER @USER You may be able to reconnect now. Mind trying for us? 🤞']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btAs2RDT9uoS",
        "colab_type": "text"
      },
      "source": [
        "# **BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp5AzW_zilHm",
        "colab_type": "text"
      },
      "source": [
        "### BERT Preprocessing - single example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3woANjZiltJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_bert_before_tokenization(text):\n",
        "    preprocessed_text = preprocess_common(text)\n",
        "    spacy_x = preprocess_spacy(text)\n",
        "    cleaned_x = remove_redundant_users(spacy_x)\n",
        "    return ' '.join(cleaned_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkqKH8ssRuFI",
        "colab_type": "text"
      },
      "source": [
        "### Getting the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxg4a3Y97Mt",
        "colab_type": "code",
        "outputId": "0da4e1ac-8cdb-4693-cf42-8f7a6737b1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
        "!unzip wwm_uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-04 03:00:44--  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1248381879 (1.2G) [application/zip]\n",
            "Saving to: ‘wwm_uncased_L-24_H-1024_A-16.zip.2’\n",
            "\n",
            "wwm_uncased_L-24_H- 100%[===================>]   1.16G  13.1MB/s    in 85s     \n",
            "\n",
            "2020-03-04 03:02:10 (13.9 MB/s) - ‘wwm_uncased_L-24_H-1024_A-16.zip.2’ saved [1248381879/1248381879]\n",
            "\n",
            "Archive:  wwm_uncased_L-24_H-1024_A-16.zip\n",
            "replace wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  A\n",
            "\n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlzqsoCERraJ",
        "colab_type": "text"
      },
      "source": [
        "### Building a tf.Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOyKrgZRRqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "    def bert_module_fn(is_training):\n",
        "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
        "\n",
        "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "        config = BertConfig.from_json_file(config_path)\n",
        "        model = BertModel(config=config, is_training=is_training,\n",
        "                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
        "          \n",
        "        seq_output = model.all_encoder_layers[-1]\n",
        "        pool_output = model.get_pooled_output()\n",
        "\n",
        "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "        lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "            \n",
        "        input_map = {\"input_ids\": input_ids,\n",
        "                     \"input_mask\": input_mask,\n",
        "                     \"segment_ids\": token_type}\n",
        "        \n",
        "        output_map = {\"pooled_output\": pool_output,\n",
        "                      \"sequence_output\": seq_output}\n",
        "\n",
        "        output_info_map = {\"vocab_file\": vocab_file,\n",
        "                           \"do_lower_case\": lower_case}\n",
        "                \n",
        "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "    return bert_module_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubGdlgtTsjW1",
        "colab_type": "text"
      },
      "source": [
        "### Exporting the module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fShtfnSQbO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = \"wwm_uncased_L-24_H-1024_A-16\"\n",
        "\n",
        "config_path = \"/content/gdrive/My Drive/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/gdrive/My Drive/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "\n",
        "module_fn = build_module_fn(config_path, vocab_path)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/gdrive/My Drive/{}/bert_model.ckpt\".format(MODEL_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgf8hzNYMst",
        "colab_type": "text"
      },
      "source": [
        "### Building the text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMkUX6C5V3r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(str_list):\n",
        "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
        "    unique_id = 0\n",
        "    for s in str_list:\n",
        "        line = convert_to_unicode(s)\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        text_a = line.strip()\n",
        "        splitted = re.split(r'[.?!]\\s*', text_a)\n",
        "        text_b = splitted[-1]\n",
        "        text_a.replace(text_b, '')\n",
        "\n",
        "\n",
        "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n",
        "        unique_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmsepoRxVsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.input_type_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_061naESlic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_preprocessor(voc_path, seq_len, lower=True):\n",
        "    tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
        "\n",
        "    def strings_to_arrays(sents):\n",
        "\n",
        "        sents = np.atleast_1d(sents).reshape((-1,))\n",
        "\n",
        "        examples = []\n",
        "        for example in read_examples(sents):\n",
        "            example.text_a = preprocess_bert_before_tokenization(example.text_a)\n",
        "            examples.append(example)\n",
        "\n",
        "        features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
        "        arrays = features_to_arrays(features)\n",
        "        \n",
        "        return arrays\n",
        "\n",
        "    return strings_to_arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycbxDVlalim",
        "colab_type": "text"
      },
      "source": [
        "### Implementing a BERT Keras layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIIv5wCKUkgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
        "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
        "                 tune_embeddings=False, trainable=True, **kwargs):\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.n_tune_layers = n_tune_layers\n",
        "        self.tune_embeddings = tune_embeddings\n",
        "        self.do_preprocessing = do_preprocessing\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.seq_len = seq_len\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "\n",
        "        self.var_per_encoder = 16\n",
        "        if self.pooling not in [\"cls\", \"mean\", \"lstm\", None]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
        "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
        "\n",
        "        trainable_layers = []\n",
        "        if self.tune_embeddings:\n",
        "            trainable_layers.append(\"embeddings\")\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            trainable_layers.append(\"pooler\")\n",
        "\n",
        "        if self.n_tune_layers > 0:\n",
        "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
        "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
        "            for i in range(self.n_tune_layers):\n",
        "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
        "        \n",
        "        # Add module variables to layer's trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if any([l in var.name for l in trainable_layers]):\n",
        "                self._trainable_weights.append(var)\n",
        "            else:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        self.build_preprocessor()\n",
        "        self.initialize_module()\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def build_abspath(self, path):\n",
        "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
        "          return path\n",
        "        else:\n",
        "          return os.path.abspath(path)\n",
        "\n",
        "    def build_preprocessor(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
        "\n",
        "    def initialize_module(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        \n",
        "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
        "                                     for var in self.bert.variables])\n",
        "\n",
        "        uninitialized = []\n",
        "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
        "            if not is_initialized:\n",
        "                uninitialized.append(var)\n",
        "\n",
        "        if len(uninitialized):\n",
        "            sess.run(tf.variables_initializer(uninitialized))\n",
        "\n",
        "    def call(self, input):\n",
        "        if self.do_preprocessing:\n",
        "          input = tf.numpy_function(self.preprocessor, \n",
        "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
        "                                    name='preprocessor')\n",
        "          for feature in input:\n",
        "            feature.set_shape((None, self.seq_len))\n",
        "        \n",
        "        input_ids, input_mask, segment_ids = input\n",
        "        \n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "\n",
        "            if self.pooling == \"lstm\":\n",
        "              lstm = tf.keras.layers.LSTM(1)\n",
        "              pooled = lstm(result)\n",
        "\n",
        "            else:\n",
        "              input_mask = tf.cast(input_mask, tf.float32)\n",
        "              mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "              masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                      tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "              \n",
        "              if self.pooling == \"mean\":\n",
        "                pooled = masked_reduce_mean(result, input_mask)\n",
        "              else:\n",
        "                pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dict = {\n",
        "            \"bert_path\": self.bert_path, \n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"n_tune_layers\": self.n_tune_layers,\n",
        "            \"tune_embeddings\": self.tune_embeddings,\n",
        "            \"do_preprocessing\": self.do_preprocessing,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "        super(BertLayer, self).get_config()\n",
        "        return config_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIjYf1EL_nQA",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQLsvYzaysD",
        "colab_type": "code",
        "outputId": "2981f57d-aead-4d38-87b4-5dae288671ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=200, tune_embeddings=False,\n",
        "                    pooling='cls', n_tune_layers=24, verbose=False)\n",
        "\n",
        "l1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "l2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "# l3 = tf.keras.layers.Dense(1024, activation='relu')\n",
        "d = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(d(l2(l1(encoder(inp)))))\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DmN-x8-FgAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall + tf.keras.backend.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH-6kHUmZ7Uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                            np.unique(training_y),\n",
        "                                            training_y)\n",
        "class_weights /= max(class_weights)\n",
        "\n",
        "# print(class_weights) # [majority class weight, minority class weight]\n",
        "\n",
        "minority_count = len(training_labels_A[training_labels_A == 1])\n",
        "majority_count = len(training_labels_A[training_labels_A == 0])\n",
        "\n",
        "def weighted_loss(actual, predicted):\n",
        "    weights = class_weights\n",
        "    # weights = [0.0714, 1.0]\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    loss = bce(actual, predicted)\n",
        "   \n",
        "    return tf.keras.backend.mean(loss * weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuy1OMCbGTw",
        "colab_type": "code",
        "outputId": "4efb70c5-4596-4ca9-c274-a4fbc5d15ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=5e-6, ),\n",
        "      loss=weighted_loss,\n",
        "      metrics=[f1_m])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer (BertLayer)       (None, 1024)              335141888 \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 335,289,729\n",
            "Trainable params: 303,506,817\n",
            "Non-trainable params: 31,782,912\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7AbjTBifTl",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbagCxaieoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a2cppA_UDg5W",
        "outputId": "2ea108e7-6656-4623-fbb7-b8ae2c0450de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "saver = keras.callbacks.ModelCheckpoint(\"bert_large_H8_S256_B32.hdf5\")\n",
        "\n",
        "history = model.fit(training_x, training_y, validation_data=[validation_x, validation_y], batch_size=16, epochs=1, callbacks=[saver])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 101916 samples, validate on 11324 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/extract_features.py:283: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 33680/101916 [========>.....................] - ETA: 1:30:45 - loss: 0.1497 - f1_m: 0.7725"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2713d8a2eb64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert_large_H8_S256_B32.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwOSC395hUmY",
        "colab_type": "text"
      },
      "source": [
        "### Loss function graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-ROTbohUXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mM9p7JRErm4",
        "colab_type": "text"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJNgkArdBAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = model.predict(test_x, batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkP3H-PCEwxP",
        "colab_type": "text"
      },
      "source": [
        "### Count f1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLDV8k9M4kBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def round_int(array, threshold):\n",
        "  return np.array([np.ceil(x) if x >= threshold else np.floor(x) for x in array]).astype(np.float64)\n",
        "\n",
        "flat_predicted = round_int(predicted[:, 0], 0.34)\n",
        "print(f1_score(test_y, flat_predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rivD55Qc85D",
        "colab_type": "text"
      },
      "source": [
        "## Saving and restoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix02jwj_u_64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json.dump(model.to_json(), open(\"bert_H4_S256_B64.json\", \"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWsbzSaeSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.model_from_json(json.load(open(\"bert_H4_S256_B64.json\")), \n",
        "                                        custom_objects={\"BertLayer\": BertLayer})\n",
        "\n",
        "model.load_weights(\"bert_H5_S256_B64.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7H2AY9MsvUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(training_x[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_RWAQ-Um9AT",
        "colab_type": "text"
      },
      "source": [
        "### Freeze model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg5crVOofJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
        "\n",
        "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes a Keras model into a pruned computation graph.\n",
        "\n",
        "    @param model The Keras model to be freezed.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    \n",
        "    sess = tf.keras.backend.get_session()\n",
        "    graph = sess.graph\n",
        "    \n",
        "    with graph.as_default():\n",
        "\n",
        "        input_tensors = model.inputs\n",
        "        output_tensors = model.outputs\n",
        "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
        "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
        "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
        "        \n",
        "        tmp_g = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in tmp_g.node:\n",
        "                node.device = \"\"\n",
        "        \n",
        "        tmp_g = optimize_for_inference(\n",
        "            tmp_g, input_ops, output_ops, dtypes, False)\n",
        "        \n",
        "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
        "        \n",
        "        if export_path is not None:\n",
        "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
        "                f.write(tmp_g.SerializeToString())\n",
        "        \n",
        "        return tmp_g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMUrFpgofMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph_4_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShmlWdXLcH80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/gaphex/bert_experimental/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/bert_experimental\")\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.graph_ops import load_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKRLyiXvrqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "restored_graph = load_graph(\"frozen_graph_4_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8bT-YXrgo6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_ops = restored_graph.get_operations()\n",
        "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
        "print(input_op, output_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHr2ZQGfg3y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
        "y = restored_graph.get_tensor_by_name(output_op + ':0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoapr4e86Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQFQOML7ivdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRf75n6ECUa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session(graph=restored_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw55NB6YseBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = sess.run(y, feed_dict={\n",
        "        x: training_x[:10].reshape((-1,1))\n",
        "    })\n",
        "\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vAiiJEnfYpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}