{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy SemEval-2020 Task 12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ds8aZ8OFzw8",
        "colab_type": "code",
        "outputId": "6af2951c-f323-4fcb-9cb2-5a1fa139d90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install necessary packages -> uncomment what is currently needed\n",
        "\n",
        "!pip install unidecode\n",
        "!pip install contractions\n",
        "!pip install wordsegment\n",
        "!pip install -U symspellpy\n",
        "!pip install emoji --upgrade\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install bert-for-tf2\n",
        "!pip install transformers\n",
        "# !pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 4.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 6.3MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81700 sha256=f5244711b1e0e42c0a358995733063df2709aff9d622fb6748526baca511b5e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, textsearch, contractions\n",
            "Successfully installed contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Collecting wordsegment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/6c/e6f4734d6f7d28305f52ec81377d7ce7d1856b97b814278e9960183235ad/wordsegment-1.3.1-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 6.3MB/s \n",
            "\u001b[?25hInstalling collected packages: wordsegment\n",
            "Successfully installed wordsegment-1.3.1\n",
            "Collecting symspellpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/0b/2daa14bf1ed649fff0d072b2e51ae98d8b45cae6cf8fdda41be01ce6c289/symspellpy-6.5.2-py3-none-any.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.17.4)\n",
            "Installing collected packages: symspellpy\n",
            "Successfully installed symspellpy-6.5.2\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=13c70be9a23ced82e568a71abddde5a9cf58e9337eb0009ab4b69d0db505db66\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "Collecting imbalanced-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/aa/eba717a14df36f0b6f000ebfaf24c3189cd7987130f66cc3513efead8c2a/imbalanced_learn-0.6.1-py3-none-any.whl (162kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.14.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.17.4)\n",
            "Collecting scikit-learn>=0.22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/d0/860c4f6a7027e00acff373d9f5327f4ae3ed5872234b3cbdd7bcb52e5eff/scikit_learn-0.22-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 41.7MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\n",
            "  Found existing installation: scikit-learn 0.21.3\n",
            "    Uninstalling scikit-learn-0.21.3:\n",
            "      Successfully uninstalled scikit-learn-0.21.3\n",
            "  Found existing installation: imbalanced-learn 0.4.3\n",
            "    Uninstalling imbalanced-learn-0.4.3:\n",
            "      Successfully uninstalled imbalanced-learn-0.4.3\n",
            "Successfully installed imbalanced-learn-0.6.1 scikit-learn-0.22\n",
            "Collecting bert-for-tf2\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/d8/14e0cfa03bbeb72c314f0648267c490bcceec5e8fb25081ec31307b5509c/bert-for-tf2-0.12.6.tar.gz\n",
            "Collecting py-params>=0.7.3\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/d1/55d228c9a8fa565c518f410efdcb23baaf09a95e2ad637c012f64d5d1133/py-params-0.7.4.tar.gz\n",
            "Collecting params-flow>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/44/eb1414c6c201bf2bdaa3f037d2f7f35d13f3242003278cef47bf8b3aa681/params-flow-0.7.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (4.28.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.12.6-cp36-none-any.whl size=29115 sha256=91fe1d6e562d9d31eb6e735129d310e36a0a2e6fb463e35779faa61a423645b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/19/54/51eeca468b219a1bc910c54aff87f0648b28a1fb71c115ba0f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.7.4-cp36-none-any.whl size=4352 sha256=8204cf126b45992a26609a48648a1b9c6c6cd14f5283014a2e12d497200164fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/12/f9/07461c9970813d0452e4459e9d8f8bc0a1b951e140abf74301\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.7.2-cp36-none-any.whl size=16141 sha256=09c02aff06abea4bf9f969bd72e177a46e05071e480ce4ca0cb36fa373bddb0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/11/f0/cf35bb79050bd7ad8e058c98afeb3ac23c149060776c4283cf\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.12.6 params-flow-0.7.2 py-params-0.7.4\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/1a/364556102943cacde1ee00fdcae3b1615b39e52649eddbf54953e5b144c9/transformers-2.2.1-py3-none-any.whl (364kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 6.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.32)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/db/4b29a0adec5881542cd81cb5d1929b5c0787003c5740b3c921e627d9c2e5/regex-2019.12.9.tar.gz (669kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.32 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.32)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.32->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses, regex\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=e1441fdf97ffc536acc1e6fd625bd2a50b0db7777ee82137c08d8ef28d60c137\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.12.9-cp36-cp36m-linux_x86_64.whl size=609175 sha256=349a894f49be8f7f04643917ffa37dd2895041b34b023d8a1c6a7a2935a8e22e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/fb/b3/a89169557229468c49ca64f6839418f22461f6ee0a74f342b1\n",
            "Successfully built sacremoses regex\n",
            "Installing collected packages: sentencepiece, sacremoses, regex, transformers\n",
            "Successfully installed regex-2019.12.9 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7864AKjDrMsa",
        "colab_type": "code",
        "outputId": "5c03a479-05e6-4b5d-97dc-a96aa30f63de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "# All imports - DO NOT CHANGE THE ORDER OF INSTRUCTIONS\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "    sys.path.insert(0, 'bert_repo')\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "import unidecode\n",
        "import contractions\n",
        "import gensim.downloader as api\n",
        "import re\n",
        "import wordsegment\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import emoji\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from modeling import BertModel, BertConfig\n",
        "from tokenization import FullTokenizer, convert_to_unicode\n",
        "from extract_features import InputExample, convert_examples_to_features\n",
        "from tqdm import tqdm\n",
        "# import nltk\n",
        "from google.colab import auth, drive\n",
        "# nltk.download('punkt')\n",
        "\n",
        "wordsegment.load()\n",
        "\n",
        "# Load SymSpell -> package for correcting misspellings\n",
        "sym_spell = SymSpell(2, 7)\n",
        "\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "bigram_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
        "\n",
        "# get TF logger \n",
        "log = logging.getLogger('tensorflow')\n",
        "log.handlers = []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert_repo'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "remote: Total 336 (delta 0), reused 0 (delta 0), pack-reused 336\u001b[K\n",
            "Receiving objects: 100% (336/336), 283.40 KiB | 948.00 KiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mER1mPUtiaPl",
        "colab_type": "code",
        "outputId": "0230b8a6-33a9-4cd6-a79f-31268e0695ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "#Import data\n",
        "training_examples_url = 'https://raw.githubusercontent.com/piotrjaniszewski1/SemEval-2020-Task12/master/data2019/olid-training-v1.0.tsv'\n",
        "training_dataset = pd.read_csv(training_examples_url, delimiter='\\t')\n",
        "print(training_dataset.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id                                              tweet  ... subtask_b subtask_c\n",
            "0  86426  @USER She should ask a few native Americans wh...  ...       UNT       NaN\n",
            "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...  ...       TIN       IND\n",
            "2  16820  Amazon is investigating Chinese employees who ...  ...       NaN       NaN\n",
            "3  62688  @USER Someone should'veTaken\" this piece of sh...  ...       UNT       NaN\n",
            "4  43605  @USER @USER Obama wanted liberals &amp; illega...  ...       NaN       NaN\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73i5BO8TqoTE",
        "colab_type": "text"
      },
      "source": [
        "# **Training and validation sets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xHoiMYYlJQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_examples, validation_examples = train_test_split(training_dataset, test_size=0.1)\n",
        "\n",
        "# prepare training examples\n",
        "training_examples_A = training_examples['tweet'][training_examples['subtask_a'].notnull()]\n",
        "training_examples_B = training_examples['tweet'][training_examples['subtask_b'].notnull()]\n",
        "training_examples_C = training_examples['tweet'][training_examples['subtask_c'].notnull()]\n",
        "\n",
        "# prepare validation examples\n",
        "validation_examples_A = validation_examples['tweet'][validation_examples['subtask_a'].notnull()]\n",
        "validation_examples_B = validation_examples['tweet'][validation_examples['subtask_b'].notnull()]\n",
        "validation_examples_C = validation_examples['tweet'][validation_examples['subtask_c'].notnull()]\n",
        "\n",
        "# prepare training labels\n",
        "training_labels_A = (training_examples['subtask_a'][training_examples['subtask_a'].notnull()] == 'OFF').astype(int)\n",
        "training_labels_B = (training_examples['subtask_b'][training_examples['subtask_b'].notnull()] == 'TIN').astype(int)\n",
        "c_mapping = {'IND': 0, 'GRP': 1, 'OTH': 2}\n",
        "training_labels_C = training_examples['subtask_c'][training_examples['subtask_c'].notnull()].replace(c_mapping)\n",
        "\n",
        "# prepare validation labels\n",
        "validation_labels_A = (validation_examples['subtask_a'][validation_examples['subtask_a'].notnull()] == 'OFF').astype(int)\n",
        "validation_labels_B = (validation_examples['subtask_b'][validation_examples['subtask_b'].notnull()] == 'TIN').astype(int)\n",
        "validation_labels_C = (validation_examples['subtask_c'][validation_examples['subtask_c'].notnull()]).replace(c_mapping)\n",
        "\n",
        "\n",
        "training_x = np.array(training_examples_A)\n",
        "validation_x = np.array(validation_examples_A)\n",
        "training_y = np.array(training_labels_A)\n",
        "validation_y = np.array(validation_labels_A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2bNpHc9qMpw",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fao-Vte1gtJ2",
        "colab_type": "text"
      },
      "source": [
        "### Common preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re0mCq1ogs6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html tags if exist\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    stripped_text = soup.get_text(separator=' ')\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "# remove unnecessary whitespaces\n",
        "def remove_whitespace(text):\n",
        "    text = text.strip()\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# remove accented chars (e.g. caffè -> caffe)\n",
        "def remove_accented_chars(text):\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "# remove hashes and split words (e.g. '#fortTrump' -> 'fort trump')\n",
        "def split_hashtags(text):\n",
        "    splitted = text.split()\n",
        "    new_word_sequence = []\n",
        "\n",
        "    for chunk in splitted:\n",
        "        if chunk[0] == '#':\n",
        "            chunk = chunk[1:]\n",
        "            new_word_sequence.extend(wordsegment.segment(chunk))\n",
        "        else:\n",
        "            new_word_sequence.append(chunk)\n",
        "        \n",
        "    return ' '.join(tuple(new_word_sequence))\n",
        "\n",
        "\n",
        "def substitute_emojis(text):\n",
        "    demojized_text = emoji.demojize(text)\n",
        "    return re.compile('[_:]+').sub(' ', demojized_text)\n",
        "\n",
        "\n",
        "def preprocess_common(text):\n",
        "    text = strip_html_tags(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = split_hashtags(text)\n",
        "    text = substitute_emojis(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4Gjm9OhOzug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove redundant @user tokens\n",
        "def remove_redundant_users(example):\n",
        "    user_count = 0\n",
        "    new_example = example[:]\n",
        "    for i, token in reversed(list(enumerate(example))):\n",
        "        if token == '@user':\n",
        "            user_count += 1\n",
        "        if user_count > 3:\n",
        "            new_example.pop(i)\n",
        "    else:\n",
        "        user_count = 0\n",
        "\n",
        "    return new_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pJMq-8ygixr",
        "colab_type": "text"
      },
      "source": [
        "### Spacy preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPYd37I_giKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try leaving '?' and '!' as far as punctuation is concerned\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# exclude negation words from spacy stopwords list\n",
        "deselect_stop_words = ['no', 'not', 'noone', 'none', 'lacks', 'lack', 'nor', 'never', 'neighter', 'hardly', 'nobody', 'nothing', 'lacking', 'nowhere']\n",
        "for w in deselect_stop_words:\n",
        "    nlp.vocab[w].is_stop = False\n",
        "\n",
        "def preprocess_spacy(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    clean_text = []\n",
        "    \n",
        "    for token in doc:\n",
        "        flag = True\n",
        "        edit = token.text\n",
        "\n",
        "        # remove punctuations\n",
        "        if token.pos_ == 'PUNCT' and flag == True and token.text != '@user': \n",
        "            flag = False\n",
        "       \n",
        "        # remove special characters\n",
        "        if token.pos_ == 'SYM' and flag == True: \n",
        "            flag = False\n",
        "        \n",
        "        # remove numbers\n",
        "        if (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n",
        "            flag = False\n",
        "\n",
        "        # correct misspelings\n",
        "        if flag == True:\n",
        "            suggestions = sym_spell.lookup(edit, Verbosity.TOP, 2)\n",
        "            if len(suggestions) > 0:\n",
        "                edit = suggestions[0].term\n",
        "\n",
        "        # remove stop words\n",
        "        if token.is_stop and token.pos_ != 'NUM': \n",
        "            flag = False\n",
        "\n",
        "        # convert tokens to base form\n",
        "        elif token.lemma_ != '-PRON-' and flag == True:\n",
        "            edit = token.lemma_\n",
        "\n",
        "        # append tokens edited and not removed to list \n",
        "        if edit != '' and flag == True:\n",
        "            clean_text.append(edit)        \n",
        "    \n",
        "    return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyyCkwWhFw1",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAHp6jlH12sD",
        "colab_type": "code",
        "outputId": "516e254f-f898-4629-e2d2-ad70e948307f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        }
      },
      "source": [
        "# normalization -> papers, complicated solutions, replace abbreviations with full names (e.g. MAGA)\n",
        "# check removing less stop words (some may have some significance)\n",
        "\n",
        "cleaned_x = [preprocess_spacy(example) for example in training_x[0:30]]\n",
        "reduced_users_x = [remove_redundant_users(example) for example in cleaned_x]\n",
        "print(reduced_users_x[0:30])\n",
        "print(training_x[0:30])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['@USER', 'peanut', 'farm', 'old', 'man'], ['@USER', '@USER', 'fish'], ['@USER', '@USER', 'stoned', 'Wheat', 'Crackers', 'hell', 'Stoned', 'Wheat', 'cracker'], ['@user', 'babe', 'hope', 'not', 'wonte', 'woman'], ['@USER', '@USER', '@USER', '@USER', '@USER', '@USER', 'popular', 'vote', 'terrible', 'democracy'], ['@user', 'good', 'booty', 'stunning', 'love', 'want', 'cum', 'deep'], ['@USER', '@USER', 'Corbyn', 'antisemitism', 'believe', 'way', 'racist', 'see', 'believe', 'antisemitism', 'go', 'leadership', 'continue', 'conservative', 'show', 'actively', 'support'], ['@USER', '@USER', 'of', 'bitch', 'psycho'], ['@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', 'follow', 'not', 'follow'], ['@USER', 'loot', 'money', 'repay', 'debt'], ['@USER', 'hope', 'not', 'block', 'Monday', 'want', 'page', '’s', 'reaction', 'Kavanaugh', 'get', 'confirm', 'Supreme', 'Court', 'gun', 'control', 'effectively', 'dead'], ['@USER', 'Bull', 'g.soros', 'pay', 'statement', 'PRESIDENT', 'country'], ['@USER', '@USER', 'sure', 'not', 'think', 'free', 'nothing', 'wrong', '😂'], ['@USER', 'desperate', 'liberal', 'look', 'like', 'spygate', 'ObamaGate'], ['HOUSEHOLD', 'net', 'WORTH', '  ', '  ', 'TRILLION', '\\xa0  ', 'url', '\\xa0  ', '#', 'MAGA'], ['@user', 'come', 'corrupt', 'AG', '’s'], ['@USER', '@USER', '@USER', 'BLM', 'brat', 'block', 'highway', 'ANTIFA', 'thug', 'swinge', 'bike', 'lock', 'come', 'screamer', 'gallery', 'childish', 'scoundrel', 'scream', 'rape', 'sit', 'middle', 'room', 'stick', 'thumb', 'mouth', 'whine', 'prove', 'wrong'], ['@USER', '@USER', '@USER', 'not'], ['@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', '@USER', 'Antifa', 'constantly', 'lie', 'url'], ['@USER', 'complain', 'SP', 'comment', 'blame', 'medium', 'him(Spath', 'feel', 'certain', 'way', 'not', 'actually', 'read', 'article', 'basically', 'say', 'Shea', 'know', 'shit', 'basically', 'shut', 'mouth'], ['@USER', '@USER', 'Brandon', 'Lewis', 'express', 'url'], ['@USER', '@USER', 'kid', 'break', 'try', 'bring', 'change', 'desperately', 'need', 'change', 'violent', 'society'], ['@USER', '@USER', 'Mob', 'mentality'], ['@USER', '@USER', '@USER', '@USER', '@USER', 'aaaah', 'thank', 'u', 'sm'], ['@USER', 'definitely', 'not', 'man', 'God'], ['@USER', 'Trump', 'call', 'Trudeau', 'dishonest', 'weak', 'Conservatives', 'cheer', 'conservative', 'Canada', 'Trump', 'supporter', 'accept', 'crumb', 'fall', 'Trumps', 'table'], ['@USER', 'know', '😂', '😂', '😂', 'blow', 'coverage', 'give', 'Ole', 'Miss', 'TD'], ['@USER', '@USER', '@USER', '@USER', '@USER', 'Antifa', 'mean', 'anti', '-', 'fascist', 'not', 'sure', 'Antifa', 'organization', 'million', 'anti', '-', 'fascist', ' ', 'not'], ['@user', 'crap', 'come', 'excuse', 'come', 'dogooodther', 'lawyer', 'lefty', 'liberal', 'commit', 'time', 'assist', 'help', 'criminal', ' ', 'excuse', 'mate', 'commit', 'crime', 'pay', 'hard', 'long'], ['@USER', 'small', ' ', 'need', 'big', 'clear', 'beach']]\n",
            "['@USER Get back on your peanut farm old man'\n",
            " '@USER @USER take your fish and get out!'\n",
            " '@USER @USER My Stoned Wheat Crackers! Where the hell did they put the Stoned Wheat Crackers?'\n",
            " '@USER babe i hope you are not wonting these women'\n",
            " '@USER @USER @USER @USER @USER @USER A popular vote would be terrible! We would become a Democracy!'\n",
            " '@USER you have the best booty and everything you are stunning love you so want to cum in you so deep!'\n",
            " \"@USER @USER I could only call Corbyn out on antisemitism if I believed he was in any way racist. I just haven't seen anything to believe it. Antisemitism has gone down under his leadership and will continue to do so. The conservatives have just shown they actively support it.\"\n",
            " '@USER @USER y’all bitches psycho'\n",
            " '@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Followed all those that I haven’t. Please follow back.'\n",
            " '@USER He looted our money and now he is repaying our debts'\n",
            " '@USER I really hope you don’t block me until after next Monday. I want to see this page’s reaction when Kavanaugh gets confirmed to the Supreme Court. That gun control is effectively dead.'\n",
            " '@USER Bull ! How much is G.SOROS paying you for your statements against the PRESIDENT of out country ?'\n",
            " \"@USER @USER I'm sure you are not. You just thought it was free 😁 there's nothing wrong with that 😂\"\n",
            " '@USER This is what desperate liberals look like. #SpyGate #ObamaGate'\n",
            " 'HOUSEHOLD NET WORTH:\\xa0$107\\xa0TRILLION\\xa0  URL\\xa0  #MAGA'\n",
            " '@USER Coming from one of the most corrupt AG’s!'\n",
            " '@USER @USER @USER Just another BLM brat blocking a highway, ANTIFA thug swinging a bike lock. Here comes just another screamer from the gallery - except this childish scoundrel screamed he RAPED me!\", sat down in the middle of the room, stuck her thumb in her mouth, and whined \"prove me wrong!\"\"'\n",
            " '@USER @USER @USER You are not alone'\n",
            " '@USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER @USER Antifa constantly lies and... URL'\n",
            " \"@USER Complaining about SP's comments and blames the media for him(Spath) feeling a certain way and not actually reading an article. Basically saying Shea doesn't know shit and basically he should shut his mouth.\"\n",
            " '@USER @USER The Brandon Lewis express URL'\n",
            " '@USER @USER Give the kid a break! He is trying to bring about change! We desperately need something to change our violent society!'\n",
            " '@USER @USER Mob mentality...'\n",
            " '@USER @USER @USER @USER @USER aaaah thank u sm'\n",
            " '@USER You are most definitely not a man of God.'\n",
            " '@USER Trump called Trudeau very dishonest and weak\" and the Conservatives cheer. Conservatives in Canada are Trump supporters. They will accept any crumbs that fall off Trumps table.\"'\n",
            " '@USER You already know he is 😂😂😂 and then blown coverages that gave Ole Miss TD'\n",
            " '@USER @USER @USER @USER @USER Antifa means anti-fascist\". I\\'m not sure there is an Antifa organization, but there are millions who are anti-fascist.  Why are you not one of them?\"'\n",
            " '@USER The crap they come up with as excuses\" comes from dogooodthers, lawyers, lefties, liberals who commit their time to assisting and helping these criminals.  No excuses mate, you committed a crime, you pay for it, hard and long!\"'\n",
            " '@USER he is small  and he needs someone BIG to clear the beach for him']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btAs2RDT9uoS",
        "colab_type": "text"
      },
      "source": [
        "# **BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp5AzW_zilHm",
        "colab_type": "text"
      },
      "source": [
        "### BERT Preprocessing - single example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3woANjZiltJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctation(tokens):\n",
        "    punctuation = [char for char in string.punctuation]\n",
        "    return list(filter(lambda token: token not in punctuation, tokens))\n",
        "\n",
        "\n",
        "def preprocess_bert_before_tokenization(text):\n",
        "    preprocessed_text = preprocess_common(text)\n",
        "    spacy_x = preprocess_spacy(preprocessed_text)\n",
        "    cleaned_x = remove_redundant_users(spacy_x)\n",
        "    return ' '.join(cleaned_x)\n",
        "\n",
        "\n",
        "def preprocess_bert_after_tokenization(tokenized_x):\n",
        "    # update input_ids, input_mask, input_type_ids\n",
        "    return remove_punctation(cleaned_x) # remove i.a. hyphens remainings from words like 'de-platforming'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkqKH8ssRuFI",
        "colab_type": "text"
      },
      "source": [
        "### Getting the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxg4a3Y97Mt",
        "colab_type": "code",
        "outputId": "d48d5d0f-7370-4d95-c6ac-7a5b256c594f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-12 14:24:01--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.133.128, 2a00:1450:400c:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.133.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   165MB/s    in 2.4s    \n",
            "\n",
            "2019-12-12 14:24:03 (165 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlzqsoCERraJ",
        "colab_type": "text"
      },
      "source": [
        "### Building a tf.Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOyKrgZRRqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_module_fn(config_path, vocab_path, do_lower_case=True):\n",
        "\n",
        "    def bert_module_fn(is_training):\n",
        "        \"\"\"Spec function for a token embedding module.\"\"\"\n",
        "\n",
        "        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n",
        "        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n",
        "        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "        config = BertConfig.from_json_file(config_path)\n",
        "        model = BertModel(config=config, is_training=is_training,\n",
        "                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n",
        "          \n",
        "        seq_output = model.all_encoder_layers[-1]\n",
        "        pool_output = model.get_pooled_output()\n",
        "\n",
        "        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n",
        "        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n",
        "        lower_case = tf.constant(do_lower_case)\n",
        "\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n",
        "        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n",
        "        \n",
        "        input_map = {\"input_ids\": input_ids,\n",
        "                     \"input_mask\": input_mask,\n",
        "                     \"segment_ids\": token_type}\n",
        "        \n",
        "        output_map = {\"pooled_output\": pool_output,\n",
        "                      \"sequence_output\": seq_output}\n",
        "\n",
        "        output_info_map = {\"vocab_file\": vocab_file,\n",
        "                           \"do_lower_case\": lower_case}\n",
        "                \n",
        "        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n",
        "        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n",
        "\n",
        "    return bert_module_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubGdlgtTsjW1",
        "colab_type": "text"
      },
      "source": [
        "### Exporting the module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fShtfnSQbO",
        "colab_type": "code",
        "outputId": "6d43b783-f69b-48bf-905c-f6cf17a525ef",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "source": [
        "MODEL_DIR = \"uncased_L-12_H-768_A-12\" #@param {type:\"string\"} ['uncased_L-12_H-768_A-12']\n",
        "\n",
        "config_path = \"/content/{}/bert_config.json\".format(MODEL_DIR)\n",
        "vocab_path = \"/content/{}/vocab.txt\".format(MODEL_DIR)\n",
        "\n",
        "tags_and_args = []\n",
        "for is_training in (True, False):\n",
        "  tags = set()\n",
        "  if is_training:\n",
        "    tags.add(\"train\")\n",
        "  tags_and_args.append((tags, dict(is_training=is_training)))\n",
        "\n",
        "module_fn = build_module_fn(config_path, vocab_path)\n",
        "spec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\n",
        "spec.export(\"bert-module\", \n",
        "            checkpoint_path=\"/content/{}/bert_model.ckpt\".format(MODEL_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From bert_repo/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "From bert_repo/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "From bert_repo/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "From bert_repo/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "From bert_repo/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_hub/saved_model_lib.py:110: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xgf8hzNYMst",
        "colab_type": "text"
      },
      "source": [
        "### Building the text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMkUX6C5V3r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_examples(str_list):\n",
        "    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n",
        "    unique_id = 0\n",
        "    for s in str_list:\n",
        "        line = convert_to_unicode(s)\n",
        "        if not line:\n",
        "            continue\n",
        "        \n",
        "        text_a = line.strip()\n",
        "        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=None)\n",
        "        unique_id += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmsepoRxVsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.input_type_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_061naESlic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_preprocessor(voc_path, seq_len, lower=True):\n",
        "    tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n",
        "\n",
        "    def strings_to_arrays(sents):\n",
        "\n",
        "        sents = np.atleast_1d(sents).reshape((-1,))\n",
        "\n",
        "        examples = []\n",
        "        for example in read_examples(sents):\n",
        "            example.text_a = preprocess_bert_before_tokenization(example.text_a)\n",
        "            examples.append(example)\n",
        "\n",
        "        features = convert_examples_to_features(examples, seq_len, tokenizer)\n",
        "        arrays = features_to_arrays(features)\n",
        "        \n",
        "        return arrays\n",
        "\n",
        "    return strings_to_arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycbxDVlalim",
        "colab_type": "text"
      },
      "source": [
        "### Implementing a BERT Keras layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIIv5wCKUkgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n",
        "                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n",
        "                 tune_embeddings=False, trainable=True, **kwargs):\n",
        "\n",
        "        self.trainable = trainable\n",
        "        self.n_tune_layers = n_tune_layers\n",
        "        self.tune_embeddings = tune_embeddings\n",
        "        self.do_preprocessing = do_preprocessing\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.seq_len = seq_len\n",
        "        self.pooling = pooling\n",
        "        self.bert_path = bert_path\n",
        "\n",
        "        self.var_per_encoder = 16\n",
        "        if self.pooling not in [\"cls\", \"mean\", None]:\n",
        "            raise NameError(\n",
        "                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n",
        "            )\n",
        "\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.bert = hub.Module(self.build_abspath(self.bert_path), \n",
        "                               trainable=self.trainable, name=f\"{self.name}_module\")\n",
        "\n",
        "        trainable_layers = []\n",
        "        if self.tune_embeddings:\n",
        "            trainable_layers.append(\"embeddings\")\n",
        "\n",
        "        if self.pooling == \"cls\":\n",
        "            trainable_layers.append(\"pooler\")\n",
        "\n",
        "        if self.n_tune_layers > 0:\n",
        "            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n",
        "            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n",
        "            for i in range(self.n_tune_layers):\n",
        "                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n",
        "        \n",
        "        # Add module variables to layer's trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if any([l in var.name for l in trainable_layers]):\n",
        "                self._trainable_weights.append(var)\n",
        "            else:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        self.build_preprocessor()\n",
        "        self.initialize_module()\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def build_abspath(self, path):\n",
        "        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n",
        "          return path\n",
        "        else:\n",
        "          return os.path.abspath(path)\n",
        "\n",
        "    def build_preprocessor(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n",
        "\n",
        "    def initialize_module(self):\n",
        "        sess = tf.keras.backend.get_session()\n",
        "        \n",
        "        vars_initialized = sess.run([tf.is_variable_initialized(var) \n",
        "                                     for var in self.bert.variables])\n",
        "\n",
        "        uninitialized = []\n",
        "        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n",
        "            if not is_initialized:\n",
        "                uninitialized.append(var)\n",
        "\n",
        "        if len(uninitialized):\n",
        "            sess.run(tf.variables_initializer(uninitialized))\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "        if self.do_preprocessing:\n",
        "          input = tf.numpy_function(self.preprocessor, \n",
        "                                    [input], [tf.int32, tf.int32, tf.int32], \n",
        "                                    name='preprocessor')\n",
        "          for feature in input:\n",
        "            feature.set_shape((None, self.seq_len))\n",
        "        \n",
        "        input_ids, input_mask, segment_ids = input\n",
        "        \n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        \n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "            \n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            \n",
        "            if self.pooling == \"mean\":\n",
        "              pooled = masked_reduce_mean(result, input_mask)\n",
        "            else:\n",
        "              pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def get_config(self):\n",
        "        config_dict = {\n",
        "            \"bert_path\": self.bert_path, \n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pooling\": self.pooling,\n",
        "            \"n_tune_layers\": self.n_tune_layers,\n",
        "            \"tune_embeddings\": self.tune_embeddings,\n",
        "            \"do_preprocessing\": self.do_preprocessing,\n",
        "            \"verbose\": self.verbose\n",
        "        }\n",
        "        super(BertLayer, self).get_config()\n",
        "        return config_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIjYf1EL_nQA",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BQLsvYzaysD",
        "colab_type": "code",
        "outputId": "7f3bb055-fab6-4d9c-f6d2-9737af1017c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "inp = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "encoder = BertLayer(bert_path=\"./bert-module/\", seq_len=256, tune_embeddings=False,\n",
        "                    pooling='cls', n_tune_layers=3, verbose=False)\n",
        "\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(encoder(inp))\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pred])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuy1OMCbGTw",
        "colab_type": "code",
        "outputId": "e1bfc689-3bd6-460d-c318-6642d83df5a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, ),\n",
        "      loss=\"binary_crossentropy\",\n",
        "      metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "bert_layer_1 (BertLayer)     (None, 768)               109482240 \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 769       \n",
            "=================================================================\n",
            "Total params: 109,483,009\n",
            "Trainable params: 21,854,977\n",
            "Non-trainable params: 87,628,032\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7AbjTBifTl",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lbagCxaieoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6svZ93niek0",
        "colab_type": "code",
        "outputId": "2eba4f24-12d6-4182-9919-66e441bc68d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "saver = keras.callbacks.ModelCheckpoint(\"bert_256_64.hdf5\")\n",
        "\n",
        "history = model.fit(training_x, training_y, validation_data=[validation_x, validation_y], batch_size=64, epochs=2, callbacks=[saver])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/2\n",
            "11916/11916 [==============================] - 357s 30ms/sample - loss: 0.4671 - acc: 0.7782 - val_loss: 0.4681 - val_acc: 0.7855\n",
            "Epoch 2/2\n",
            "11916/11916 [==============================] - 354s 30ms/sample - loss: 0.4393 - acc: 0.7963 - val_loss: 0.4609 - val_acc: 0.7772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwOSC395hUmY",
        "colab_type": "text"
      },
      "source": [
        "### Loss function graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-ROTbohUXE",
        "colab_type": "code",
        "outputId": "eac19ebd-b263-40b2-ec12-0e500df115df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXwV5dn/8c+VnSVsSQBZg8qOChIR\nF0R/iOIGKu5Lq/aR2taKQm21T22r7VNrW3FprWu1Veu+VOoGgoK4gEREFmUXJaAS9jWQkOv3x0zI\nIQZISE5OTs73/XqdV3Jm7plz30C4MvOducfcHRERkapKinUHREQkvqhwiIhItahwiIhItahwiIhI\ntahwiIhItahwiIhItahwiESRmf3TzH5fxbbLzezkmu5HJNpUOEREpFpUOEREpFpUOCThhaeIbjSz\nOWa21cz+YWZtzOwNM9tsZpPMrGVE++FmNt/MNpjZFDPrGbGun5nNCrd7Fsio8FlnmtnscNsPzOzw\nA+zz1Wa2xMzWmdl4M2sXLjczu8vMVpvZJjOba2Z9wnWnm9lnYd9WmtnPDugPTBKeCodIYCQwFOgG\nnAW8AfwSyCH4ObkOwMy6AU8D14frXgf+a2ZpZpYG/Ad4AmgFPB/ul3DbfsCjwA+BLOBBYLyZpVen\no2b2/4DbgQuAg4AvgWfC1acAJ4TjaB62WRuu+wfwQ3fPBPoAb1fnc0XKqHCIBP7q7t+6+0pgGjDD\n3T9x9yLgZaBf2O5C4DV3f8vdi4G/AI2AY4GBQCpwt7sXu/sLwMyIzxgFPOjuM9x9l7v/C9gRblcd\nlwKPuvssd98B3AwcY2a5QDGQCfQAzN0/d/evw+2KgV5m1szd17v7rGp+rgigwiFS5tuI77dX8r5p\n+H07gt/wAXD3UmAF0D5ct9L3nDn0y4jvOwNjw9NUG8xsA9Ax3K46KvZhC8FRRXt3fxv4G3AfsNrM\nHjKzZmHTkcDpwJdmNtXMjqnm54oAKhwi1bWKoAAAQaZA8J//SuBroH24rEyniO9XAP/n7i0iXo3d\n/eka9qEJwamvlQDufq+79wd6EZyyujFcPtPdRwCtCU6pPVfNzxUBVDhEqus54AwzG2JmqcBYgtNN\nHwAfAiXAdWaWambnAgMitn0YuMbMjg5D7CZmdoaZZVazD08DV5pZ3zAf+QPBqbXlZnZUuP9UYCtQ\nBJSGGcylZtY8PMW2CSitwZ+DJDAVDpFqcPeFwGXAX4E1BEH6We6+0913AucCVwDrCPKQlyK2zQeu\nJjiVtB5YEratbh8mAbcALxIc5RwCXBSubkZQoNYTnM5aC/w5XHc5sNzMNgHXEGQlItVmepCTiIhU\nh444RESkWlQ4RESkWlQ4RESkWlQ4RESkWlJi3YG6kJ2d7bm5ubHuhohIXPn444/XuHtOxeUJUThy\nc3PJz8+PdTdEROKKmX1Z2XKdqhIRkWpR4RARkWpR4RARkWpJiIyjMsXFxRQUFFBUVBTrrkRVRkYG\nHTp0IDU1NdZdEZEGImELR0FBAZmZmeTm5rLnZKYNh7uzdu1aCgoK6NKlS6y7IyINRMKeqioqKiIr\nK6vBFg0AMyMrK6vBH1WJSN1K2MIBNOiiUSYRxigidSthT1VVyZbC4GtKOqRmQFIq6D9iEUlwCX3E\nsV/b1sCmAli3FL6dD9/MhcKFsP5L2PItFG2Ekh1wAFPTb9iwgb///e/V3u70009nw4YN1d5ORKS2\n6IhjX3J6QGkJlBQFr+Lw645NsH1dRMOk4KgkJSM4Mkkpe6WBVV6bywrHj3/84z2Wl5SUkJKy97+W\n119/vTZGJiJywFQ49sUMklODV3qFp3uWlpQXkpIdULIdirdC0frIHQTFIyWjwiudm266iaVLl9K3\nb19SU1PJyMigZcuWLFiwgEWLFnH22WezYsUKioqKGD16NKNGjQLKp0/ZsmULp512GscffzwffPAB\n7du355VXXqFRo0Z19+cjIglJhQO49b/z+WzVplram4M7vdpk8JshB5UfqRRt3KPVH8dexbxPZzF7\n6mtM+SCfM867hHlzZtPlkK4APProo7Rq1Yrt27dz1FFHMXLkSLKysvbYx+LFi3n66ad5+OGHueCC\nC3jxxRe57LLLamkcIiKVU+GodRYcqaRkQLN25Yu9NDwyCY9SUrcG2cjWQti6mgFH9KJLoy3wzTxI\nSefev/ydl1+bCJbEihUrWLxoEVnHHLPHJ3Xp0oW+ffsC0L9/f5YvX16H4xSRRKXCAfzmrN7R/xBL\ngtRGwQugxY4gFznoCGi5miYtsiCzHZQUMWXqVCa9PYUP//MwjRs14sTzrqZo1WdQmAWlu4KrvXaU\nkJ6eFhQfM5KTk9m+fXv0xyEiCU+FYx+27ywhNTmJlOTav/gsMzOTzZs3hzlKGiSlQGYbADZac1q2\n6UDjznksmD+H6bPmQXrToK2XwpavYev24AjmmzlBAdq2HnbsgO0bducounRYRKJBhWMv3J0v126j\n1KFdiwyaN0qt1ZvpsrKyOO644+jTpw+NGjWiTZs2u9cNGzaMBx54gJ6HHUH37t0ZOHAgNG0D2V2D\noD6nJ6SuC+4raZwVZCi7dsDOzbD+i3AvVn6lV9FGmPciZHeHrEODK79ERA6Q+QHcgxBv8vLyvOKD\nnD7//HN69uy5z+227yyhYP12thfvIjMjlXYtMkhPSY5mV2umdFd5hhJx+fDny1bSc8L5QRtLgpa5\nwaXG2d0gp3vwyu723SvHRCShmdnH7p5XcbmOOPahUVoKh7ZuytqtO/lmYxGLv91C62bpZDdNJ6k+\nngZKSoa0JsEr0lqDa96HwgWwZlHwtXARLH4LSovL2zVrHxaR7hEFpTs02fNqLhFJbCoc+2FmZDdN\np1lGKl9v3M43G4vYsK2Y9i0a0SQ9Tv74zKBtn+AVaVdJcGqrcGFEUVkIs/4FxdvK2zXO3rOQ5HQL\njlgyD1KOIpKA4uR/vthLS0mic1YTNm4vZtWG7Swt3EJWkzTaNM8gJSlOZ25JTglyk+yu0PPM8uWl\npcFUK4ULg9ea8Ou8l6AoYrqT9Gblp7uyw2KS0w1adA6OfkSkQVLhqKbmjVJpmp7Ct5uKWLtlBxu3\nl0QlPI+ppCRo0Sl4dR1avtwdtqwuLyRlRWXJJJj97/J2KRmQ1bX8yKSsqLQ6OLiTXkTimgrHAUhO\nMtq1aESLxqmsXL+dr9ZtIzMjlfYtMkirz+F5TZkFlwxntoEuJ+y5bvt6WLM4zE/ColIwM7iaa/f2\nyUHxKDvtVVZUsrtBWuO6HYuIHDAVjhpoXBaeb9nJN5uKWFTfw/NoatQSOg4IXpF2bg0Kyu5QfmHw\n/cI3wHeVt2vR6buhfE63YL8iUq+ocNSQmZGdmU6zRqms2lD18HzDhg089dRT35kdtyruvvtuRo0a\nRePGcfBbeloTaNc3eEUq2Qnrlu152qtwISyfFlxKXKZpm8qv9GraWsG8SIyocNSStJQkcrOrHp7v\nbVr1qrj77ru57LLL4qNw7E1KGrTuEbwile6CDV/tGcoXLoQ5zwbT2ZfJaFEhlA+LSrMOQUYjIlGj\nwlHLvhOeF5XQrvl3w/PIadWHDh1K69atee6559ixYwfnnHMOt956K1u3buWCCy6goKCAXbt2ccst\nt/Dtt9+yatUqTjrpJLKzs3nnnXdiONooSEqGVl2CV/dh5cvdYfPXFa70Ck95ffJEebvUxsFVYpGh\nfE53aNkluIpMRGosqj9JZjYMuAdIBh5x9z/upd1I4AXgKHfPD5cdDjwINANKw3VFZjYFOAgom9Hv\nFHdfXaOOvnFT8HS/WpIMtGt7GC2G/H6v4fkf//hH5s2bx+zZs5k4cSIvvPACH330Ee7O8OHDeffd\ndyksLKRdu3a89tprAGzcuJHmzZszbtw43nnnHbKzs2utz/WeWTDbcLN2cMhJe67burb86KQsS1n+\nfnCUUiYpNZhuJafbnqe9srpqChaRaopa4TCzZOA+YChQAMw0s/Hu/lmFdpnAaGBGxLIU4Engcnf/\n1MyygIhbnLm0rMDUZ2Xh+ZotO/m2QngeaeLEiUycOJF+/foBsGXLFhYvXsygQYMYO3Ysv/jFLzjz\nzDMZNGhQLIZR/zXJgibHQudj91y+Y3NYSBaV3+D4zVz4/L/BZJEQTMHSonOFUL5HcNSS0azuxyIS\nB6J5xDEAWOLuywDM7BlgBPBZhXa/A+4AboxYdgowx90/BXD3tVHsJ5xW6YFQrTAzcjLTaV4hPC/Z\nUbK7jbtz880388Mf/vA728+aNYvXX3+dX/3qVwwZMoRf//rXUetrg5OeCe37B69IxUXBc+TLpl4p\nKypL34ZdO8vbZbaLuHQ4IqBvkkBHeiKViGbhaA+siHhfABwd2cDMjgQ6uvtrZhZZOLoBbmYTgBzg\nGXf/U8T6x8xsF/Ai8HuPg5kaK4bna3YmsWHjJnaVlnLqqadyyy23cOmll9K0aVNWrlxJamoqJSUl\ntGrVissuu4wWLVrwyCOPAOVTsifUqaralJoBbXoHr0i7SmD98vC0V1hU1iyEWU8EjwUu0zirwlVe\nYZbSrJ2u9JKEELO00MySgHHAFZWsTgGOB44CtgGTw1kaJxOcploZnuJ6EbgceLyS/Y8CRgF06tQp\nKmM4EGXhefNGqRzefwA9evVh2LBhXHzxxRwTPuGvadOmPPnkkyxZsoQbb7yRpKQkUlNTuf/++wEY\nNWoUw4YNo127dg0vHI+l5BTIPjR49TijfHlpKWxa+d0rvea/vOcULGmZERlKxF3zLXM1BYs0KFGb\nVt3MjgF+6+6nhu9vBnD328P3zYGlwJZwk7bAOmA4cChwmrt/P2x7C1Dk7n+u8BlXAHnufu2++nKg\n06pH27adJayMmLY9Wnee14exNkhlj/6tWFDWLAquACuTnB7OCRYxn1dOD2h1iKZgkXotFtOqzwS6\nmlkXYCVwEXBJ2Up33wjsPtcSXi31M3fPN7OlwM/NrDGwExgM3BWG5i3cfY2ZpQJnApOiOIaoqiw8\nb9MsnaxEvPM8HpkFNyI2bQ1dKly4sH1D+RQsZZcOr/w4OEoh/GXNwkuP97h0uGwKlibf+TiR+iJq\nhcPdS8zsWmACwRWqj7r7fDO7Dch39/H72Ha9mY0jKD4OvB7mIE2ACWHRSCYoGg9Hawx1oWJ4/vXG\nItZvK6ZDi0Y0jpdp2+W7GrWAjkcFr0g7t8HaxeX5SVmWsuhNKC2/YILmnSpMEhnmKZqCReqBhH4C\nYI8ePerVjLbuzqaiElZt2E7xrlKymqbTtlk6yTW4E9rdWbBggU5V1Xe7ioMpWPa4wXFBcNQSOQVL\nk9bfvcorp3swNUs9+rcsDYOeAFhBRkYGa9euJSsrq94UDzMLw/Nkvt20gzVbdrBpezHtmmfQ7ACm\nbXd31q5dS0aGbnCr95JTy4tApNJS2PjVnvN5rVkIc56HHRvL22U0rxDKh/tq3lFTsEitS9gjjuLi\nYgoKCigqKtrLVrG3s6SUDdt2snOX0yg1ieaNU6v90KiMjAw6dOhAampqlHopMeEOm7/5bihfuCAI\n7MukNg7vmO+xZ1Fp1SUoViL7oCOOClJTU+nSpUusu7FfJbtK+ecHy7nzjUUA3DC0K1cd14WUZP0W\nmdDMoNlBwevgE/dct23dnvN5FS6Arz6Euc+Vt0lKhaxD9pzPK7tbcPVXaqO6HInEoYQ94og3Beu3\n8ZtX5jN5wWp6HtSMP5zTh36dFJRKNezYEhyV7H42Svh1/RflU7Bg0LJzhVA+/F5TsCScvR1xqHDE\nEXdnwvxv+M34+azevIPLB3bmZ6d2p1mGTjlIDZTsgLVLy6deKSsqaxdXMgVLhUkic3poCpYGTIWj\nARSOMpuLirlz4iL+9eFyWmem89uzejOsT9t6E/JLA7GrBDZ8GWYoZUUlzFJ2bilv16jVd5/cmNMD\nmrXXlV5xToWjARWOMrNXbOCXL83ls683MaRHa24d0ZsOLeP44U4SH9zLp2CpeNf89nXl7dKalp/u\n2uPZKLmagiVOqHA0wMIBEeH5xCA8HzO0G1cel6vwXGJj65ryZ8tHBvSbV5W3SU4vfzZK5F3zWYdA\nSvre9y11ToWjgRaOMpHhea+DmvGHcw+jb8cWse6WSKBoY/kULJGXDq//kj2mYGmZW+HS4XAKlvSm\nsex9wlLhaOCFA4Lw/M15QXheuGUH3z8ml7GndCNT4bnUV8Xbg4JSlp+UZSlrl1SYgqVjhfm8wkyl\ncavY9T0BqHAkQOEos6momDsnLOTx6V/SOjOdW4f35tTeCs8ljuwqhnVf7Dmf1+4pWLaXt2uS8935\nvLK7Q2ZbBfO1QIUjgQpHmdkrNnDzS3P5/OtNnNyzNbeO6EP7Frq5S+JYaSlsXLHnfF6F4dFK5BQs\n6c0ruXS4ezB5pKZgqTIVjgQsHBCE54+9v5xxby3CLAjPrzhW4bk0MO6w5dvKr/Taurq8XUqj4EFd\nkfN55XSHVgdrCpZKqHAkaOEoU7B+G79+ZT5vh+H57ecexhEKzyURbFu35z0oZUcpG78qb5OUEjxY\na49JIrtBVldIS9xL3FU4ErxwgMJzkT3s2FL+bJTIu+bXfQG+K2xk0KJThVA+/D6jeUy7XxdUOFQ4\ndttUVMxfJizkielf0iYzg98O782pvdsoPBeBYAqWdcsqhPKLgmB+147ydpkHfTeUL5uCpYH8LKlw\nqHB8xydfrefml+ay4JvNnNyzDbeO6K3wXGRvSnfB+uV7nu4qy1L2mIKl5XdD+ezu0LxD3BUUFQ4V\njkoV7yrlsfe/4K63Fis8FzkQ7rBp1Z7zeZUF9NvWlrdLbRJxuisiS2mZC8n18wkXKhwqHPu0Yt02\nfv3KPN5ZWEjvdkF4fngHheciNbJ1zXev8lqzKJjrq0xyWjAFS+QNjjk9gmUxnoJFhUOFY7/cnTfm\nfcNvx89nzZYdfO+YXH52aneaptfP34ZE4lbRpvIpWCIfuLV+OeVTsCSVT8Gyx13z3SA9s066qcKh\nwlFlm4qK+fObC3lyRhCe3zoiuPNcRKKseHsw3UrFS4fXLoHS4vJ2zTpUmCQyDOZreQoWFQ4Vjmqb\n9dV6fhmG50N7teHW4b1pp/BcpO7tKg6ORvZ4Nko4BUvxtvJ2jbO/O59X5+MgJe2APlaFQ4XjgBTv\nKuXR977grkmLSDJj7Cnd+f4xnRWei9QHpaWwqaA8P4ksKkXhFCy/XAVpTQ5o9yocKhw1smLdNm55\nZR5TFhbSp30zbj/ncA7r0PBvgBKJS+6wZXXwPPlOAw94N3srHPq1UaqkY6vGPHbFUdx3yZF8u2kH\nI+57j1v/O58tO0r2v7GI1C0zyGxTo6KxLyocUmVmxhmHH8SkMYO55OhO/POD5QwdN5WJ87+JdddE\npA5FtXCY2TAzW2hmS8zspn20G2lmbmZ5EcsON7MPzWy+mc01s4xwef/w/RIzu9c0T0ada94old+f\nfRgvXHMszRulMuqJjxn1eD6rNmzf/8YiEveiVjjMLBm4DzgN6AVcbGa9KmmXCYwGZkQsSwGeBK5x\n997AiUDZtWj3A1cDXcPXsGiNQfatf+eW/Penx3PTaT14d3EhQ8dN5dH3vmBXacPPzUQSWTSPOAYA\nS9x9mbvvBJ4BRlTS7nfAHUBRxLJTgDnu/imAu691911mdhDQzN2ne5DqPw6cHcUxyH6kJidxzeBD\neOuGweTltuK2Vz/j7PveZ27Bxv1vLCJxKZqFoz2wIuJ9QbhsNzM7Eujo7q9V2LYb4GY2wcxmmdnP\nI/ZZsK99Rux7lJnlm1l+YWFhTcYhVdCxVWP+eeVR/O2SfnyzqYgR973Hbf/9jK0Kz0UanJiF42aW\nBIwDxlayOgU4Hrg0/HqOmQ2pzv7d/SF3z3P3vJycnBr3V/bPzDjz8Ha7w/PHPvhC4blIAxTNwrES\n6BjxvkO4rEwm0AeYYmbLgYHA+DAgLwDedfc17r4NeB04Mty+wz72KfVAZHiemVEenn+9UeG5SEMQ\nzcIxE+hqZl3MLA24CBhfttLdN7p7trvnunsuMB0Y7u75wATgMDNrHAblg4HP3P1rYJOZDQyvpvoe\n8EoUxyA10L9zS1697nh+MSwIz0++cyqPva/wXCTeRa1wuHsJcC1BEfgceM7d55vZbWY2fD/bric4\njTUTmA3MishBfgw8AiwBlgJvRGkIUgtSk5P40YmHMPH6wfTPbcWt//2Mc/7+PvNWKjwXiVeackTq\njLvz6pyvufW/n7Fu6w6uPK4LY4Z2o4mmbReplzTliMScmXHWEe2YPHYwFw/oxD/eC8Lztz77NtZd\nE5FqUOGQOte8USr/d85hvPijY8jMSOXqx/P54RMKz0XihQqHxEz/zq149brj+fmw7kxZqPBcJF6o\ncEhMpSYn8eMTD+WtGwZzZOeWCs9F4oAKh9QLnbIa8/hVA7j34n6s2lDE8L+9x+9f1Z3nIvWRCofU\nG2bG8CPaMXnMYC4a0IlHwvB8ksJzkXpFhUPqneaNU/nDOYfxwjXH0DQjhf95PJ9rnviYbzYW7X9j\nEYk6FQ6pt/JyW/HqTwdx46ndeWfhak4eN5V/fbBc4blIjKlwSL2WlpLET046lIk3nEC/Ti34zfj5\nnPv395m/SuG5SKyocEhc6JzVhMevGsA9F/Vl5YbtDP/b+/zfawrPRWJBhUPihpkxom97Jo85kQvy\nOvLwtC845a53mfy5wnORuqTCIXGneeNUbj83CM8bpyXzg3/l86MnFZ6L1BUVDolbebmteO26IDx/\ne4HCc5G6osIhca0sPJ9wfUR4fv8HCs9FokiFQxqE3OyI8Hz9Nob/7X3+8PrnbNup8FyktqlwSINR\nFp5PGjOYC/I68NC7yxg67l3eXqDwXKQ2qXBIg9OicRq3n3s4z4fh+VX/zOfH//6YbzcpPBepDSoc\n0mAdFRGeT/p8NSffOZUnPlR4LlJTKhzSoO2+8/z6EziiYwtueWU+I+//gM9WbYp110TilgqHJITc\n7CY88YMB3H1hX1as28ZZf3uP2xWeixwQFQ5JGGbG2f3aM3nsYM7v34EHw/D8nQWrY901kbiiwiEJ\np0XjNP448nCe++ExNEpL5sp/zuQn/56l8FykilQ4JGEN6NKK168bxNih3Xjr828VnotUkQqHJLS0\nlCR+OqQrE64/gcM7Nt8dnn/+tcJzkb1R4RABumQ34ckfHM1dFx7BV+u2ceZf3+P2NxSei1QmqoXD\nzIaZ2UIzW2JmN+2j3UgzczPLC9/nmtl2M5sdvh6IaDsl3GfZutbRHIMkDjPjnH4dmDxmMOcd2YEH\npy7jlLve5Z2FCs9FIkWtcJhZMnAfcBrQC7jYzHpV0i4TGA3MqLBqqbv3DV/XVFh3acQ6/VRLrWrZ\nJI07zjucZ0cNJD0liSsfm8lPnprFaoXnIkB0jzgGAEvcfZm77wSeAUZU0u53wB2AfiqlXjn64Cxe\nHx2G5599y5A7p/LE9C8pVXguCS6ahaM9sCLifUG4bDczOxLo6O6vVbJ9FzP7xMymmtmgCuseC09T\n3WJmVtmHm9koM8s3s/zCwsKajEMSWHpK8u7w/LAOzbnlP/MY+YDCc0lsMQvHzSwJGAeMrWT110An\nd+8HjAGeMrNm4bpL3f0wYFD4uryy/bv7Q+6e5+55OTk5tT8ASShdspvw7/85mnEXHMGXa7dx1l/f\n449vLGD7zl2x7ppInYtm4VgJdIx43yFcViYT6ANMMbPlwEBgvJnlufsOd18L4O4fA0uBbuH7leHX\nzcBTBKfERKLOzDj3yCA8P/fI9jwwdSlD75rKFIXnkmCqVDjMbLSZNbPAP8xslpmdsp/NZgJdzayL\nmaUBFwHjy1a6+0Z3z3b3XHfPBaYDw90938xywnAdMzsY6AosM7MUM8sOl6cCZwLzqjlmkRpp2SSN\nP513BM+E4fkVj83kWoXnkkCqesRxlbtvAk4BWhKcHvrjvjZw9xLgWmAC8DnwnLvPN7PbzGz4fj7v\nBGCOmc0GXgCucfd1QDowwczmALMJjmAeruIYRGrVwDA8HzO0GxM/+5Yh46bypMJzSQDmvv9/5GY2\nx90PN7N7gCnu/rKZfRJmEPVeXl6e5+fnx7ob0oAtK9zCr/4zjw+WruXITi34w7mH0aNts/1vKFKP\nmdnH7p5XcXlVjzg+NrOJwOkEv/FnAqW12UGReHZwTlP+/T9Hc+f5R/DFmq2cee973PGmwnNpmKp6\nxJEE9AWWufsGM2sFdHD3OdHuYG3QEYfUpXVbd3L765/z/McFdGzViN+N6MOJ3TXBgcSfmh5xHAMs\nDIvGZcCvgI212UGRhqJVkzT+fH4QnqcmB+H5T5/+hNWbFZ5Lw1DVwnE/sM3MjiC472Ip8HjUeiXS\nAAw8OIs3Rg/ihpO7MWHeNwy5cyr/nqHwXOJfVQtHiQfntEYAf3P3+wjuwxCRfUhPSWb0yV154/pB\n9GnXnP99eR7nPfABC7/ZHOuuiRywqhaOzWZ2M8FluK+FmUdq9Lol0rAcktOUp64uD8/PuHeawnOJ\nW1UtHBcCOwju5/iG4C7wP0etVyINkJkxsn8HJo89kbP7tef+KUs59e53mbpIc6lJfKlS4QiLxb+B\n5mZ2JlDk7so4RA5AqyZp/OX8I3j66oGkJBnff/QjrlN4LnGkqlOOXAB8BJwPXADMMLPzotkxkYbu\nmEOyeOP6QVx/clfenPcNJ985ladmfKXwXOq9qt7H8SkwtOyhSWaWA0xy9yOi3L9aofs4pL5bWriF\n/315LtOXraN/55b84ZzD6N5W159IbNX0Po6kCk/aW1uNbUVkPw7JacrTVw/kL+cfwbLCLZxx7zT+\n9OYCiooVnkv9U9X//N80swlmdoWZXQG8BrwevW6JJB4z47yI8PzvU5Zyyl3v8q7Cc6lnqnSqCsDM\nRgLHhW+nufvLUetVLdOpKolHHyxdw69enseyNVsZ0bcdvzqjFzmZ6bHuliSQvZ2qqnLhiGcqHBKv\niop3cf+Updw/ZSkZqUncfHpPLszrSFJSpU9MFqlVB5RxmNlmM9tUyWuzmemhyyJRlpGazA1Du/H6\n6EH0PKgZN780lwse/JBF3+rOc4mdfRYOd89092aVvDLdXQ8bEKkjh7ZuyjOjBvLn8w5nSeEWTr9n\nGn+eoPBcYkNXRonECTPj/AflRx4AABO9SURBVLyOTB4zmBF923PfO8Gd59MWKzyXuqXCIRJnspqm\nc+cFR/DU/xxNkhmX/+MjRj/zCWu27Ih11yRBqHCIxKljD83mjdGDuG5IV16f+zVD7pzKMx/pznOJ\nPhUOkTiWkZrMmKHdeGP0CXRvm8lNL83lwoc+ZLHCc4kiFQ6RBuDQ1k15dtRA/nTe4SxevYXT753G\nXyYsVHguUaHCIdJAmBkXhOH5WUe042/vLOHUu9/lvcVrYt01aWBUOEQamKym6Yy7oO/u8Pyyf8zg\nhmdnKzyXWqPCIdJA7Q7P/9+hvDpnFUPunMqzMxWeS82pcIg0YBmpyYw5pTtvjB5E9zaZ/OLFuVz0\n0HSF51IjUS0cZjbMzBaa2RIzu2kf7UaamZtZXvg+18y2m9ns8PVARNv+ZjY33Oe9ZqZJe0T249DW\nmTwzaiB/Gnk4C7/dzOn3TuPOiQrP5cBErXCYWTJwH3Aa0Au42Mx6VdIuExgNzKiwaqm79w1f10Qs\nvx+4GugavoZFo/8iDU1SknHBUR2ZPHYwZx3ejr++vYRhCs/lAETziGMAsMTdl7n7TuAZYEQl7X4H\n3AHs94HLZnYQ0Mzdp3swre/jwNm12GeRBi+7aTrjLuzLkz84GmB3eL5W4blUUTQLR3tgRcT7gnDZ\nbmZ2JNDR3V+rZPsuZvaJmU01s0ER+yzY1z4j9j3KzPLNLL+wUHP5iFR0fNds3rz+hPLwfNxUnpu5\ngkR41ILUTMzCcTNLAsYBYytZ/TXQyd37AWOAp8ysWrPxuvtD7p7n7nk5OTk177BIA1QWnr9+3SC6\ntm7Kz1+cw4UPTWfJaoXnsnfRLBwrgY4R7zuEy8pkAn2AKWa2HBgIjDezPHff4e5rAdz9Y2Ap0C3c\nvsM+9ikiB6Brm0yeHXUMd4w8jIXfbOa0e6YxTuG57EU0C8dMoKuZdTGzNOAiYHzZSnff6O7Z7p7r\n7rnAdGC4u+ebWU4YrmNmBxOE4Mvc/Wtgk5kNDK+m+h7wShTHIJIwkpKMC4/qxOSxgznz8Hbc+/YS\nTrtnGu8vUXgue4pa4XD3EuBaYALwOfCcu883s9vMbPh+Nj8BmGNms4EXgGvcfV247sfAI8ASgiOR\nN6IyAJEEld00nbvC8LzUnUsfmcGY5xSeSzk9c1xE9qqoeBf3vbOEB6YupUl6Cr88vSfn9++Abp9K\nDAf0zHERSWwZqcmMjQzPXygLz7fEumsSQyocIrJfZeH5H889jAVfb+K0e95l3FuLFJ4nKBUOEamS\npCTjogGdmDz2RM447CDunbyY0+6ZxgcKzxOOCoeIVEtOZjp3X9SPJ34wgFJ3LlF4nnBUOETkgAzq\nmsOE60/g2pMOZfzsVZw8birP5+vO80SgwiEiBywjNZmfndqd10cP4pCcptz4whwuUnje4KlwiEiN\ndWuTyXM/PIbbzz2Mz7/exOn3TOMuhecNlgqHiNSKpCTj4jA8P+2wttwzeTGn3zOND5YqPG9oVDhE\npFblZKZzz0X9ePyqAZSUOpc8PIOxz33Kuq07Y901qSUqHCISFSd0y2HiDSfwk5MO4ZXZKxly5xSF\n5w2ECoeIRE1GajI3ntqD10cP4uAwPL/44eksLVR4Hs9UOEQk6rq1yeT5Hx7DH845jM9WbeK0u6dx\n96RF7ChReB6PVDhEpE4kJRmXHN2JSWMHM6xPW+6eFNx5/uHStbHumlSTCoeI1KnWmRnce3E//nXV\nAIp3lXLxw9P52fMKz+OJCoeIxMTgbjlMvH4wPz7xEP7zSRCev/BxgcLzOKDCISIx0ygtmZ8P68Fr\n1wXh+c+e/5RLHp7BMoXn9ZoKh4jEXPe25eH5vFUbGXb3NO6ZtFjheT2lwiEi9UJZeD557GBO7dOW\nuyYt4rR7pjF9mcLz+kaFQ0TqldaZGfz14n7888qjKN5VykUPTefG5z9lvcLzekOFQ0TqpRO7t2bi\n9YP50YmH8PInKxkybiovKjyvF1Q4RKTeapSWzC+G9eDV644nN6sxY5//lEsfUXgeayocIlLv9Wjb\njBeuOZb/O6cPc1duZNg907h3ssLzWFHhEJG4kJRkXHp05yA8792WcW8t4vR7pjFD4XmdU+EQkbhS\nFp4/duVR7Cgp5cKHpvPzFxSe1yUVDhGJSyd1b81bNwzmmsGH8OKsIDx/aZbC87qgwiEicatRWjI3\nndaDV396PJ2zGjPmuU+57B8z+GLN1lh3rUGLauEws2FmttDMlpjZTftoN9LM3MzyKizvZGZbzOxn\nEcuWm9lcM5ttZvnR7L+IxIeeBzXjxWuO5fdn92FOwUZOvftdhedRFLXCYWbJwH3AaUAv4GIz61VJ\nu0xgNDCjkt2MA96oZPlJ7t7X3fMqWSciCSgpybhsYGcmjxnM0F5tGPfWIs649z0++mJdrLvW4ETz\niGMAsMTdl7n7TuAZYEQl7X4H3AEURS40s7OBL4D5UeyjiDQwrZtlcN8lR/LYlUdRVLyLCx78kF+8\nMIcN2xSe15ZoFo72wIqI9wXhst3M7Eigo7u/VmF5U+AXwK2V7NeBiWb2sZmN2tuHm9koM8s3s/zC\nwsIDHYOIxKmy8PyHgw/mhVkFDLlzKi9/ovC8NsQsHDezJIJTUWMrWf1b4C53r+z20OPd/UiCU2A/\nMbMTKtu/uz/k7nnunpeTk1Nb3RaRONIoLZmbT+vJqz89no6tGnPDswrPa0M0C8dKoGPE+w7hsjKZ\nQB9gipktBwYC48OA/GjgT+Hy64Ffmtm1AO6+Mvy6GniZ4JSYiMhe9TyoGS/+6Fh+d3Yf5qwIwvO/\nTl7MzpLSWHctLkWzcMwEuppZFzNLAy4CxpetdPeN7p7t7rnungtMB4a7e767D4pYfjfwB3f/m5k1\nCcN0zKwJcAowL4pjEJEGIjnJuHxgcOf50F5tuPOtRZx+7zSF5wcgaoXD3UuAa4EJwOfAc+4+38xu\nM7PhB7jbNsB7ZvYp8BHwmru/WTs9FpFEsDs8v+Iotu8MwvObXlR4Xh2WCEFRXl6e5+frlg8R2dO2\nnSXcM2kxj7z3BS0apXLLmb0Y0bcdZhbrrtULZvZxZbc96M5xEUlYjdNSuPn0nvz32iA8v/7Z2Vz+\nj49YrvB8n1Q4RCTh9WoXhucjevPpig2ccve7/O1thed7o8IhIkIYnh+Ty6Sxgxnasw1/mbiIM+6d\nxszlCs8rUuEQEYnQplkG9116JI9ekce2nbs4/4EPufklheeRVDhERCrx/3q04a0xJzDqhIN5Lr+A\nk8dN5ZXZK3XnOSocIiJ71TgthV+e3pPx1x5H+5aNGf3MbL736Ed8uTaxw3MVDhGR/ejdrjkv/ehY\nbhvRm0++2sApd73Lfe8sSdjwXIVDRKQKkpOM7x2Ty+SxgxnSszV/nrAwYcNzFQ4RkWpo0yyDv1/a\nn398PzI8n8vGbcWx7lqdUeEQETkAQ3q2YeINZeH5CoaMm5Iw4bkKh4jIAWqSHhGet2iUMOG5CoeI\nSA31btecl358HLcOT4zwXIVDRKQWJCcZ3z82l0ljBvP/egTh+Vl/fY+Pv2x44bkKh4hILWrbPIP7\nL+vPI9/LY8uOEkbe/yG/fLlhhecqHCIiUXByryA8v3pQF5756CuGjJvK+E9XNYjwXIVDRCRKmqSn\n8L9n9GL8tcfTvkUG1z39Cd9/bCZfrd0W667ViAqHiEiU9WkfhOe/PasXs75cz9C7pvL3KUso3hWf\n4bkKh4hIHUhOMq44rgtvjTmBk7q35k9vLuTMe+MzPFfhEBGpQwc1b8QDl/fn4e/lsbmomJH3f8j/\nvjyXjdvjJzxX4RARiYGhvdrw1pjB/M/xXXj6o68YcudU/hsn4bkKh4hIjDRJT+FXZwbhebsWGfz0\n6U+44rGZrFhXv8NzFQ4RkRjr0745L4fhef7ydQy9ayr3T1lab8NzFQ4RkXqgLDyfNHYwg7vlcMeb\nC8I7z9fHumvfocIhIlKPHNS8EQ9ensdDl/dn4/Ziznvgg3oXnqtwiIjUQ6f0bstbYwZz1XFBeH7y\nuKm8Oqd+hOdRLRxmNszMFprZEjO7aR/tRpqZm1leheWdzGyLmf2suvsUEYl3TdNTuCUMz9s2y+Da\npz7hyn/GPjyPWuEws2TgPuA0oBdwsZn1qqRdJjAamFHJbsYBb1R3nyIiDUmf9s35z0+O4zdn9WLm\nF0F4/sDU2IXn0TziGAAscfdl7r4TeAYYUUm73wF3AEWRC83sbOALYP4B7FNEpEFJTjKuPK4Lb40Z\nzAldc/jjG0F4Puurug/Po1k42gMrIt4XhMt2M7MjgY7u/lqF5U2BXwC3VnefEfsYZWb5ZpZfWFh4\nYCMQEaln2rVoxEPfy+PBMDwfef8H/Oo/c9lUVHfheczCcTNLIjgVNbaS1b8F7nL3LQe6f3d/yN3z\n3D0vJyfnQHcjIlIvnRqG51ce24WnZgR3nr825+s6Cc9TorjvlUDHiPcdwmVlMoE+wBQzA2gLjDez\n4cDRwHlm9iegBVBqZkXAx/vZp4hIwmiansKvz+rFOf3ac/PLc/jJU7M4qXsOt43oQ8dWjaP2uRat\n6mRmKcAiYAjBf+4zgUvcff5e2k8Bfubu+RWW/xbY4u5/qe4+y+Tl5Xl+fv6+moiIxLWSXaU8/uGX\n3DlxIbvcueHkblx1fBdSkw/8xJKZfezueRWXR+1UlbuXANcCE4DPgefcfb6Z3RYeVdTaPmurzyIi\n8SolOYmrjg/C80Fdc7g9DM+/3VS0/42rKWpHHPWJjjhEJNFMmP8NL80q4O+X9ic5yQ5oH3s74ohm\nxiEiIjFyau+2nNq7bVT2rSlHRESkWlQ4RESkWlQ4RESkWlQ4RESkWlQ4RESkWlQ4RESkWlQ4RESk\nWlQ4RESkWhLiznEzKwS+PMDNs4E1tdideKAxJ4ZEG3OijRdqPubO7v6d6cUTonDUhJnlV3bLfUOm\nMSeGRBtzoo0XojdmnaoSEZFqUeEQEZFqUeHYv4di3YEY0JgTQ6KNOdHGC1EaszIOERGpFh1xiIhI\ntahwiIhItahwhMxsmJktNLMlZnZTJevTzezZcP0MM8ut+17WniqMd4yZfWZmc8xsspl1jkU/a9P+\nxhzRbqSZuZnF/aWbVRmzmV0Q/l3PN7On6rqPta0K/7Y7mdk7ZvZJ+O/79Fj0s7aY2aNmttrM5u1l\nvZnZveGfxxwzO7LGH+ruCf8CkoGlwMFAGvAp0KtCmx8DD4TfXwQ8G+t+R3m8JwGNw+9/FM/jreqY\nw3aZwLvAdCAv1v2ug7/nrsAnQMvwfetY97sOxvwQ8KPw+17A8lj3u4ZjPgE4Epi3l/WnA28ABgwE\nZtT0M3XEERgALHH3Ze6+E3gGGFGhzQjgX+H3LwBDzOzAHuQbe/sdr7u/4+7bwrfTgQ513MfaVpW/\nY4DfAXcARXXZuSipypivBu5z9/UA7r66jvtY26oyZgeahd83B1bVYf9qnbu/C6zbR5MRwOMemA60\nMLODavKZKhyB9sCKiPcF4bJK27h7CbARyKqT3tW+qow30g8IfmOJZ/sdc3gI39HdX6vLjkVRVf6e\nuwHdzOx9M5tuZsPqrHfRUZUx/xa4zMwKgNeBn9ZN12Kmuj/v+5VSo+5Ig2dmlwF5wOBY9yWazCwJ\nGAdcEeOu1LUUgtNVJxIcVb5rZoe5+4aY9iq6Lgb+6e53mtkxwBNm1sfdS2PdsXihI47ASqBjxPsO\n4bJK25hZCsEh7to66V3tq8p4MbOTgf8Fhrv7jjrqW7Tsb8yZQB9gipktJzgXPD7OA/Kq/D0XAOPd\nvdjdvwAWERSSeFWVMf8AeA7A3T8EMggmA2yoqvTzXh0qHIGZQFcz62JmaQTh9/gKbcYD3w+/Pw94\n28PkKQ7td7xm1g94kKBoxPt5b9jPmN19o7tnu3uuu+cS5DrD3T0/Nt2tFVX5d/0fgqMNzCyb4NTV\nsrrsZC2rypi/AoYAmFlPgsJRWKe9rFvjge+FV1cNBDa6+9c12aFOVRFkFmZ2LTCB4KqMR919vpnd\nBuS7+3jgHwSHtEsIgqiLYtfjmqnieP8MNAWeD68B+Mrdh8es0zVUxTE3KFUc8wTgFDP7DNgF3Oju\n8XokXdUxjwUeNrMbCILyK+L4l0DM7GmC4p8d5ja/AVIB3P0BghzndGAJsA24ssafGcd/XiIiEgM6\nVSUiItWiwiEiItWiwiEiItWiwiEiItWiwiEiItWiwiFSj5nZiWb2aqz7IRJJhUNERKpFhUOkFpjZ\nZWb2kZnNNrMHzSzZzLaY2V3hcy4mm1lO2LZvOKHgHDN72cxahssPNbNJZvapmc0ys0PC3Tc1sxfM\nbIGZ/TuOZ2WWBkKFQ6SGwmkrLgSOc/e+BHdgXwo0IbhbuTcwleCOXoDHgV+4++HA3Ijl/yaY4vwI\n4FigbFqIfsD1BM+OOBg4LuqDEtkHTTkiUnNDgP7AzPBgoBGwGigFng3bPAm8ZGbNgRbuPjVc/i+C\naV0ygfbu/jKAuxcBhPv7yN0LwvezgVzgvegPS6RyKhwiNWfAv9z95j0Wmt1Sod2Bzu8TOTPxLvRz\nKzGmU1UiNTcZOM/MWgOYWavwGe1JBDMpA1wCvOfuG4H1ZjYoXH45MNXdNwMFZnZ2uI90M2tcp6MQ\nqSL95iJSQ+7+mZn9CpgYPhCqGPgJsBUYEK5bTZCDQDA9/wNhYVhG+WyllwMPhjO5FgPn1+EwRKpM\ns+OKRImZbXH3prHuh0ht06kqERGpFh1xiIhIteiIQ0REqkWFQ0REqkWFQ0REqkWFQ0REqkWFQ0RE\nquX/A4V2mmSNA4jsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rivD55Qc85D",
        "colab_type": "text"
      },
      "source": [
        "## Saving and restoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJNgkArdBAs",
        "colab_type": "code",
        "outputId": "938a5a19-fd87-4c8f-fa89-70a96d4922e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "print(training_y[11:20])\n",
        "model.predict(training_x[11:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 1 1 0 0 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1199744 ],\n",
              "       [0.08535063],\n",
              "       [0.46131864],\n",
              "       [0.09784642],\n",
              "       [0.5780003 ],\n",
              "       [0.73423684],\n",
              "       [0.03702918],\n",
              "       [0.05184305],\n",
              "       [0.85905075]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix02jwj_u_64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json.dump(model.to_json(), open(\"model.json\", \"w\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgWsbzSaeSC6",
        "colab_type": "code",
        "outputId": "e4080bda-a402-4891-b711-66a6b65affac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        }
      },
      "source": [
        "model = tf.keras.models.model_from_json(json.load(open(\"model.json\")), \n",
        "                                        custom_objects={\"BertLayer\": BertLayer})\n",
        "\n",
        "model.load_weights(\"bert_tuned.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-2b4a801d7e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                         custom_objects={\"BertLayer\": BertLayer})\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert_tuned.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    181\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1365\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   1366\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'bert_tuned.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7H2AY9MsvUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(training_x[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_RWAQ-Um9AT",
        "colab_type": "text"
      },
      "source": [
        "### Freeze model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg5crVOofJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
        "\n",
        "def freeze_keras_model(model, export_path=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes a Keras model into a pruned computation graph.\n",
        "\n",
        "    @param model The Keras model to be freezed.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    \n",
        "    sess = tf.keras.backend.get_session()\n",
        "    graph = sess.graph\n",
        "    \n",
        "    with graph.as_default():\n",
        "\n",
        "        input_tensors = model.inputs\n",
        "        output_tensors = model.outputs\n",
        "        dtypes = [t.dtype.as_datatype_enum for t in input_tensors]\n",
        "        input_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in input_tensors]\n",
        "        output_ops = [t.name.rsplit(\":\", maxsplit=1)[0] for t in output_tensors]\n",
        "        \n",
        "        tmp_g = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in tmp_g.node:\n",
        "                node.device = \"\"\n",
        "        \n",
        "        tmp_g = optimize_for_inference(\n",
        "            tmp_g, input_ops, output_ops, dtypes, False)\n",
        "        \n",
        "        tmp_g = convert_variables_to_constants(sess, tmp_g, output_ops)\n",
        "        \n",
        "        if export_path is not None:\n",
        "            with tf.gfile.GFile(export_path, \"wb\") as f:\n",
        "                f.write(tmp_g.SerializeToString())\n",
        "        \n",
        "        return tmp_g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gMUrFpgofMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = freeze_keras_model(model, export_path=\"frozen_graph_256_64.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShmlWdXLcH80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/gaphex/bert_experimental/\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/bert_experimental\")\n",
        "\n",
        "from bert_experimental.finetuning.text_preprocessing import build_preprocessor\n",
        "from bert_experimental.finetuning.graph_ops import load_graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avKRLyiXvrqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "restored_graph = load_graph(\"frozen_graph.pb\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8bT-YXrgo6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_ops = restored_graph.get_operations()\n",
        "input_op, output_op = graph_ops[0].name, graph_ops[-1].name\n",
        "print(input_op, output_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHr2ZQGfg3y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = restored_graph.get_tensor_by_name(input_op + ':0')\n",
        "y = restored_graph.get_tensor_by_name(output_op + ':0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPoapr4e86Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessor = build_preprocessor(\"./uncased_L-12_H-768_A-12/vocab.txt\", 64)\n",
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32], name='preprocessor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQFQOML7ivdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "py_func = tf.numpy_function(preprocessor, [x], [tf.int32, tf.int32, tf.int32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRf75n6ECUa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session(graph=restored_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw55NB6YseBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = sess.run(y, feed_dict={\n",
        "        x: training_x[:10].reshape((-1,1))\n",
        "    })\n",
        "\n",
        "y_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou527zR39gbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}